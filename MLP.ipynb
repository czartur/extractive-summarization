{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuxa/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import custom_utils\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize(texts):\n",
    "    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text data\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        max_len (int): Maximum sentence length\n",
    "    \"\"\"\n",
    "\n",
    "    max_len = 0\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "\n",
    "    # Add <pad> and <unk> tokens to the vocabulary\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "\n",
    "    # Building our vocab from the corpus starting from index 2\n",
    "    idx = 2\n",
    "    for sent in texts:\n",
    "        tokenized_sent = word_tokenize(sent)\n",
    "\n",
    "        # Add `tokenized_sent` to `tokenized_texts`\n",
    "        tokenized_texts.append(tokenized_sent)\n",
    "\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_sent:\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # Update `max_len`\n",
    "        max_len = max(max_len, len(tokenized_sent))\n",
    "\n",
    "    return tokenized_texts, word2idx, max_len\n",
    "\n",
    "def encode(tokenized_texts, word2idx, max_len):\n",
    "    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n",
    "    their index in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will the input of our CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    for tokenized_sent in tokenized_texts:\n",
    "        # Pad sentences to max_len\n",
    "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
    "\n",
    "        # Encode tokens to input_ids\n",
    "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
    "        input_ids.append(input_id)\n",
    "    \n",
    "    return np.array(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mpunkt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_texts, word2idx, max_len = tokenize(X)\n",
    "input_ids = encode(tokenized_texts, word2idx, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'ME': 2,\n",
       " ':': 3,\n",
       " '<': 4,\n",
       " 'vocalsound': 5,\n",
       " '>': 6,\n",
       " 'Okay': 7,\n",
       " '.': 8,\n",
       " 'Oh': 9,\n",
       " 'I': 10,\n",
       " 'totally': 11,\n",
       " 'disfmarker': 12,\n",
       " 'Yeah': 13,\n",
       " \"'cause\": 14,\n",
       " 'moved': 15,\n",
       " 'it': 16,\n",
       " \"'S\": 17,\n",
       " 'put': 18,\n",
       " 'over': 19,\n",
       " 'here': 20,\n",
       " 'Then': 21,\n",
       " 'we': 22,\n",
       " 'do': 23,\n",
       " \"n't\": 24,\n",
       " 'have': 25,\n",
       " 'to': 26,\n",
       " 'worry': 27,\n",
       " 'about': 28,\n",
       " 'UI': 29,\n",
       " 'Ready': 30,\n",
       " 'for': 31,\n",
       " 'this': 32,\n",
       " '?': 33,\n",
       " 'PM': 34,\n",
       " 'All': 35,\n",
       " 'set': 36,\n",
       " 'Cool': 37,\n",
       " 'Alright': 38,\n",
       " ',': 39,\n",
       " 'is': 40,\n",
       " 'PowerPoint': 41,\n",
       " 'time': 42,\n",
       " \"'ve\": 43,\n",
       " 'done': 44,\n",
       " 'more': 45,\n",
       " 'PowerPoints': 46,\n",
       " 'in': 47,\n",
       " 'particular': 48,\n",
       " 'experiment': 49,\n",
       " 'than': 50,\n",
       " 'ever': 51,\n",
       " 'my': 52,\n",
       " 'life': 53,\n",
       " 'before': 54,\n",
       " 'ID': 55,\n",
       " 'which': 56,\n",
       " 'kind': 57,\n",
       " 'of': 58,\n",
       " 'fun': 59,\n",
       " 'man': 60,\n",
       " 'So': 61,\n",
       " 'uh': 62,\n",
       " 'our': 63,\n",
       " 'detailed': 64,\n",
       " 'design': 65,\n",
       " 'meeting': 66,\n",
       " 'where': 67,\n",
       " 'will': 68,\n",
       " 'um': 69,\n",
       " 'look': 70,\n",
       " 'at': 71,\n",
       " 'the': 72,\n",
       " 'prototype': 73,\n",
       " 'and': 74,\n",
       " 'right': 75,\n",
       " 'so': 76,\n",
       " 'finally': 77,\n",
       " 'figured': 78,\n",
       " 'out': 79,\n",
       " 'what': 80,\n",
       " 'whole': 81,\n",
       " 'second': 82,\n",
       " 'bullet': 83,\n",
       " 'point': 84,\n",
       " 'that': 85,\n",
       " 'coach': 86,\n",
       " 'was': 87,\n",
       " 'sending': 88,\n",
       " 'me': 89,\n",
       " 'It': 90,\n",
       " 'means': 91,\n",
       " \"'m\": 92,\n",
       " 'supposed': 93,\n",
       " 'read': 94,\n",
       " 'minutes': 95,\n",
       " 'from': 96,\n",
       " 'previous': 97,\n",
       " 'really': 98,\n",
       " 'think': 99,\n",
       " 'Huh': 100,\n",
       " 'know': 101,\n",
       " 'Otherwise': 102,\n",
       " \"'s\": 103,\n",
       " 'just': 104,\n",
       " 'saying': 105,\n",
       " 'secretary': 106,\n",
       " 'therefore': 107,\n",
       " 'taking': 108,\n",
       " 's': 109,\n",
       " 'go': 110,\n",
       " 'real': 111,\n",
       " 'briefly': 112,\n",
       " 'last': 113,\n",
       " 'open': 114,\n",
       " 'them': 115,\n",
       " 'slowly': 116,\n",
       " 'no': 117,\n",
       " 'Wait': 118,\n",
       " 'wait': 119,\n",
       " 'not': 120,\n",
       " 'you': 121,\n",
       " 'No': 122,\n",
       " 'That': 123,\n",
       " 'how': 124,\n",
       " 'This': 125,\n",
       " 'very': 126,\n",
       " 'high-powered': 127,\n",
       " 'stuff': 128,\n",
       " 'double-clicking': 129,\n",
       " 'there': 130,\n",
       " 'basically': 131,\n",
       " 'moral': 132,\n",
       " 'story': 133,\n",
       " 'minute': 134,\n",
       " 'had': 135,\n",
       " 'meetings': 136,\n",
       " 'presentations': 137,\n",
       " 'by': 138,\n",
       " 'Industrial': 139,\n",
       " 'Designer': 140,\n",
       " 'or': 141,\n",
       " 'Nathan': 142,\n",
       " 'Ron': 143,\n",
       " 'Sarah': 144,\n",
       " 'can': 145,\n",
       " 'sort': 146,\n",
       " 'limitations': 147,\n",
       " \"'re\": 148,\n",
       " 'operating': 149,\n",
       " 'with': 150,\n",
       " 'excuse': 151,\n",
       " 'under': 152,\n",
       " 'risk': 153,\n",
       " \"'d\": 154,\n",
       " 'be': 155,\n",
       " 'looking': 156,\n",
       " 'some': 157,\n",
       " 'various': 158,\n",
       " 'approaches': 159,\n",
       " 'were': 160,\n",
       " 'discussing': 161,\n",
       " 'essentially': 162,\n",
       " 'came': 163,\n",
       " 'conclusion': 164,\n",
       " 'should': 165,\n",
       " 'develop': 166,\n",
       " 'a': 167,\n",
       " 'remote': 168,\n",
       " 'voice': 169,\n",
       " 'recognition': 170,\n",
       " 'I_E_': 171,\n",
       " 'vaguely': 172,\n",
       " 'non-remote': 173,\n",
       " 'like': 174,\n",
       " 'shape': 175,\n",
       " 'because': 176,\n",
       " 'did': 177,\n",
       " 'need': 178,\n",
       " 'use': 179,\n",
       " 'as': 180,\n",
       " 'since': 181,\n",
       " 'could': 182,\n",
       " 'your': 183,\n",
       " 'would': 184,\n",
       " 'include': 185,\n",
       " 'mostly': 186,\n",
       " 'simple': 187,\n",
       " 'features': 188,\n",
       " 'television': 189,\n",
       " 'operation': 190,\n",
       " 'but': 191,\n",
       " 'slide': 192,\n",
       " 'fold-out': 193,\n",
       " 'bay': 194,\n",
       " 'advanced': 195,\n",
       " 'functions': 196,\n",
       " 'users': 197,\n",
       " 'Um': 198,\n",
       " 'U_I_D_': 199,\n",
       " 'I_D_': 200,\n",
       " 'asked': 201,\n",
       " 'ahead': 202,\n",
       " 'start': 203,\n",
       " 'developing': 204,\n",
       " 'us': 205,\n",
       " 'sorted': 206,\n",
       " 'back': 207,\n",
       " 'main': 208,\n",
       " 'meet': 209,\n",
       " 'take': 210,\n",
       " 'away': 211,\n",
       " 'guys': 212,\n",
       " 'Well': 213,\n",
       " 'Uh': 214,\n",
       " 'assembled': 215,\n",
       " 'What': 216,\n",
       " 'said': 217,\n",
       " 'took': 218,\n",
       " 'into': 219,\n",
       " 'account': 220,\n",
       " 'lot': 221,\n",
       " 'things': 222,\n",
       " 'went': 223,\n",
       " 'Some': 224,\n",
       " 'most': 225,\n",
       " 'important': 226,\n",
       " 'consider': 227,\n",
       " 'are': 228,\n",
       " 'decided': 229,\n",
       " 'touch': 230,\n",
       " 'screen': 231,\n",
       " 'see': 232,\n",
       " 'opted': 233,\n",
       " 'large': 234,\n",
       " 'buttons': 235,\n",
       " 'primary': 236,\n",
       " 'going': 237,\n",
       " 'on': 238,\n",
       " 'off': 239,\n",
       " 'button': 240,\n",
       " 'these': 241,\n",
       " 'through': 242,\n",
       " 'channels': 243,\n",
       " 'then': 244,\n",
       " 'two': 245,\n",
       " 'volume': 246,\n",
       " 'down': 247,\n",
       " 'd': 248,\n",
       " 'those': 249,\n",
       " 'And': 250,\n",
       " 'panel': 251,\n",
       " 'lots': 252,\n",
       " 'other': 253,\n",
       " 'But': 254,\n",
       " 'actually': 255,\n",
       " 'provides': 256,\n",
       " 'nice': 257,\n",
       " 'aesthetic': 258,\n",
       " 'when': 259,\n",
       " 'all': 260,\n",
       " 'As': 261,\n",
       " 'far': 262,\n",
       " 'visible': 263,\n",
       " 'light': 264,\n",
       " 'thing': 265,\n",
       " 'multiple': 266,\n",
       " 'colours': 267,\n",
       " 'coming': 268,\n",
       " 'Nice': 269,\n",
       " 'why': 270,\n",
       " 'Fair': 271,\n",
       " 'enough': 272,\n",
       " 'Of': 273,\n",
       " 'course': 274,\n",
       " 'if': 275,\n",
       " 'annoying': 276,\n",
       " 'people': 277,\n",
       " 'function': 278,\n",
       " 'turned': 279,\n",
       " 'Perfect': 280,\n",
       " 'Go': 281,\n",
       " 'talked': 282,\n",
       " 'quite': 283,\n",
       " 'bit': 284,\n",
       " 'interchangeable': 285,\n",
       " 'faces': 286,\n",
       " 'come': 287,\n",
       " 'up': 288,\n",
       " 'natural': 289,\n",
       " 'f': 290,\n",
       " 'call': 291,\n",
       " 'fruity': 292,\n",
       " 'Right': 293,\n",
       " 'Appropriate': 294,\n",
       " 'okay': 295,\n",
       " 'desirable': 296,\n",
       " 'regular': 297,\n",
       " 'product': 298,\n",
       " 'first': 299,\n",
       " 'packaging': 300,\n",
       " 'something': 301,\n",
       " 'little': 302,\n",
       " 'subdued': 303,\n",
       " 'Mm': 304,\n",
       " \"'kay\": 305,\n",
       " 'an': 306,\n",
       " 'option': 307,\n",
       " 'detector': 308,\n",
       " 'device': 309,\n",
       " 'top': 310,\n",
       " 'Ah': 311,\n",
       " 'work': 312,\n",
       " 'well': 313,\n",
       " 'regard': 314,\n",
       " 'finding': 315,\n",
       " 'contraption': 316,\n",
       " 'give': 317,\n",
       " 'does': 318,\n",
       " 'spongy': 319,\n",
       " 'feel': 320,\n",
       " 'regards': 321,\n",
       " 'market': 322,\n",
       " 'let': 323,\n",
       " 'clearly': 324,\n",
       " 'gon': 325,\n",
       " 'na': 326,\n",
       " 'available': 327,\n",
       " 'anything': 328,\n",
       " 'else': 329,\n",
       " 'add': 330,\n",
       " 'worried': 331,\n",
       " 'materials': 332,\n",
       " 'entire': 333,\n",
       " 'covered': 334,\n",
       " 'rubber': 335,\n",
       " 'coating': 336,\n",
       " 'durable': 337,\n",
       " 'break': 338,\n",
       " 'types': 339,\n",
       " 'plastic': 340,\n",
       " 'dropped': 341,\n",
       " 'squishy': 342,\n",
       " 'note': 343,\n",
       " 'earthquake': 344,\n",
       " 'i': 345,\n",
       " 'edible': 346,\n",
       " 'inside': 347,\n",
       " 'Fact': 348,\n",
       " 'dunno': 349,\n",
       " 'noticed': 350,\n",
       " 'wrote': 351,\n",
       " 'company': 352,\n",
       " 'name': 353,\n",
       " 'telephone': 354,\n",
       " 'yeah': 355,\n",
       " 'oh': 356,\n",
       " 'ok': 357,\n",
       " 'thought': 358,\n",
       " 'kinda': 359,\n",
       " 'apple': 360,\n",
       " 'Do': 361,\n",
       " 'rot': 362,\n",
       " 'factors': 363,\n",
       " 'encased': 364,\n",
       " 'new': 365,\n",
       " 'type': 366,\n",
       " 'preservatives': 367,\n",
       " 'involved': 368,\n",
       " 'polymer': 369,\n",
       " 'fine': 370,\n",
       " 'We': 371,\n",
       " 'got': 372,\n",
       " 'ourselves': 373,\n",
       " 'talking': 374,\n",
       " 'making': 375,\n",
       " 'televisions': 376,\n",
       " 'Edible': 377,\n",
       " 'wave': 378,\n",
       " 'future': 379,\n",
       " 'couple': 380,\n",
       " 'years': 381,\n",
       " 'least': 382,\n",
       " 'pos': 383,\n",
       " 'possible': 384,\n",
       " 'sums': 385,\n",
       " 'any': 386,\n",
       " 'questions': 387,\n",
       " 'Brilliant': 388,\n",
       " 'whether': 389,\n",
       " 'marketing': 390,\n",
       " 'areas': 391,\n",
       " 'nature': 392,\n",
       " 'budget': 393,\n",
       " 'cost': 394,\n",
       " 'production': 395,\n",
       " 'goal': 396,\n",
       " 'twelve': 397,\n",
       " 'fifty': 398,\n",
       " 'Euro': 399,\n",
       " 'eleven': 400,\n",
       " 'ninety': 401,\n",
       " 'nine': 402,\n",
       " 'pleased': 403,\n",
       " 'One': 404,\n",
       " 'obviously': 405,\n",
       " 'choice': 406,\n",
       " 'scroll': 407,\n",
       " 'standard': 408,\n",
       " 'classic': 409,\n",
       " 'many': 410,\n",
       " 'microchips': 411,\n",
       " 'helped': 412,\n",
       " 'keep': 413,\n",
       " 'even': 414,\n",
       " 'though': 415,\n",
       " 'has': 416,\n",
       " 'modern': 417,\n",
       " 'technology': 418,\n",
       " 'example': 419,\n",
       " 'ways': 420,\n",
       " 'shopped': 421,\n",
       " 'around': 422,\n",
       " 'manufacturers': 423,\n",
       " 'might': 424,\n",
       " 'able': 425,\n",
       " 'get': 426,\n",
       " 'cheaper': 427,\n",
       " 'Did': 428,\n",
       " 'talk': 429,\n",
       " 'yet': 430,\n",
       " 'area': 431,\n",
       " 'console': 432,\n",
       " 'nicely': 433,\n",
       " 'designed': 434,\n",
       " 'overall': 435,\n",
       " 'incorporates': 436,\n",
       " 'latest': 437,\n",
       " 'designs': 438,\n",
       " 'research': 439,\n",
       " 'team': 440,\n",
       " 'been': 441,\n",
       " 'cufw': 442,\n",
       " 'Basically': 443,\n",
       " 'similar': 444,\n",
       " 'coffee': 445,\n",
       " 'maker': 446,\n",
       " 'earlier': 447,\n",
       " 'given': 448,\n",
       " 'proven': 449,\n",
       " 'ease': 450,\n",
       " 'Hmm': 451,\n",
       " 'allows': 452,\n",
       " 'user': 453,\n",
       " 'Any': 454,\n",
       " 'lack': 455,\n",
       " 'better': 456,\n",
       " 'word': 457,\n",
       " 'skins': 458,\n",
       " 'Covers': 459,\n",
       " 'In': 460,\n",
       " 'play': 461,\n",
       " 'now': 462,\n",
       " 'ones': 463,\n",
       " 'developed': 464,\n",
       " 'later': 465,\n",
       " 'once': 466,\n",
       " 'g': 467,\n",
       " 'wan': 468,\n",
       " 'answer': 469,\n",
       " 'one': 470,\n",
       " 'stand': 471,\n",
       " 'want': 472,\n",
       " 'material': 473,\n",
       " 'expecting': 474,\n",
       " 'mind': 475,\n",
       " 'superficial': 476,\n",
       " 'layer': 477,\n",
       " 'easy': 478,\n",
       " 'another': 479,\n",
       " 'Just': 480,\n",
       " 'veneer': 481,\n",
       " 'Actually': 482,\n",
       " 'bottom': 483,\n",
       " 'red': 484,\n",
       " 'ring': 485,\n",
       " 'unclips': 486,\n",
       " 'plate': 487,\n",
       " 'mean': 488,\n",
       " 'definitely': 489,\n",
       " 'priced': 490,\n",
       " 'spongier': 491,\n",
       " 'non-natural': 492,\n",
       " 'There': 493,\n",
       " 'worked': 494,\n",
       " 'also': 495,\n",
       " 'continued': 496,\n",
       " 'ideas': 497,\n",
       " 'following': 498,\n",
       " 'Apple': 499,\n",
       " 'colour': 500,\n",
       " 'schemes': 501,\n",
       " 'orange': 502,\n",
       " 'green': 503,\n",
       " 'cool': 504,\n",
       " 'face': 505,\n",
       " 'pseudo-face': 506,\n",
       " 'factory': 507,\n",
       " 'easily': 508,\n",
       " 'different': 509,\n",
       " 'locks': 510,\n",
       " 'place': 511,\n",
       " 'such': 512,\n",
       " 'pretty': 513,\n",
       " 'permanent': 514,\n",
       " 'same': 515,\n",
       " 'way': 516,\n",
       " 'matter': 517,\n",
       " 'adjustments': 518,\n",
       " 'Yep': 519,\n",
       " 'still': 520,\n",
       " 'Very': 521,\n",
       " 'job': 522,\n",
       " 'thanks': 523,\n",
       " 'good': 524,\n",
       " 'brilliant': 525,\n",
       " 'discuss': 526,\n",
       " 'finance': 527,\n",
       " 'provided': 528,\n",
       " 'number': 529,\n",
       " 'sounds': 530,\n",
       " 'trouble': 531,\n",
       " 'spreadsheet': 532,\n",
       " 'parts': 533,\n",
       " 'tentatively': 534,\n",
       " 'Ooh': 535,\n",
       " 'clear': 536,\n",
       " 'quickly': 537,\n",
       " 'looks': 538,\n",
       " \"'ll\": 539,\n",
       " 'itemize': 540,\n",
       " 'solar': 541,\n",
       " 'cell': 542,\n",
       " 'With': 543,\n",
       " 'back-up': 544,\n",
       " 'battery': 545,\n",
       " 'yep': 546,\n",
       " 'ba': 547,\n",
       " 'The': 548,\n",
       " 'doubles': 549,\n",
       " 'Clever': 550,\n",
       " 'clever': 551,\n",
       " 'guess': 552,\n",
       " 'speaker': 553,\n",
       " 'sensor': 554,\n",
       " 'space': 555,\n",
       " 'case': 556,\n",
       " 'single-curved': 557,\n",
       " 'general': 558,\n",
       " 'big': 559,\n",
       " 'curve': 560,\n",
       " 'say': 561,\n",
       " 'skin': 562,\n",
       " 'throughout': 563,\n",
       " 'Push': 564,\n",
       " 'interface': 565,\n",
       " 'drop-down': 566,\n",
       " 'maybe': 567,\n",
       " 'push': 568,\n",
       " 'interfaces': 569,\n",
       " 'mm-hmm': 570,\n",
       " 'special': 571,\n",
       " 'wood': 572,\n",
       " 'materi': 573,\n",
       " 'rubbery': 574,\n",
       " 'mark': 575,\n",
       " 'form': 576,\n",
       " \"'Cause\": 577,\n",
       " 'unconventional': 578,\n",
       " 'unique': 579,\n",
       " 'M': 580,\n",
       " 'sixteen': 581,\n",
       " 'match': 582,\n",
       " 'perhaps': 583,\n",
       " 'fix': 584,\n",
       " 'switch': 585,\n",
       " 'cells': 586,\n",
       " 'How': 587,\n",
       " 'being': 588,\n",
       " 'selling': 589,\n",
       " 'environmental': 590,\n",
       " 'without': 591,\n",
       " 'batteries': 592,\n",
       " 'although': 593,\n",
       " 'sure': 594,\n",
       " 'sell': 595,\n",
       " 'probably': 596,\n",
       " 'per': 597,\n",
       " 'cent': 598,\n",
       " 'centre': 599,\n",
       " 'calling': 600,\n",
       " 'working': 601,\n",
       " 'am': 602,\n",
       " 'Mm-hmm': 603,\n",
       " 'k': 604,\n",
       " 'People': 605,\n",
       " 'upset': 606,\n",
       " 'long-run': 607,\n",
       " 'True': 608,\n",
       " 'hard': 609,\n",
       " 'scrap': 610,\n",
       " 'idea': 611,\n",
       " 'integral': 612,\n",
       " 'theme': 613,\n",
       " 'difficult': 614,\n",
       " 'ca': 615,\n",
       " 'Nah': 616,\n",
       " 'seems': 617,\n",
       " 'square': 618,\n",
       " 'try': 619,\n",
       " 'undo': 620,\n",
       " 'Although': 621,\n",
       " 'rid': 622,\n",
       " 'piece': 623,\n",
       " 'honestly': 624,\n",
       " 'cut': 625,\n",
       " 'math': 626,\n",
       " 'correctly': 627,\n",
       " 'sway': 628,\n",
       " 'already': 629,\n",
       " 'sets': 630,\n",
       " 'apart': 631,\n",
       " 'Which': 632,\n",
       " 'setting': 633,\n",
       " 'young': 634,\n",
       " 'started': 635,\n",
       " 'comes': 636,\n",
       " 'price': 637,\n",
       " 'bring': 638,\n",
       " 'game': 639,\n",
       " 'may': 640,\n",
       " 'league': 641,\n",
       " 'reality': 642,\n",
       " 'ideological': 643,\n",
       " 'stick': 644,\n",
       " 'h': 645,\n",
       " 'throw': 646,\n",
       " 'myself': 647,\n",
       " 'business': 648,\n",
       " 'structure': 649,\n",
       " 'model': 650,\n",
       " 'compromise': 651,\n",
       " 'either': 652,\n",
       " 'move': 653,\n",
       " 'project': 654,\n",
       " 'unfortunately': 655,\n",
       " 'best': 656,\n",
       " 'only': 657,\n",
       " 'below': 658,\n",
       " 'remove': 659,\n",
       " 'they': 660,\n",
       " 'removing': 661,\n",
       " 'Savings': 662,\n",
       " 'changing': 663,\n",
       " 'much': 664,\n",
       " 'mm-mm': 665,\n",
       " 'nor': 666,\n",
       " 'Mm-mm': 667,\n",
       " 'major': 668,\n",
       " 'change': 669,\n",
       " 'Got': 670,\n",
       " 'ta': 671,\n",
       " 'agreement': 672,\n",
       " 'Unfortunately': 673,\n",
       " 'brought': 674,\n",
       " 'forward': 675,\n",
       " 'Moving': 676,\n",
       " 'along': 677,\n",
       " 'swiftly': 678,\n",
       " 'evaluation': 679,\n",
       " 'allow': 680,\n",
       " 'cord': 681,\n",
       " 'sorry': 682,\n",
       " 'problem': 683,\n",
       " 'Whoosh': 684,\n",
       " 'Can': 685,\n",
       " 'reach': 686,\n",
       " 'great': 687,\n",
       " 'thank': 688,\n",
       " 'purpose': 689,\n",
       " 'damn': 690,\n",
       " 'evaluating': 691,\n",
       " 'criteria': 692,\n",
       " 'beginning': 693,\n",
       " 'needed': 694,\n",
       " 'basic': 695,\n",
       " 'fulfil': 696,\n",
       " 'its': 697,\n",
       " 'Is': 698,\n",
       " 'wanted': 699,\n",
       " 'technologies': 700,\n",
       " 'hoped': 701,\n",
       " 'qualities': 702,\n",
       " 'original': 703,\n",
       " 'Basic': 704,\n",
       " 'turn': 705,\n",
       " 'Does': 706,\n",
       " 'respond': 707,\n",
       " 'par': 708,\n",
       " 'pull-out': 709,\n",
       " 'adjusting': 710,\n",
       " 'looked': 711,\n",
       " 'rough': 712,\n",
       " 'used': 713,\n",
       " 'make': 714,\n",
       " 'sense': 715,\n",
       " 'Really': 716,\n",
       " 'headed': 717,\n",
       " 'direction': 718,\n",
       " 'They': 719,\n",
       " 'paging': 720,\n",
       " 'works': 721,\n",
       " 'Six': 722,\n",
       " 'hear': 723,\n",
       " 'eventually': 724,\n",
       " 'branch': 725,\n",
       " 'higher': 726,\n",
       " 'options': 727,\n",
       " 'goes': 728,\n",
       " 'wider': 729,\n",
       " 'range': 730,\n",
       " 'rooms': 731,\n",
       " 'house': 732,\n",
       " 'disappointed': 733,\n",
       " 'losing': 734,\n",
       " 'everything': 735,\n",
       " 'set-back': 736,\n",
       " 'W': 737,\n",
       " 'lost': 738,\n",
       " 'granola': 739,\n",
       " 'again': 740,\n",
       " 'own': 741,\n",
       " 'tellys': 742,\n",
       " 'anyway': 743,\n",
       " 'true': 744,\n",
       " 'unclear': 745,\n",
       " 'perfectly': 746,\n",
       " 'fair': 747,\n",
       " 'connected': 748,\n",
       " 'process': 749,\n",
       " 'report': 750,\n",
       " 'role': 751,\n",
       " 'satisfaction': 752,\n",
       " 'room': 753,\n",
       " 'creativity': 754,\n",
       " 'forth': 755,\n",
       " 'within': 756,\n",
       " 'mic': 757,\n",
       " \"'Kay\": 758,\n",
       " 'hope': 759,\n",
       " 'screwing': 760,\n",
       " 'charge': 761,\n",
       " 'trust': 762,\n",
       " 'she': 763,\n",
       " 'jump': 764,\n",
       " 'Whatever': 765,\n",
       " 'thoughts': 766,\n",
       " 'Are': 767,\n",
       " 'considering': 768,\n",
       " 'points': 769,\n",
       " 'starting': 770,\n",
       " 'blocks': 771,\n",
       " 'day': 772,\n",
       " 'worth': 773,\n",
       " 'relatively': 774,\n",
       " 'productive': 775,\n",
       " 'amount': 776,\n",
       " 'input': 777,\n",
       " 'help': 778,\n",
       " 'interesting': 779,\n",
       " 'whiteboard': 780,\n",
       " 'designing': 781,\n",
       " 'changed': 782,\n",
       " 'constraints': 783,\n",
       " 'intrigued': 784,\n",
       " 'pen': 785,\n",
       " 'woulda': 786,\n",
       " 'write': 787,\n",
       " 'barely': 788,\n",
       " 'notes': 789,\n",
       " 'often': 790,\n",
       " 'usual': 791,\n",
       " 'liked': 792,\n",
       " 'Was': 793,\n",
       " 'tack': 794,\n",
       " 'Definitely': 795,\n",
       " 'personal': 796,\n",
       " 'response': 797,\n",
       " 'three': 798,\n",
       " 'months': 799,\n",
       " 'weird': 800,\n",
       " 'creepy': 801,\n",
       " 'Attempts': 802,\n",
       " 'contact': 803,\n",
       " 'ineffective': 804,\n",
       " 'coaching': 805,\n",
       " 'n': 806,\n",
       " 'whatever': 807,\n",
       " 'until': 808,\n",
       " 'decent': 809,\n",
       " 'Organic': 810,\n",
       " 'brilliance': 811,\n",
       " 'p': 812,\n",
       " 'peeler': 813,\n",
       " 'highly': 814,\n",
       " 'resourceful': 815,\n",
       " 'mates': 816,\n",
       " 'always': 817,\n",
       " 'plus': 818,\n",
       " 're': 819,\n",
       " 'creative': 820,\n",
       " 'teamwork': 821,\n",
       " 'impressed': 822,\n",
       " 'prove': 823,\n",
       " 'wasteful': 824,\n",
       " 'waste': 825,\n",
       " 'single': 826,\n",
       " 'Play-Doh': 827,\n",
       " 'every': 828,\n",
       " 'four': 829,\n",
       " 'containers': 830,\n",
       " 'Including': 831,\n",
       " 'multi-coloured': 832,\n",
       " 'pattern': 833,\n",
       " 'My': 834,\n",
       " 'criticism': 835,\n",
       " 'You': 836,\n",
       " 'amazing': 837,\n",
       " 'roles': 838,\n",
       " 'never': 839,\n",
       " 'too': 840,\n",
       " 'track': 841,\n",
       " 'information': 842,\n",
       " 'filled': 843,\n",
       " 'gaps': 844,\n",
       " 'At': 845,\n",
       " 'Though': 846,\n",
       " 'th': 847,\n",
       " 'level': 848,\n",
       " 'severely': 849,\n",
       " 'presentation': 850,\n",
       " 'Nothing': 851,\n",
       " 'email': 852,\n",
       " 'slightly': 853,\n",
       " 'lacking': 854,\n",
       " 'fill': 855,\n",
       " 'blanks': 856,\n",
       " 'upped': 857,\n",
       " 'issue': 858,\n",
       " 'kept': 859,\n",
       " 'reporting': 860,\n",
       " 'each': 861,\n",
       " 'doing': 862,\n",
       " 'confused': 863,\n",
       " 'Uh-huh': 864,\n",
       " 'felt': 865,\n",
       " 'discussion': 866,\n",
       " 'specifically': 867,\n",
       " 'task': 868,\n",
       " 'portion': 869,\n",
       " 'Yes': 870,\n",
       " 'mm': 871,\n",
       " 'end': 872,\n",
       " 'jobs': 873,\n",
       " 'melded': 874,\n",
       " 'together': 875,\n",
       " 'helpful': 876,\n",
       " 'getting': 877,\n",
       " 'slides': 878,\n",
       " 'having': 879,\n",
       " 'formatted': 880,\n",
       " 'leadership': 881,\n",
       " 'She': 882,\n",
       " 'made': 883,\n",
       " 'comment': 884,\n",
       " 'gap': 885,\n",
       " 'boy': 886,\n",
       " 'slipped': 887,\n",
       " 'rest': 888,\n",
       " 'hey': 889,\n",
       " 'An': 890,\n",
       " 'taken': 891,\n",
       " 'management': 892,\n",
       " 'usually': 893,\n",
       " 'organise': 894,\n",
       " 'crap': 895,\n",
       " 'party': 896,\n",
       " 'friends': 897,\n",
       " 'Little': 898,\n",
       " 'suspension': 899,\n",
       " 'disbelief': 900,\n",
       " 'except': 901,\n",
       " 'moments': 902,\n",
       " 'hand': 903,\n",
       " 'knew': 904,\n",
       " 'lying': 905,\n",
       " 'teeth': 906,\n",
       " 'admit': 907,\n",
       " 'soon': 908,\n",
       " 'w': 909,\n",
       " 'concept': 910,\n",
       " 'trying': 911,\n",
       " 'window': 912,\n",
       " 'imagine': 913,\n",
       " 'Maybe': 914,\n",
       " 'Legos': 915,\n",
       " 'Be': 916,\n",
       " 'Possibly': 917,\n",
       " 'control': 918,\n",
       " 'spaceship': 919,\n",
       " 'build': 920,\n",
       " 'spaceships': 921,\n",
       " 'Totally': 922,\n",
       " \"'em\": 923,\n",
       " 'everybody': 924,\n",
       " 'knows': 925,\n",
       " 'minor': 926,\n",
       " 'actual': 927,\n",
       " 'building': 928,\n",
       " 'collaborative': 929,\n",
       " 'brainstorming': 930,\n",
       " 'board': 931,\n",
       " 'huh': 932,\n",
       " 'six': 933,\n",
       " \"'\": 934,\n",
       " 'hours': 935,\n",
       " 'serious': 936,\n",
       " 'brainstormed': 937,\n",
       " 'during': 938,\n",
       " 'Course': 939,\n",
       " 'conscious': 940,\n",
       " 'Project': 941,\n",
       " 'Manager': 942,\n",
       " 'asking': 943,\n",
       " 'hmm': 944,\n",
       " 'Interesting': 945,\n",
       " 'fascinating': 946,\n",
       " 'Wonder': 947,\n",
       " 'inform': 948,\n",
       " 'ri': 949,\n",
       " 'Mine': 950,\n",
       " 'mics': 951,\n",
       " 'dealing': 952,\n",
       " 'wires': 953,\n",
       " 'afraid': 954,\n",
       " 'loose': 955,\n",
       " 'possibility': 956,\n",
       " 'tripping': 957,\n",
       " 'tangled': 958,\n",
       " 'shown': 959,\n",
       " 'occur': 960,\n",
       " 'Nor': 961,\n",
       " 'consciously': 962,\n",
       " 'paper': 963,\n",
       " 'laptop': 964,\n",
       " 'Standard': 965,\n",
       " 'literally': 966,\n",
       " 'front': 967,\n",
       " 'output': 968,\n",
       " 'files': 969,\n",
       " 'digital': 970,\n",
       " 'professional': 971,\n",
       " 'wh': 972,\n",
       " 'handwriting': 973,\n",
       " 'digitized': 974,\n",
       " 'P_D_F_': 975,\n",
       " 'format': 976,\n",
       " 'Usually': 977,\n",
       " 'doodling': 978,\n",
       " 'draw': 979,\n",
       " 'entirely': 980,\n",
       " 'doodled': 981,\n",
       " 'less': 982,\n",
       " 'T': 983,\n",
       " 'curious': 984,\n",
       " 'de-briefing': 985,\n",
       " 'exactly': 986,\n",
       " 'found': 987,\n",
       " 'New': 988,\n",
       " 'file': 989,\n",
       " 'ch': 990,\n",
       " 'concepts': 991,\n",
       " 'none': 992,\n",
       " 'their': 993,\n",
       " 'thinking': 994,\n",
       " 'outside': 995,\n",
       " 'box': 996,\n",
       " 'circle': 997,\n",
       " 'alright': 998,\n",
       " 'wonder': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "\n",
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=50):\n",
    "    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, val_inputs, train_labels, val_labels])\n",
    "\n",
    "    # Specify batch_size\n",
    "    batch_size = 50\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687013642.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tuple(torch.tensor(data) for data in\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train Test Split\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader = \\\n",
    "data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_NLP(nn.Module):\n",
    "    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        The constructor for CNN_NLP class.\n",
    "\n",
    "        Args:\n",
    "            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
    "                shape (vocab_size, embed_dim)\n",
    "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
    "                vectors. Default: False\n",
    "            vocab_size (int): Need to be specified when not pretrained word\n",
    "                embeddings are not used.\n",
    "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
    "                when pretrained word embeddings are not used. Default: 300\n",
    "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
    "            num_filters (List[int]): List of number of filters, has the same\n",
    "                length as `filter_sizes`. Default: [100, 100, 100]\n",
    "            n_classes (int): Number of classes. Default: 2\n",
    "            dropout (float): Dropout rate. Default: 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN_NLP, self).__init__()\n",
    "        # Embedding layer\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
    "                (batch_size, max_sent_length)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
    "                n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
    "        x_embed = self.embedding(input_ids).float()\n",
    "\n",
    "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
    "        # Output shape: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        \n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        \n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def initialize_model(pretrained_embedding=None,\n",
    "                    freeze_embedding=False,\n",
    "                    vocab_size=None,\n",
    "                    embed_dim=300,\n",
    "                    filter_sizes=[3, 4, 5],\n",
    "                    num_filters=[100, 100, 100],\n",
    "                    num_classes=2,\n",
    "                    dropout=0.5,\n",
    "                    learning_rate=0.01):\n",
    "    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n",
    "\n",
    "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n",
    "    num_filters need to be of the same length.\"\n",
    "\n",
    "    # Instantiate CNN model\n",
    "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
    "                        freeze_embedding=freeze_embedding,\n",
    "                        vocab_size=vocab_size,\n",
    "                        embed_dim=embed_dim,\n",
    "                        filter_sizes=filter_sizes,\n",
    "                        num_filters=num_filters,\n",
    "                        num_classes=2,\n",
    "                        dropout=0.5)\n",
    "    \n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    cnn_model.to(device)\n",
    "\n",
    "    # Instantiate Adadelta optimizer\n",
    "    optimizer = optim.Adadelta(cnn_model.parameters(),\n",
    "                               lr=learning_rate,\n",
    "                               rho=0.95)\n",
    "\n",
    "    return cnn_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
    "    \"\"\"Train the CNN model.\"\"\"\n",
    "    \n",
    "    # Tracking best validation accuracy\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch'} | {'Train Loss'} | {'Val Loss'} | {'Val Acc'} | {'F1-score'}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if val_dataloader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_accuracy, f1_value = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Track the best accuracy\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1} | {avg_train_loss} | {val_loss} | {val_accuracy} | {f1_value}\")\n",
    "            \n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
    "    # during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    f1_value = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "        \n",
    "        preds_np = preds.cpu().numpy()\n",
    "        b_labels_np = b_labels.cpu().numpy()\n",
    "        f1_value.append(f1_score(b_labels_np, preds_np))\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    f1_value = np.mean(f1_value)\n",
    "\n",
    "    return val_loss, val_accuracy, f1_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-02 10:08:08--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.86.91.38, 99.86.91.123, 99.86.91.116, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.86.91.38|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1523785255 (1,4G) [application/zip]\n",
      "Saving to: ‘fastText/crawl-300d-2M.vec.zip’\n",
      "\n",
      "crawl-300d-2M.vec.z 100%[===================>]   1,42G  8,86MB/s    in 2m 3s   \n",
      "\n",
      "2023-12-02 10:10:11 (11,9 MB/s) - ‘fastText/crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
      "\n",
      "Archive:  fastText/crawl-300d-2M.vec.zip\n",
      "  inflating: fastText/crawl-300d-2M.vec  \n"
     ]
    }
   ],
   "source": [
    "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
    "FILE = \"fastText\"\n",
    "\n",
    "if os.path.isdir(FILE):\n",
    "    print(\"fastText exists.\")\n",
    "else:\n",
    "    !wget -P $FILE $URL\n",
    "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def load_pretrained_vectors(word2idx, fname):\n",
    "    \"\"\"Load pretrained vectors and create embedding layers.\n",
    "    \n",
    "    Args:\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        fname (str): Path to pretrained vector file\n",
    "\n",
    "    Returns:\n",
    "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
    "            the size of word2idx and d is embedding dimension\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading pretrained vectors...\")\n",
    "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "\n",
    "    # Initilize random embeddings\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
    "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
    "\n",
    "    # Load pretrained vectors\n",
    "    count = 0\n",
    "    for line in tqdm_notebook(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        word = tokens[0]\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
    "\n",
    "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to be optimized\n",
    "    embed_dim = trial.suggest_int('embed_dim', 50, 300)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    cnn_rand, optimizer = initialize_model(vocab_size=len(word2idx),\n",
    "                                           embed_dim=embed_dim,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           dropout=dropout)\n",
    "\n",
    "    # Train the model\n",
    "    train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=50)\n",
    "\n",
    "    # Evaluate the model and return the validation accuracy as the objective value\n",
    "    val_loss, val_accuracy, f1_value = evaluate(cnn_rand, val_dataloader)\n",
    "    print(val_loss)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 20:56:34,958] A new study created in memory with name: no-name-dce5ca9f-2f61-4f03-a3f7-1e673e0e1829\n",
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n",
      "1 | 0.5200359354006405 | 0.43928947167037286 | 81.69336143308746 | 0.0\n",
      "2 | 0.4330288100794003 | 0.41041451182267435 | 81.69336143308746 | 0.0\n",
      "3 | 0.41311916312493313 | 0.3930017733818864 | 81.69336143308746 | 0.0\n",
      "4 | 0.3993401284862184 | 0.3808402621174512 | 81.69336143308746 | 0.0\n",
      "5 | 0.39014101739323465 | 0.3721039127813627 | 81.66596417281349 | 0.0\n",
      "6 | 0.38362657022448854 | 0.36532236584653593 | 81.93782929399369 | 0.035430589197712484\n",
      "7 | 0.37878954359287514 | 0.3602353598769397 | 82.06111696522656 | 0.07186719092811379\n",
      "8 | 0.3741923388127887 | 0.35617724163075015 | 82.28029504741835 | 0.11404243137300095\n",
      "9 | 0.3716903121327406 | 0.35297195388846203 | 82.54056902002108 | 0.16474203118038735\n",
      "10 | 0.36844412699934176 | 0.3504187479002835 | 82.66385669125395 | 0.1840135567270942\n",
      "11 | 0.3663537341998805 | 0.348258155870111 | 82.88303477344574 | 0.21625504609992924\n",
      "12 | 0.3644258167811125 | 0.3464519156985087 | 82.86933614330876 | 0.2280532196592822\n",
      "13 | 0.3627470481331924 | 0.3448790047470837 | 82.93782929399369 | 0.25072753020977956\n",
      "14 | 0.36178125721325566 | 0.34364986378852636 | 82.86933614330876 | 0.24600531178893098\n",
      "15 | 0.3604590741957364 | 0.342483630021141 | 82.93782929399369 | 0.26046573545826746\n",
      "16 | 0.3590978130880481 | 0.3414790846713602 | 83.1159114857745 | 0.2752615291839352\n",
      "17 | 0.3586813097745636 | 0.34054106994442745 | 83.23919915700738 | 0.2965131345934778\n",
      "18 | 0.35784440460281636 | 0.33975365982480243 | 83.19810326659642 | 0.3031598811794712\n",
      "19 | 0.357041939831017 | 0.33905231289259374 | 83.23919915700738 | 0.3070056001037558\n",
      "20 | 0.35675908022471886 | 0.33839000281814025 | 83.25289778714438 | 0.3227763833229824\n",
      "21 | 0.3557375426595 | 0.3377952262350958 | 83.22550052687039 | 0.31980556225767054\n",
      "22 | 0.3555237930325011 | 0.3372852518542172 | 83.23919915700738 | 0.31977127386721776\n",
      "23 | 0.35413097756536 | 0.336803342687757 | 83.15700737618546 | 0.31620794422241727\n",
      "24 | 0.3538626134794331 | 0.33628844653498635 | 83.3213909378293 | 0.3340199864878927\n",
      "25 | 0.3533809710669955 | 0.33588175036727563 | 83.33508956796629 | 0.33326071017462766\n",
      "26 | 0.3523766388062855 | 0.3354630863217458 | 83.34878819810328 | 0.3364878533401971\n",
      "27 | 0.3536498633523782 | 0.3350859023120305 | 83.37618545837725 | 0.3387371350818237\n",
      "28 | 0.3509087131817224 | 0.33471378949407027 | 83.37618545837725 | 0.34122954772471104\n",
      "29 | 0.3524513541398974 | 0.33442026182805024 | 83.41728134878821 | 0.34063426427548793\n",
      "30 | 0.3507833824199639 | 0.3340584216256664 | 83.5268703898841 | 0.35815998235362745\n",
      "31 | 0.35030272336454565 | 0.33380267452703766 | 83.45837723919917 | 0.35088257823793495\n",
      "32 | 0.3498985180887607 | 0.3335573910033866 | 83.47207586933615 | 0.34379907555875217\n",
      "33 | 0.35079648353683474 | 0.33321041215772496 | 83.58166491043204 | 0.3626810351718109\n",
      "34 | 0.3493915881323614 | 0.33300838639883146 | 83.48577449947314 | 0.35529095630825797\n",
      "35 | 0.3476417201260726 | 0.3327140492731578 | 83.60695468914648 | 0.36535788347771464\n",
      "36 | 0.34852871034488037 | 0.33245812570803784 | 83.63435194942045 | 0.3675594176711697\n",
      "37 | 0.3480786462898284 | 0.33223818324200094 | 83.64805057955743 | 0.3674886510989611\n",
      "38 | 0.3474215692541468 | 0.3320102788610001 | 83.64805057955743 | 0.3692397187471194\n",
      "39 | 0.3488833917709301 | 0.33178478081340657 | 83.63435194942045 | 0.37730258585117\n",
      "40 | 0.3469636699680341 | 0.3315663378532619 | 83.68914646996839 | 0.3800490416516145\n",
      "41 | 0.34652771431645124 | 0.33136518595561587 | 83.68914646996839 | 0.3792156004793125\n",
      "42 | 0.34580214818318683 | 0.3311236417048598 | 83.68914646996839 | 0.38253974140852576\n",
      "43 | 0.34484482559999197 | 0.33097624788953833 | 83.77133825079031 | 0.3776342200905187\n",
      "44 | 0.3448900132004274 | 0.33072104131522245 | 83.70284510010538 | 0.3831944553488964\n",
      "45 | 0.3445818492749689 | 0.3305945188215334 | 83.77133825079031 | 0.37690139127549815\n",
      "46 | 0.3438201902337395 | 0.33033967487616084 | 83.77133825079031 | 0.38889262274698383\n",
      "47 | 0.3447594827878366 | 0.3302003759635638 | 83.81243414120127 | 0.3845931922106475\n",
      "48 | 0.34414197731227686 | 0.3300168422395236 | 83.83983140147524 | 0.3849611750202097\n",
      "49 | 0.34229074335380794 | 0.32981965854151607 | 83.7850368809273 | 0.3871733385841058\n",
      "50 | 0.3433830112856827 | 0.3296433805602871 | 83.85353003161222 | 0.3940209459413827\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.85%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 21:26:57,518] Trial 0 finished with value: 0.3296433805602871 and parameters: {'embed_dim': 158, 'learning_rate': 0.001144328670337944, 'dropout': 0.14915879947400185}. Best is trial 0 with value: 0.3296433805602871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3296433805602871\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.46146349733074504 | 0.395658381385346 | 81.69336143308746 | 0.0\n",
      "2 | 0.3936299792510836 | 0.364869539358028 | 81.92623814541624 | 0.05069140920056989\n",
      "3 | 0.37659517949054 | 0.3525843211028674 | 82.622760800843 | 0.17795496256998017\n",
      "4 | 0.3685237299252176 | 0.3463302149552189 | 82.95152792413067 | 0.27267321529896277\n",
      "5 | 0.3637129761027998 | 0.34269041006695733 | 83.03371970495259 | 0.2856560582496797\n",
      "6 | 0.36040919741177047 | 0.34020703571708233 | 83.10221285563752 | 0.2957234347416059\n",
      "7 | 0.35882947607598176 | 0.3382647628449414 | 83.17070600632245 | 0.3294955605551484\n",
      "8 | 0.3560879358321155 | 0.3374547915507669 | 83.03371970495259 | 0.2921962210264301\n",
      "9 | 0.3553425843516986 | 0.3359088094471252 | 83.34668071654373 | 0.3375912333251076\n",
      "10 | 0.3524854377891857 | 0.3352183923125267 | 83.16859852476291 | 0.3175756931992123\n",
      "11 | 0.3515952074108503 | 0.3341314350701358 | 83.44257112750263 | 0.361109153306111\n",
      "12 | 0.35104762968416425 | 0.3334876377492735 | 83.49736564805058 | 0.3552336944401503\n",
      "13 | 0.3492542911970287 | 0.33283468815561845 | 83.52476290832455 | 0.3595099321131605\n",
      "14 | 0.3483440634148019 | 0.33237138084352835 | 83.53846153846153 | 0.357226259120382\n",
      "15 | 0.34716988150998723 | 0.3317466134282008 | 83.5795574288725 | 0.38247524354528545\n",
      "16 | 0.34499066149878577 | 0.3313422126516904 | 83.59325605900949 | 0.36589095421170925\n",
      "17 | 0.34525397161323723 | 0.33083258814191163 | 83.62065331928346 | 0.3735500826202333\n",
      "18 | 0.3433364782972256 | 0.33041703435656145 | 83.52476290832455 | 0.3782523934708117\n",
      "19 | 0.34374818560134746 | 0.33041154705498316 | 83.5795574288725 | 0.35856668378700585\n",
      "20 | 0.3419511589002354 | 0.3296504604489836 | 83.68914646996839 | 0.41691906070218365\n",
      "21 | 0.34162334712729175 | 0.3293344986765352 | 83.52476290832455 | 0.38879957462372255\n",
      "22 | 0.34103534091548815 | 0.3292500147060172 | 83.64805057955743 | 0.37181756073208283\n",
      "23 | 0.3405305488599003 | 0.3287515546361061 | 83.75763962065332 | 0.4139743661725858\n",
      "24 | 0.3376763702145228 | 0.32842247749436393 | 83.71654373024236 | 0.3964126255581611\n",
      "25 | 0.33785377027016894 | 0.3281817529095362 | 83.71654373024236 | 0.3911337647615726\n",
      "26 | 0.33662428076888806 | 0.32780726458111853 | 83.93572181243414 | 0.4279686855094113\n",
      "27 | 0.335956446826458 | 0.3275438094996426 | 83.92202318229715 | 0.4123853072111091\n",
      "28 | 0.33597702839460214 | 0.32730282577749803 | 83.9768177028451 | 0.4257053262032488\n",
      "29 | 0.3340820298054532 | 0.32704890294842526 | 84.00421496311907 | 0.4293751723388484\n",
      "30 | 0.33368306010050147 | 0.32684946876682647 | 83.9768177028451 | 0.4169588188560614\n",
      "31 | 0.3328274786928014 | 0.3265652450388425 | 84.086406743941 | 0.42948699972242255\n",
      "32 | 0.33338660163525774 | 0.3263794214349903 | 84.04531085353003 | 0.4248440745695938\n",
      "33 | 0.33220880790770235 | 0.32618605171980924 | 84.11380400421497 | 0.4276907712519069\n",
      "34 | 0.33165988224681 | 0.3259036761440643 | 84.04531085353003 | 0.43440470715937723\n",
      "35 | 0.33111413667371514 | 0.3257973780983115 | 84.05900948366701 | 0.4400548157486546\n",
      "36 | 0.33026395198698255 | 0.32558597675333284 | 84.04531085353003 | 0.4376754397803744\n",
      "37 | 0.3289095129168362 | 0.32553059301555975 | 84.05900948366701 | 0.4440215400352955\n",
      "38 | 0.3296509006314139 | 0.3254236602415777 | 84.1822971548999 | 0.43245702682131354\n",
      "39 | 0.32987507755102 | 0.32520523463210016 | 84.10010537407798 | 0.4408488780583272\n",
      "40 | 0.3280843339678162 | 0.325081565200466 | 84.05900948366701 | 0.4402690187556637\n",
      "41 | 0.327282902274839 | 0.32494181517052323 | 84.12750263435196 | 0.4506989216465487\n",
      "42 | 0.32677627916501933 | 0.32485260763396956 | 84.10010537407798 | 0.4566304445066623\n",
      "43 | 0.32622639882728593 | 0.324762627584477 | 84.07270811380401 | 0.4577076387871629\n",
      "44 | 0.32641551725634743 | 0.3245604601624894 | 84.12750263435196 | 0.45118710140812857\n",
      "45 | 0.3241563687152272 | 0.32438872652511075 | 84.086406743941 | 0.44592337619682443\n",
      "46 | 0.32558659469817025 | 0.3243658712057218 | 84.22339304531086 | 0.4461133627915567\n",
      "47 | 0.32415150836201256 | 0.32421586917687767 | 84.1822971548999 | 0.45299092688556186\n",
      "48 | 0.3236095256852812 | 0.3241566008288566 | 84.19599578503689 | 0.4481318348336727\n",
      "49 | 0.3244505856949437 | 0.3240438783413743 | 84.1822971548999 | 0.45128488457266747\n",
      "50 | 0.32261121531076936 | 0.32421940371190033 | 84.36037934668072 | 0.4399245788729578\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.36%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 21:50:26,737] Trial 1 finished with value: 0.32421940371190033 and parameters: {'embed_dim': 116, 'learning_rate': 0.0041854756888207644, 'dropout': 0.3785499268554985}. Best is trial 1 with value: 0.32421940371190033.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32421940371190033\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.5540863416109245 | 0.480059100544616 | 81.69336143308746 | 0.0\n",
      "2 | 0.4656934118188849 | 0.4419757799334722 | 81.69336143308746 | 0.0\n",
      "3 | 0.4413221803827023 | 0.42540478359346523 | 81.69336143308746 | 0.0\n",
      "4 | 0.4283715053276367 | 0.41365924702115253 | 81.69336143308746 | 0.0\n",
      "5 | 0.4200571578121331 | 0.40433829481879324 | 81.69336143308746 | 0.0\n",
      "6 | 0.4110878276715585 | 0.39647905324419885 | 81.69336143308746 | 0.0\n",
      "7 | 0.40480062652618515 | 0.38990040756251715 | 81.69336143308746 | 0.0\n",
      "8 | 0.3981602099761139 | 0.3842666558411023 | 81.69336143308746 | 0.0\n",
      "9 | 0.39476724269740077 | 0.37936494417794764 | 81.69336143308746 | 0.0\n",
      "10 | 0.3893385830845126 | 0.37510935941787615 | 81.70706006322445 | 0.0012453300124533001\n",
      "11 | 0.3859927215206149 | 0.3713587539857381 | 81.81664910432033 | 0.016852135687752123\n",
      "12 | 0.38353920877207676 | 0.36806452968349196 | 81.87144362486828 | 0.022815882747389597\n",
      "13 | 0.3803139748422013 | 0.3651534296470146 | 81.85774499473129 | 0.03543191967849502\n",
      "14 | 0.3787367711191148 | 0.3625887497647168 | 81.9673340358272 | 0.0541126603274065\n",
      "15 | 0.3749145954984044 | 0.36025329975232684 | 82.13171759747102 | 0.0831990419822813\n",
      "16 | 0.37419402151616343 | 0.35821746320348896 | 82.21390937829294 | 0.09668793691759042\n",
      "17 | 0.37244497163261114 | 0.3563740444305825 | 82.28240252897787 | 0.11730261264857728\n",
      "18 | 0.370658339163579 | 0.35468766613774105 | 82.36248682824026 | 0.14004240531181877\n",
      "19 | 0.37020809433511276 | 0.35316410599506065 | 82.45837723919917 | 0.1592892364643711\n",
      "20 | 0.36877502668478074 | 0.35177920156554 | 82.51317175974711 | 0.1738508200264424\n",
      "21 | 0.36640007521626783 | 0.35053725591669344 | 82.58166491043204 | 0.18444682099735324\n",
      "22 | 0.3657121561376509 | 0.3493760005660253 | 82.54056902002108 | 0.19441759768427566\n",
      "23 | 0.36462128867568955 | 0.3483335766686152 | 82.59536354056902 | 0.20670117547794717\n",
      "24 | 0.3635508707619133 | 0.3473764802903345 | 82.622760800843 | 0.2135958169255807\n",
      "25 | 0.3629160198619424 | 0.34646129802073516 | 82.80084299262383 | 0.23203346814346007\n",
      "26 | 0.3626135493363809 | 0.345645131109512 | 82.85563751317177 | 0.23908286851477825\n",
      "27 | 0.36112341022728417 | 0.34486034665613957 | 82.96522655426766 | 0.2530025266554363\n",
      "28 | 0.36043338907102196 | 0.3441593495950307 | 82.93782929399369 | 0.2559965645809811\n",
      "29 | 0.35949167311875097 | 0.3434734421029483 | 82.99262381454163 | 0.2639976790990174\n",
      "30 | 0.36039211586208886 | 0.3428358396846954 | 83.03371970495259 | 0.27369682533032563\n",
      "31 | 0.3594227327028181 | 0.3422396935216368 | 83.10221285563752 | 0.2806123876473874\n",
      "32 | 0.3582692750891961 | 0.34169535920636296 | 83.12961011591149 | 0.2844815017904742\n",
      "33 | 0.35751984545260396 | 0.3411535100169378 | 83.18440463645943 | 0.29063706951936985\n",
      "34 | 0.35707850926285123 | 0.3406516257418345 | 83.19810326659642 | 0.2923865790750841\n",
      "35 | 0.3573424320451528 | 0.34019393014581234 | 83.18440463645943 | 0.2920060615712789\n",
      "36 | 0.35695982509113233 | 0.33973568422745354 | 83.18440463645943 | 0.2961187147042698\n",
      "37 | 0.35565537463771096 | 0.3393128107059492 | 83.17070600632245 | 0.295152785656149\n",
      "38 | 0.3567450265271948 | 0.33891190761980944 | 83.25079030558483 | 0.30398002664442314\n",
      "39 | 0.35608593797146 | 0.33852300915407807 | 83.34668071654373 | 0.31179112230542944\n",
      "40 | 0.3548332710126671 | 0.33814656499722234 | 83.42887249736565 | 0.31926739189491415\n",
      "41 | 0.35485189755529806 | 0.3377841937623612 | 83.45626975763962 | 0.32202317046218765\n",
      "42 | 0.3549586809283003 | 0.3374374083256068 | 83.56585879873552 | 0.3304607054754155\n",
      "43 | 0.35410182621181924 | 0.3371221237394908 | 83.5795574288725 | 0.32964829713883315\n",
      "44 | 0.3529189519060132 | 0.33679577432674906 | 83.63435194942045 | 0.3352184339199821\n",
      "45 | 0.35333621442682517 | 0.3364883803137361 | 83.64805057955743 | 0.33832653206232677\n",
      "46 | 0.3521589769295414 | 0.3362030121561599 | 83.63435194942045 | 0.3362468025312635\n",
      "47 | 0.3521447057183547 | 0.33590964064614415 | 83.66174920969442 | 0.3393758822555126\n",
      "48 | 0.3503408827890132 | 0.3356232516569634 | 83.66174920969442 | 0.34003412811923794\n",
      "49 | 0.3517182658822347 | 0.33535559924498 | 83.68914646996839 | 0.3435072148720255\n",
      "50 | 0.3503324721202209 | 0.33508303455294 | 83.62065331928346 | 0.3436502225492798\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.69%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 22:30:41,896] Trial 2 finished with value: 0.33508303455294 and parameters: {'embed_dim': 256, 'learning_rate': 0.0005659086934083479, 'dropout': 0.10717660175643787}. Best is trial 1 with value: 0.32421940371190033.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33508303455294\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.4586722717395434 | 0.3974175755291769 | 81.69336143308746 | 0.0\n",
      "2 | 0.3952835505594719 | 0.3662601250083479 | 81.76185458377239 | 0.02155458696554587\n",
      "3 | 0.37722850675247493 | 0.353125906562152 | 82.38988408851424 | 0.14126194418085505\n",
      "4 | 0.36914461800993764 | 0.34573885951548405 | 82.86933614330876 | 0.25011001498519403\n",
      "5 | 0.36449229275232425 | 0.3419631771641235 | 82.85563751317177 | 0.25230465657028905\n",
      "6 | 0.36085052763194486 | 0.3390368410985764 | 83.12961011591149 | 0.3029736654563324\n",
      "7 | 0.35789187509988063 | 0.33702707760138056 | 83.22550052687039 | 0.32592496559850986\n",
      "8 | 0.3565311583174843 | 0.33572159666721135 | 83.26659641728136 | 0.32779818605327943\n",
      "9 | 0.3546247517160319 | 0.3346960469059748 | 83.31928345626976 | 0.3303912550847046\n",
      "10 | 0.3528745370925567 | 0.3337719789514803 | 83.34668071654373 | 0.33824479200351154\n",
      "11 | 0.35184792121615976 | 0.33286461642343707 | 83.51106427818756 | 0.35338639887380296\n",
      "12 | 0.3501253665541655 | 0.33225055743161946 | 83.56585879873552 | 0.35472727566453194\n",
      "13 | 0.34942805074405964 | 0.3313985277120381 | 83.66174920969442 | 0.37129426350733724\n",
      "14 | 0.346710752069221 | 0.33103736867643385 | 83.64805057955743 | 0.36440306599047706\n",
      "15 | 0.34647426792970126 | 0.33019012845542334 | 83.74394099051634 | 0.395830073290152\n",
      "16 | 0.345464872452644 | 0.32971128800960436 | 83.8809272918862 | 0.4177604378177087\n",
      "17 | 0.3443906612079078 | 0.32924000306488715 | 83.94942044257112 | 0.4194655722792154\n",
      "18 | 0.34329661585686766 | 0.3288202931823796 | 83.89462592202318 | 0.4082061299255072\n",
      "19 | 0.3412355478021348 | 0.32840972118181727 | 83.86722866174921 | 0.4109515798526654\n",
      "20 | 0.342268817058397 | 0.3281054499827019 | 83.94942044257112 | 0.4122699994471555\n",
      "21 | 0.34278315866212233 | 0.3278333094634422 | 83.89462592202318 | 0.42068199809542517\n",
      "22 | 0.3404265049879157 | 0.3276034360879088 | 83.94942044257112 | 0.4072450010254905\n",
      "23 | 0.3381810854402704 | 0.327139908014095 | 83.93572181243414 | 0.4154477732390512\n",
      "24 | 0.3378531403543388 | 0.326824609241257 | 83.96311907270811 | 0.4168789557641096\n",
      "25 | 0.3377951140706328 | 0.32652270375457526 | 83.92202318229715 | 0.42068504337860935\n",
      "26 | 0.33603579856295834 | 0.32623173480164513 | 84.01791359325605 | 0.4196998122852928\n",
      "27 | 0.3356603769192455 | 0.3259922658948049 | 83.99051633298208 | 0.423550877200228\n",
      "28 | 0.33630517217407535 | 0.32585219548989647 | 84.01791359325605 | 0.440563675314396\n",
      "29 | 0.33416189341051133 | 0.3255447093142222 | 84.07270811380401 | 0.4349757665382689\n",
      "30 | 0.3336951515212974 | 0.32530056093245335 | 84.04531085353003 | 0.4263855139414406\n",
      "31 | 0.3337240780302144 | 0.3251244938128615 | 84.04531085353003 | 0.43958301769485375\n",
      "32 | 0.3339720148969134 | 0.32504911955497034 | 83.99051633298208 | 0.4142926259966882\n",
      "33 | 0.3328961006704638 | 0.32479249798271753 | 84.01791359325605 | 0.4280038968779456\n",
      "34 | 0.3317401189391219 | 0.3247636792390314 | 83.90832455216017 | 0.4110084957139575\n",
      "35 | 0.33075746797617606 | 0.32465855573138147 | 84.16859852476291 | 0.4557671721488248\n",
      "36 | 0.3286235626524924 | 0.3242844946376265 | 84.14120126448894 | 0.4322455338565738\n",
      "37 | 0.3298145771277243 | 0.32422898679155193 | 83.96311907270811 | 0.41762549478568317\n",
      "38 | 0.3295250820502958 | 0.32403033767661005 | 84.04531085353003 | 0.4306989729324949\n",
      "39 | 0.3282657421168384 | 0.3238889990399962 | 84.1822971548999 | 0.4440892325206399\n",
      "40 | 0.32774527675674414 | 0.32373884585622237 | 84.05900948366701 | 0.4349501884679061\n",
      "41 | 0.3272014540998943 | 0.32374501218126245 | 83.9768177028451 | 0.4214769800407479\n",
      "42 | 0.3264433223446575 | 0.32360825646821767 | 83.94942044257112 | 0.4197339759534968\n",
      "43 | 0.32662891054281035 | 0.323366434300599 | 84.29188619599579 | 0.4507003108724622\n",
      "44 | 0.32539738413937597 | 0.32327307393289595 | 84.1822971548999 | 0.4385884253685754\n",
      "45 | 0.324883522391775 | 0.3231508674686902 | 84.25079030558483 | 0.44648103354734914\n",
      "46 | 0.32437987779395294 | 0.32307502760054313 | 84.36037934668072 | 0.4596832118414848\n",
      "47 | 0.32460293766190884 | 0.32303806210625663 | 84.19599578503689 | 0.43859738503687873\n",
      "48 | 0.32371795366572315 | 0.32309606844839983 | 84.00421496311907 | 0.4235383432239504\n",
      "49 | 0.3227100085569631 | 0.32282706625657537 | 84.33298208640674 | 0.4584582826706447\n",
      "50 | 0.3221443245466514 | 0.3227192132848583 | 84.34668071654373 | 0.4524970221566043\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.36%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 22:50:37,292] Trial 3 finished with value: 0.3227192128766073 and parameters: {'embed_dim': 122, 'learning_rate': 0.004050806327280356, 'dropout': 0.18959675433109235}. Best is trial 3 with value: 0.3227192128766073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3227192128766073\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.4848031878403021 | 0.4029780034333059 | 81.69336143308746 | 0.0\n",
      "2 | 0.40350654104935285 | 0.3716208556131141 | 81.72075869336143 | 0.006285856457089334\n",
      "3 | 0.38128883579592093 | 0.3570179369759886 | 82.33508956796629 | 0.12041471623833046\n",
      "4 | 0.37185612954313235 | 0.34920283753986225 | 82.77344573234986 | 0.2146704720225373\n",
      "5 | 0.3677726605571009 | 0.3447805210335614 | 83.1159114857745 | 0.2531187060359712\n",
      "6 | 0.36560497909146344 | 0.34175787794671647 | 83.30558482613277 | 0.2926368504353935\n",
      "7 | 0.36281958736183084 | 0.3397327147118033 | 83.34668071654373 | 0.30891477324297056\n",
      "8 | 0.3601879965206352 | 0.3381024372496017 | 83.40147523709167 | 0.32865049793267975\n",
      "9 | 0.3582408008157113 | 0.33695641496818357 | 83.4699683877766 | 0.334640366019647\n",
      "10 | 0.3566227687312011 | 0.335784863119256 | 83.41517386722866 | 0.35559859581213676\n",
      "11 | 0.3547699435611201 | 0.3350502247475598 | 83.51106427818756 | 0.34789509678160946\n",
      "12 | 0.35373161883529175 | 0.33418666244777917 | 83.44257112750263 | 0.3684076218688344\n",
      "13 | 0.35355432203968 | 0.33356733395628735 | 83.45626975763962 | 0.36630537837859733\n",
      "14 | 0.3532376394404913 | 0.3331303344401595 | 83.42887249736565 | 0.36590114924753503\n",
      "15 | 0.3512645611818595 | 0.33270743540296815 | 83.51106427818756 | 0.3628119049182042\n",
      "16 | 0.3487711816069183 | 0.3321684250888759 | 83.51106427818756 | 0.3804707250030362\n",
      "17 | 0.3492710329592228 | 0.3317679892459961 | 83.71654373024236 | 0.4006017343798787\n",
      "18 | 0.3480493606093827 | 0.33129337031955586 | 83.64805057955743 | 0.39374231136466026\n",
      "19 | 0.3486958618079304 | 0.33096807643975296 | 83.62065331928346 | 0.3852075231159317\n",
      "20 | 0.34527144942785803 | 0.33060002571915925 | 83.60695468914648 | 0.38902463334923854\n",
      "21 | 0.34449146636719 | 0.33022968236305944 | 83.81243414120127 | 0.4116160684255118\n",
      "22 | 0.34415890688834205 | 0.3299900945531179 | 83.6754478398314 | 0.38461888100421815\n",
      "23 | 0.3441519133222577 | 0.3296138457443616 | 83.70284510010538 | 0.3975607473476283\n",
      "24 | 0.34331758757796854 | 0.32941866155764826 | 83.7850368809273 | 0.3912714599448501\n",
      "25 | 0.34214042073588485 | 0.32916211653245636 | 83.86722866174921 | 0.39982108320141596\n",
      "26 | 0.3429621820044718 | 0.32891558610821425 | 83.93572181243414 | 0.4087565586346224\n",
      "27 | 0.3422474286288296 | 0.32892610418469936 | 83.79873551106428 | 0.3864848002352221\n",
      "28 | 0.3407162682520687 | 0.32850335201580233 | 83.93572181243414 | 0.40597145088151276\n",
      "29 | 0.3401250884119159 | 0.3284000018483972 | 83.8809272918862 | 0.3966186835329376\n",
      "30 | 0.34001715584951436 | 0.3280643498244351 | 83.96311907270811 | 0.4096928393091016\n",
      "31 | 0.33989766510118036 | 0.32789911860472537 | 84.07270811380401 | 0.43543512332834333\n",
      "32 | 0.33936083883001533 | 0.3276919597019888 | 83.96311907270811 | 0.42397265630149816\n",
      "33 | 0.338632865487071 | 0.32752081517079107 | 84.03161222339304 | 0.4182063547530424\n",
      "34 | 0.337669004096624 | 0.3273772092102325 | 83.99051633298208 | 0.41390903787461525\n",
      "35 | 0.33594100043997854 | 0.32711848371649443 | 84.05900948366701 | 0.4263815578409819\n",
      "36 | 0.3364935366932405 | 0.32695591398706175 | 84.15489989462593 | 0.43250214367932877\n",
      "37 | 0.33540557670073773 | 0.32686395157281667 | 84.03161222339304 | 0.41915589764456757\n",
      "38 | 0.33494701823191175 | 0.32663612333062575 | 84.16859852476291 | 0.4367012953318288\n",
      "39 | 0.33492878911054097 | 0.32651694741559356 | 84.12750263435196 | 0.4271503724103219\n",
      "40 | 0.334608169517477 | 0.32640223347977415 | 84.1822971548999 | 0.4309825027889555\n",
      "41 | 0.33234494010880816 | 0.3261997495612053 | 84.25079030558483 | 0.43633505422223556\n",
      "42 | 0.33275595362033317 | 0.32602986213687346 | 84.25079030558483 | 0.4483785299150159\n",
      "43 | 0.3334560281844861 | 0.32594036280292354 | 84.19599578503689 | 0.4340855614043344\n",
      "44 | 0.3329373585681121 | 0.32581469784044237 | 84.2781875658588 | 0.45402941121045204\n",
      "45 | 0.33231139906457074 | 0.32568631953980826 | 84.2781875658588 | 0.4478875038296244\n",
      "46 | 0.3302833005466957 | 0.325556727917227 | 84.29188619599579 | 0.45523864531654346\n",
      "47 | 0.33031824525069753 | 0.3256904613481809 | 84.11380400421497 | 0.4194417720069507\n",
      "48 | 0.3305521274629171 | 0.325303172411984 | 84.30558482613277 | 0.4531752283860839\n",
      "49 | 0.3302422444526938 | 0.32519324256540977 | 84.3740779768177 | 0.4485269762623239\n",
      "50 | 0.3285061707919526 | 0.3250490250448658 | 84.30558482613277 | 0.44356164475483617\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.37%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 23:07:29,169] Trial 4 finished with value: 0.3250490250448658 and parameters: {'embed_dim': 104, 'learning_rate': 0.0034180332110263336, 'dropout': 0.1356786279369077}. Best is trial 3 with value: 0.3227192128766073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3250490250448658\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.45292253872032195 | 0.3943447598243413 | 81.69336143308746 | 0.0\n",
      "2 | 0.3899733851968513 | 0.3659796199365838 | 81.9536354056902 | 0.0441226961774907\n",
      "3 | 0.3721262121556002 | 0.3533602485509768 | 82.7481559536354 | 0.1833578077068047\n",
      "4 | 0.3649245204364123 | 0.34695389223833606 | 83.00632244467862 | 0.24744466567855836\n",
      "5 | 0.35904583504668436 | 0.3429629245441254 | 83.18440463645943 | 0.28112009049196157\n",
      "6 | 0.35776100542384914 | 0.3404143584305293 | 83.37618545837725 | 0.30580217791797915\n",
      "7 | 0.3541383137214439 | 0.3382361791517636 | 83.56796628029505 | 0.3491684107153104\n",
      "8 | 0.3527234992530732 | 0.3366172650497254 | 83.63645943097998 | 0.3572965749675681\n",
      "9 | 0.3506717373950219 | 0.33533749800838836 | 83.66174920969442 | 0.36991288689216284\n",
      "10 | 0.34734589752345274 | 0.3342351546026256 | 83.82613277133825 | 0.39002615808965696\n",
      "11 | 0.34594632901638656 | 0.33315061716592476 | 84.01791359325605 | 0.39765091537194847\n",
      "12 | 0.34497183294744665 | 0.332336168276937 | 83.86722866174921 | 0.37350947722049405\n",
      "13 | 0.3425842951014866 | 0.3315626489993644 | 83.92202318229715 | 0.37779677824429786\n",
      "14 | 0.34146946354196706 | 0.33070134320487715 | 83.99051633298208 | 0.3941342948516521\n",
      "15 | 0.3412369322658314 | 0.33022077259135574 | 83.79873551106428 | 0.3782793249114792\n",
      "16 | 0.33762757447106756 | 0.32920866388164155 | 84.03161222339304 | 0.4125296614566184\n",
      "17 | 0.33779375579789145 | 0.32872600526842355 | 83.96311907270811 | 0.39803967560984677\n",
      "18 | 0.33626297794122945 | 0.3280593697747139 | 84.03161222339304 | 0.41490122097277693\n",
      "19 | 0.336315884263417 | 0.32760652042415045 | 84.05900948366701 | 0.4090144343231652\n",
      "20 | 0.33406146031317363 | 0.3271049086565841 | 84.04531085353003 | 0.42321778843003155\n",
      "21 | 0.33361288752486584 | 0.32662291181822345 | 84.04531085353003 | 0.41793222220009046\n",
      "22 | 0.3320948253599329 | 0.3262676162874862 | 84.086406743941 | 0.41519472171024674\n",
      "23 | 0.3317473584427199 | 0.3258703446959796 | 84.05900948366701 | 0.41785406446681583\n",
      "24 | 0.33035959219075856 | 0.32544673550618836 | 84.07270811380401 | 0.42822259006859986\n",
      "25 | 0.3285090812120233 | 0.32512673572318196 | 84.1822971548999 | 0.42669804102030084\n",
      "26 | 0.327271916678557 | 0.32475128688224375 | 84.11380400421497 | 0.4303210306944378\n",
      "27 | 0.3273210027098473 | 0.3245130206828248 | 84.12750263435196 | 0.42856491716407596\n",
      "28 | 0.3251256971400721 | 0.324198793058526 | 84.11380400421497 | 0.4364453777782698\n",
      "29 | 0.3253250344448498 | 0.3239158639964992 | 84.15489989462593 | 0.4355743013001913\n",
      "30 | 0.324002629641546 | 0.3236388995835226 | 84.15489989462593 | 0.437487665078032\n",
      "31 | 0.3221167081152445 | 0.32339750883513935 | 84.26448893572181 | 0.45486589380217646\n",
      "32 | 0.322463598763086 | 0.32316683202165447 | 84.22339304531086 | 0.44840751074353835\n",
      "33 | 0.32096186492147794 | 0.32299844624653257 | 84.2781875658588 | 0.4461633854730973\n",
      "34 | 0.31946309208186396 | 0.3227821539526116 | 84.14120126448894 | 0.43323746060010093\n",
      "35 | 0.31984166380599005 | 0.32247035521758743 | 84.31928345626976 | 0.44979059797242704\n",
      "36 | 0.31979359452832 | 0.32228047104731 | 84.42887249736565 | 0.4606124190280366\n",
      "37 | 0.31816771114957076 | 0.32214288382905804 | 84.4699683877766 | 0.47111988385053766\n",
      "38 | 0.3162813806729761 | 0.32200862579557993 | 84.31928345626976 | 0.4482789503116636\n",
      "39 | 0.3164619804453959 | 0.3218063549840287 | 84.25079030558483 | 0.44250792057863386\n",
      "40 | 0.31634127978657 | 0.321609910220316 | 84.40147523709167 | 0.4608530963940946\n",
      "41 | 0.3155126895425152 | 0.3215335985160854 | 84.25079030558483 | 0.4459642955691853\n",
      "42 | 0.3142247035962726 | 0.3213411569595337 | 84.41517386722866 | 0.4612741996345498\n",
      "43 | 0.3145297262646736 | 0.32130839787933924 | 84.49736564805058 | 0.4766956535080859\n",
      "44 | 0.312901642150744 | 0.32116395765787936 | 84.31928345626976 | 0.45520962990633856\n",
      "45 | 0.31096540951947554 | 0.3210196074557631 | 84.55216016859852 | 0.47856899308661466\n",
      "46 | 0.31219004020550567 | 0.32091302445081815 | 84.42887249736565 | 0.4644850131345252\n",
      "47 | 0.30998706955474087 | 0.3208076088395837 | 84.42887249736565 | 0.4598785829615676\n",
      "48 | 0.31006994497156287 | 0.3206946090476154 | 84.42887249736565 | 0.4736697963384532\n",
      "49 | 0.30955875139763234 | 0.3205452128632428 | 84.44257112750263 | 0.47225282767385346\n",
      "50 | 0.30844758933449923 | 0.32049177923839384 | 84.38777660695469 | 0.4742952786826755\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.55%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 23:44:44,016] Trial 5 finished with value: 0.32049177923839384 and parameters: {'embed_dim': 249, 'learning_rate': 0.0038632388883378267, 'dropout': 0.46457156176445746}. Best is trial 5 with value: 0.32049177923839384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32049177923839384\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.6056884326017961 | 0.5063992058985853 | 81.69336143308746 | 0.0\n",
      "2 | 0.48549601478952153 | 0.45531962015857436 | 81.69336143308746 | 0.0\n",
      "3 | 0.4553481930302917 | 0.43800359474469536 | 81.69336143308746 | 0.0\n",
      "4 | 0.443146441895116 | 0.4263145780726655 | 81.69336143308746 | 0.0\n",
      "5 | 0.4320692472068724 | 0.41677152967616304 | 81.69336143308746 | 0.0\n",
      "6 | 0.42312584860700353 | 0.4085987618524734 | 81.69336143308746 | 0.0\n",
      "7 | 0.4165001994494451 | 0.4015725404024124 | 81.69336143308746 | 0.0\n",
      "8 | 0.4109396011389177 | 0.3955072595240319 | 81.69336143308746 | 0.0\n",
      "9 | 0.40579251394769467 | 0.390138812261085 | 81.69336143308746 | 0.0\n",
      "10 | 0.40117064015854387 | 0.3854201526878631 | 81.69336143308746 | 0.0\n",
      "11 | 0.3965421140945833 | 0.38124463839890205 | 81.69336143308746 | 0.0\n",
      "12 | 0.3932027472838167 | 0.377491721653775 | 81.67966280295047 | 0.0\n",
      "13 | 0.3890948333871474 | 0.3741253064307448 | 81.70706006322445 | 0.005572851805728518\n",
      "14 | 0.38683843766467285 | 0.3711346165366369 | 81.70706006322445 | 0.006378653578492417\n",
      "15 | 0.38430361552249404 | 0.3683820063733075 | 81.88303477344574 | 0.023376281252993583\n",
      "16 | 0.3826702683480508 | 0.3659330421726998 | 81.88303477344574 | 0.03200098987770221\n",
      "17 | 0.38059630832814295 | 0.3637312364904848 | 82.07481559536355 | 0.06001167781989699\n",
      "18 | 0.378513553547203 | 0.36170534575230456 | 82.22550052687039 | 0.08237015127786577\n",
      "19 | 0.37659430532326027 | 0.35986850135130427 | 82.3213909378293 | 0.09814098622678293\n",
      "20 | 0.37511980556384505 | 0.3582007036837813 | 82.38988408851424 | 0.1160109713128289\n",
      "21 | 0.3739198354793433 | 0.356672770662667 | 82.44467860906218 | 0.13090916492407226\n",
      "22 | 0.3712600387564493 | 0.35527722312979504 | 82.56796628029505 | 0.1473856362735238\n",
      "23 | 0.3723057024522659 | 0.3540131116974844 | 82.66385669125395 | 0.1662234812819019\n",
      "24 | 0.3688090030069745 | 0.3528517881689006 | 82.77344573234986 | 0.17644702360233128\n",
      "25 | 0.36986734351251466 | 0.3517610225571345 | 82.86933614330876 | 0.19536654292505115\n",
      "26 | 0.3685179798541995 | 0.35081449202070497 | 82.8282402528978 | 0.19871242922982035\n",
      "27 | 0.3677325305907734 | 0.34982514738628306 | 82.93782929399369 | 0.2209977096735441\n",
      "28 | 0.36835882069958825 | 0.34902573029880657 | 82.99262381454163 | 0.22749418123850884\n",
      "29 | 0.36594001784783986 | 0.3482124086928694 | 83.04741833508957 | 0.2405288254375366\n",
      "30 | 0.3649172500596134 | 0.34747355482349657 | 82.97892518440464 | 0.24238918193125328\n",
      "31 | 0.36412651267255847 | 0.3467461054455744 | 83.03161222339304 | 0.25101765663810094\n",
      "32 | 0.3633882219159822 | 0.34612034628652544 | 83.07270811380401 | 0.25374673658313573\n",
      "33 | 0.3630281897540851 | 0.34554059681010574 | 83.086406743941 | 0.2565225896654056\n",
      "34 | 0.3630549981734439 | 0.3449623732738299 | 83.16859852476291 | 0.26669082376436193\n",
      "35 | 0.3621851718936127 | 0.34442086058528454 | 83.15489989462593 | 0.2732063922736767\n",
      "36 | 0.36234622772224817 | 0.3439180594805169 | 83.14120126448894 | 0.2777628327890213\n",
      "37 | 0.3622257289126379 | 0.34346713860557504 | 83.11380400421497 | 0.2786386296147884\n",
      "38 | 0.36219433167613246 | 0.3430177562244951 | 83.23709167544784 | 0.289310021240631\n",
      "39 | 0.36041034383283477 | 0.3426135859056695 | 83.23709167544784 | 0.2926922454411681\n",
      "40 | 0.3609765458749522 | 0.34219925932280004 | 83.22339304531086 | 0.29731697204413804\n",
      "41 | 0.361072894903498 | 0.3418380843654071 | 83.22339304531086 | 0.2985789520732414\n",
      "42 | 0.35912376351722886 | 0.341493133200358 | 83.26448893572181 | 0.3018323767307756\n",
      "43 | 0.3589748870524427 | 0.3411822372103391 | 83.25079030558483 | 0.30240718044184467\n",
      "44 | 0.35996037641887635 | 0.34083970248290935 | 83.2781875658588 | 0.310522209732819\n",
      "45 | 0.3587886875876958 | 0.34052353076738856 | 83.2781875658588 | 0.3127317644363302\n",
      "46 | 0.3585384370992672 | 0.3402326854124461 | 83.26448893572181 | 0.314193673711866\n",
      "47 | 0.3572773766399159 | 0.33995173085634023 | 83.26448893572181 | 0.31504168414891753\n",
      "48 | 0.3579634663047958 | 0.3396574394547776 | 83.33298208640674 | 0.32143052164276376\n",
      "49 | 0.3576669440539241 | 0.33940206835531206 | 83.34668071654373 | 0.32291317476723796\n",
      "50 | 0.3569918651623646 | 0.3391666400105986 | 83.34668071654373 | 0.3236106409683303\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.35%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 00:07:05,038] Trial 6 finished with value: 0.3391666400105986 and parameters: {'embed_dim': 146, 'learning_rate': 0.0004996209271191486, 'dropout': 0.208813443169025}. Best is trial 5 with value: 0.32049177923839384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3391666400105986\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.5737891671808852 | 0.5320960101607728 | 81.69336143308746 | 0.0\n",
      "2 | 0.5449045807897862 | 0.5120846531162523 | 81.69336143308746 | 0.0\n",
      "3 | 0.5275018339697556 | 0.49971306997619264 | 81.69336143308746 | 0.0\n",
      "4 | 0.5173734029361962 | 0.49081491995347687 | 81.69336143308746 | 0.0\n",
      "5 | 0.5076524672249406 | 0.4835786468362155 | 81.69336143308746 | 0.0\n",
      "6 | 0.49956733293671857 | 0.4772388351290193 | 81.69336143308746 | 0.0\n",
      "7 | 0.49583173632074934 | 0.4714918075359031 | 81.69336143308746 | 0.0\n",
      "8 | 0.4884791854962661 | 0.4661427993480473 | 81.69336143308746 | 0.0\n",
      "9 | 0.48425908236328613 | 0.4611429443506345 | 81.69336143308746 | 0.0\n",
      "10 | 0.4790833676034522 | 0.4564495617396211 | 81.69336143308746 | 0.0\n",
      "11 | 0.4747786998110809 | 0.4519751784327912 | 81.69336143308746 | 0.0\n",
      "12 | 0.4723710751998315 | 0.44779037306570024 | 81.69336143308746 | 0.0\n",
      "13 | 0.4664728689699545 | 0.44380748659780583 | 81.69336143308746 | 0.0\n",
      "14 | 0.46526681039402606 | 0.4400580398840447 | 81.69336143308746 | 0.0\n",
      "15 | 0.46141928764749374 | 0.4364591943074579 | 81.69336143308746 | 0.0\n",
      "16 | 0.45741220114792525 | 0.43308544005841426 | 81.69336143308746 | 0.0\n",
      "17 | 0.4540946911052097 | 0.4298567286092941 | 81.69336143308746 | 0.0\n",
      "18 | 0.45168714880350896 | 0.426818311622698 | 81.69336143308746 | 0.0\n",
      "19 | 0.4493155168876371 | 0.423926904797554 | 81.69336143308746 | 0.0\n",
      "20 | 0.44499116765751023 | 0.4211848530254952 | 81.69336143308746 | 0.0\n",
      "21 | 0.44382565082349906 | 0.418585234308896 | 81.69336143308746 | 0.0\n",
      "22 | 0.4390692606294921 | 0.4160883802257172 | 81.69336143308746 | 0.0\n",
      "23 | 0.4380489385911813 | 0.4137121663518148 | 81.69336143308746 | 0.0\n",
      "24 | 0.4340032950066463 | 0.41141274112136395 | 81.69336143308746 | 0.0\n",
      "25 | 0.43465695804731197 | 0.40923080348396956 | 81.69336143308746 | 0.0\n",
      "26 | 0.43232449639691123 | 0.4071359691554553 | 81.69336143308746 | 0.0\n",
      "27 | 0.42938199716788183 | 0.4051411898985301 | 81.69336143308746 | 0.0\n",
      "28 | 0.4281668077915087 | 0.4032352246242027 | 81.69336143308746 | 0.0\n",
      "29 | 0.4256652173235876 | 0.4014014283271685 | 81.69336143308746 | 0.0\n",
      "30 | 0.42495308645548074 | 0.3996458368962758 | 81.69336143308746 | 0.0\n",
      "31 | 0.42332055109812217 | 0.3979813014603641 | 81.69336143308746 | 0.0\n",
      "32 | 0.42298787687577605 | 0.39635982588954166 | 81.69336143308746 | 0.0\n",
      "33 | 0.4213138473553395 | 0.3948260995827309 | 81.70706006322445 | 0.0012453300124533001\n",
      "34 | 0.4189393404950972 | 0.39333801736978635 | 81.70706006322445 | 0.0012453300124533001\n",
      "35 | 0.4189283392115835 | 0.3919171737889721 | 81.70706006322445 | 0.0012453300124533001\n",
      "36 | 0.41637565528216347 | 0.39055050623743504 | 81.70706006322445 | 0.0012453300124533001\n",
      "37 | 0.4160742872830378 | 0.3892190851373215 | 81.70706006322445 | 0.0012453300124533001\n",
      "38 | 0.4140934083234826 | 0.38796239795341886 | 81.69336143308746 | 0.0012453300124533001\n",
      "39 | 0.41295887905796735 | 0.38671197119640977 | 81.69336143308746 | 0.0024906600249066002\n",
      "40 | 0.41059890252734543 | 0.385504935498107 | 81.67966280295047 | 0.0024906600249066002\n",
      "41 | 0.410708064105168 | 0.3843533402844651 | 81.66596417281349 | 0.0024906600249066002\n",
      "42 | 0.4098640360986239 | 0.38324225949097984 | 81.63856691253952 | 0.0024906600249066002\n",
      "43 | 0.40819264980266584 | 0.38217580859383493 | 81.6522655426765 | 0.0036322125363221253\n",
      "44 | 0.4080410698707862 | 0.3811453407349652 | 81.6522655426765 | 0.0036322125363221253\n",
      "45 | 0.40726933032502094 | 0.38014569807134263 | 81.6522655426765 | 0.0036322125363221253\n",
      "46 | 0.4077284339849555 | 0.3791841042980756 | 81.67966280295047 | 0.007036114570361146\n",
      "47 | 0.4045831000016552 | 0.37824861388908676 | 81.69336143308746 | 0.008089855350129325\n",
      "48 | 0.404654142939534 | 0.3773674743428622 | 81.67966280295047 | 0.008089855350129325\n",
      "49 | 0.4030098335002905 | 0.3764986784694946 | 81.72075869336143 | 0.0127598428968292\n",
      "50 | 0.401653771849765 | 0.37566639962669923 | 81.70706006322445 | 0.0127598428968292\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 81.72%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 00:18:07,470] Trial 7 finished with value: 0.37566639962669923 and parameters: {'embed_dim': 55, 'learning_rate': 0.00013929601319096518, 'dropout': 0.13472067882913705}. Best is trial 5 with value: 0.32049177923839384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37566639962669923\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.624468267601929 | 0.5738630747958405 | 81.69336143308746 | 0.0\n",
      "2 | 0.5491954122843727 | 0.5158192860345318 | 81.69336143308746 | 0.0\n",
      "3 | 0.504886250225229 | 0.48254681470459454 | 81.69336143308746 | 0.0\n",
      "4 | 0.4783693546093203 | 0.46306996598635636 | 81.69336143308746 | 0.0\n",
      "5 | 0.4630588900286488 | 0.45087992573437624 | 81.69336143308746 | 0.0\n",
      "6 | 0.4530483638975963 | 0.4425204453403003 | 81.69336143308746 | 0.0\n",
      "7 | 0.44615163018397236 | 0.43617853971376813 | 81.69336143308746 | 0.0\n",
      "8 | 0.4415702664432905 | 0.4310584948079227 | 81.69336143308746 | 0.0\n",
      "9 | 0.4366633443657411 | 0.42661396893736436 | 81.69336143308746 | 0.0\n",
      "10 | 0.43239556800927226 | 0.42264884567424044 | 81.69336143308746 | 0.0\n",
      "11 | 0.42891060483473886 | 0.41903363066176846 | 81.69336143308746 | 0.0\n",
      "12 | 0.4253389076173123 | 0.415675427203309 | 81.69336143308746 | 0.0\n",
      "13 | 0.42274229676214925 | 0.4125382473615751 | 81.69336143308746 | 0.0\n",
      "14 | 0.4199971094021192 | 0.4096167917325072 | 81.69336143308746 | 0.0\n",
      "15 | 0.4166983985299364 | 0.406845431417635 | 81.69336143308746 | 0.0\n",
      "16 | 0.41417037994125205 | 0.40424334227222286 | 81.69336143308746 | 0.0\n",
      "17 | 0.41189371167884326 | 0.40177599769340805 | 81.69336143308746 | 0.0\n",
      "18 | 0.40906683419366857 | 0.39943721063741267 | 81.69336143308746 | 0.0\n",
      "19 | 0.40695985354232495 | 0.39720015166557 | 81.69336143308746 | 0.0\n",
      "20 | 0.4061150120098474 | 0.3950811335689401 | 81.69336143308746 | 0.0\n",
      "21 | 0.40358855830511187 | 0.39306751944839136 | 81.69336143308746 | 0.0\n",
      "22 | 0.40134592085438764 | 0.3911467251712329 | 81.69336143308746 | 0.0\n",
      "23 | 0.39952841810084627 | 0.3893185679430831 | 81.69336143308746 | 0.0\n",
      "24 | 0.3984064564724034 | 0.3875853745904687 | 81.69336143308746 | 0.0\n",
      "25 | 0.3971147394599535 | 0.38592589585340187 | 81.69336143308746 | 0.0\n",
      "26 | 0.394988475862993 | 0.3843232376934731 | 81.69336143308746 | 0.0\n",
      "27 | 0.39412123143718514 | 0.3827960048228094 | 81.69336143308746 | 0.0\n",
      "28 | 0.3929195266775217 | 0.3813364237342795 | 81.69336143308746 | 0.0\n",
      "29 | 0.3915667751581844 | 0.3799339612461116 | 81.69336143308746 | 0.0\n",
      "30 | 0.39026791539715333 | 0.37859279070406743 | 81.69336143308746 | 0.0\n",
      "31 | 0.3882445391233361 | 0.3772992259019042 | 81.69336143308746 | 0.0\n",
      "32 | 0.38789217330128045 | 0.3760587911899776 | 81.69336143308746 | 0.0\n",
      "33 | 0.38589670211760274 | 0.3748697373148513 | 81.69336143308746 | 0.0\n",
      "34 | 0.3858038090533984 | 0.3737204719896186 | 81.67966280295047 | 0.0011415525114155253\n",
      "35 | 0.3843309129004449 | 0.372626929658733 | 81.70706006322445 | 0.0031479175314791754\n",
      "36 | 0.3835391519699439 | 0.3715790330547176 | 81.70706006322445 | 0.0031479175314791754\n",
      "37 | 0.38258756355408136 | 0.3705633162227395 | 81.72075869336143 | 0.004289470042894701\n",
      "38 | 0.3818901522762914 | 0.36958485239580885 | 81.77555321390938 | 0.010155369744410841\n",
      "39 | 0.3806428425583635 | 0.3686261979276187 | 81.81664910432033 | 0.017488979817746945\n",
      "40 | 0.3800479790440758 | 0.36769601171963834 | 81.87144362486828 | 0.02608289731577403\n",
      "41 | 0.3795878235877289 | 0.36682270007998974 | 81.87144362486828 | 0.029990519374081023\n",
      "42 | 0.37898408096363423 | 0.36597253382205963 | 81.88514225500528 | 0.03356571130543733\n",
      "43 | 0.37812245852493365 | 0.36514571053932793 | 81.88514225500528 | 0.03641959258397615\n",
      "44 | 0.37677062833710184 | 0.36435017949097775 | 81.85774499473129 | 0.03928943963190539\n",
      "45 | 0.37746272547119253 | 0.36358519367975733 | 81.88514225500528 | 0.04351945619068907\n",
      "46 | 0.37610015039551514 | 0.36283379242028274 | 81.99262381454163 | 0.053364481789139315\n",
      "47 | 0.3742592356734502 | 0.3621081792532581 | 82.08851422550053 | 0.06283798013250068\n",
      "48 | 0.3735875719389237 | 0.3614131479638897 | 82.12961011591149 | 0.06860964378087665\n",
      "49 | 0.3731106912278619 | 0.3607327977884306 | 82.15700737618546 | 0.07429977214213188\n",
      "50 | 0.37344529459235865 | 0.36008487498923525 | 82.19810326659642 | 0.0809268812672265\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 82.20%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 00:58:58,023] Trial 8 finished with value: 0.36008487498923525 and parameters: {'embed_dim': 281, 'learning_rate': 0.00017773061467505854, 'dropout': 0.49485666377552817}. Best is trial 5 with value: 0.32049177923839384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36008487498923525\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.4358342889046997 | 0.3775355181057159 | 81.70706006322445 | 0.00243531202435312\n",
      "2 | 0.3763928329657524 | 0.35267272999841875 | 82.93782929399369 | 0.19553101143923485\n",
      "3 | 0.3633422465932296 | 0.3435689501770555 | 83.19810326659642 | 0.280507189965634\n",
      "4 | 0.3564941413267672 | 0.339125736527247 | 83.40358271865122 | 0.3421823931160214\n",
      "5 | 0.35216864768632145 | 0.33637926890833736 | 83.47207586933615 | 0.3388657560866527\n",
      "6 | 0.34939013378244654 | 0.33433519283386126 | 83.5795574288725 | 0.3628235918274309\n",
      "7 | 0.34658653412116774 | 0.3328902028398971 | 83.56585879873552 | 0.3608058975403628\n",
      "8 | 0.3456632419281414 | 0.331695241674985 | 83.74394099051634 | 0.39864210403893374\n",
      "9 | 0.3442161530142348 | 0.3306561860523812 | 83.79873551106428 | 0.39610972596624433\n",
      "10 | 0.3412357809118904 | 0.32971484714174926 | 83.7850368809273 | 0.3982610224293225\n",
      "11 | 0.3395746336503678 | 0.32898634263913923 | 83.79873551106428 | 0.3948654507584847\n",
      "12 | 0.3368533613724173 | 0.3283659044193895 | 83.82613277133825 | 0.38730452994806225\n",
      "13 | 0.3373020504592755 | 0.32756452925809443 | 83.9768177028451 | 0.4052342386040528\n",
      "14 | 0.33582367615871106 | 0.326998453230074 | 84.04531085353003 | 0.41940448590295343\n",
      "15 | 0.3322447626324604 | 0.3266207177753318 | 84.086406743941 | 0.4410218405031529\n",
      "16 | 0.3301407987257574 | 0.3259278485832149 | 84.01791359325605 | 0.40968467047201795\n",
      "17 | 0.32959407631774923 | 0.3253141323181048 | 84.086406743941 | 0.43764474276363297\n",
      "18 | 0.32870385501909694 | 0.3248119016420351 | 83.92202318229715 | 0.41552354823133264\n",
      "19 | 0.32774706638278583 | 0.324434224873373 | 84.16859852476291 | 0.44977001152722407\n",
      "20 | 0.32645891704781704 | 0.32403015620904424 | 84.10010537407798 | 0.43073553969931455\n",
      "21 | 0.3256911471805077 | 0.3238521217279238 | 84.19599578503689 | 0.4245116778799016\n",
      "22 | 0.3243540219210703 | 0.3235838570415157 | 84.30558482613277 | 0.4710663977248373\n",
      "23 | 0.32198126638017666 | 0.32331957902810343 | 84.1822971548999 | 0.42173819805523216\n",
      "24 | 0.3210657888094949 | 0.3229079959939604 | 84.30558482613277 | 0.43390205294542605\n",
      "25 | 0.320121855284187 | 0.3224962187344081 | 84.22339304531086 | 0.4575910432124716\n",
      "26 | 0.3194859186899407 | 0.3222026258502921 | 84.30558482613277 | 0.4588286231890779\n",
      "27 | 0.3171579137626773 | 0.321874191193548 | 84.31928345626976 | 0.4572606126904512\n",
      "28 | 0.31686741529482587 | 0.3216557373126892 | 84.36037934668072 | 0.4519728757826724\n",
      "29 | 0.3156828419604433 | 0.321468312548448 | 84.31928345626976 | 0.4613982129774231\n",
      "30 | 0.31435098304820536 | 0.3212914006554917 | 84.34668071654373 | 0.4602220119454182\n",
      "31 | 0.3142044179893415 | 0.3212530756854031 | 84.42887249736565 | 0.4490701082503616\n",
      "32 | 0.3116006285624311 | 0.3210845122190371 | 84.52476290832455 | 0.45730393146785675\n",
      "33 | 0.3108895392661248 | 0.3208648344017055 | 84.42887249736565 | 0.4555694404176611\n",
      "34 | 0.3110790178293664 | 0.32057220437755324 | 84.41517386722866 | 0.46458768893822094\n",
      "35 | 0.3082766456113678 | 0.32049260951884806 | 84.44257112750263 | 0.46845263122082303\n",
      "36 | 0.3082816202637799 | 0.3203229214230629 | 84.42887249736565 | 0.47045416713681526\n",
      "37 | 0.3087752300801627 | 0.3201921558747553 | 84.53846153846153 | 0.4698809921055639\n",
      "38 | 0.3070822833817942 | 0.32021706143062406 | 84.62065331928346 | 0.46146725535795025\n",
      "39 | 0.3039772922019346 | 0.31996901016937546 | 84.56585879873552 | 0.4668244537584909\n",
      "40 | 0.3046329645639348 | 0.3198889741954738 | 84.51106427818756 | 0.48408264694632713\n",
      "41 | 0.3032042363983229 | 0.31989254241120324 | 84.63435194942045 | 0.47038932467829914\n",
      "42 | 0.30159737562208183 | 0.3196860280347197 | 84.48366701791359 | 0.48055014747397823\n",
      "43 | 0.3001195603030355 | 0.3199104715700019 | 84.66174920969442 | 0.4676805554723474\n",
      "44 | 0.3008704909240252 | 0.3196043008810853 | 84.51106427818756 | 0.4803237940635118\n",
      "45 | 0.2985280952459082 | 0.3196876156207633 | 84.6754478398314 | 0.4763137800740278\n",
      "46 | 0.2976322219128911 | 0.3194441377912482 | 84.45626975763962 | 0.4803502107259716\n",
      "47 | 0.2977560920361714 | 0.31932098714456164 | 84.42887249736565 | 0.4805415469848892\n",
      "48 | 0.29559547893862476 | 0.31929573539185196 | 84.51106427818756 | 0.4884712575398816\n",
      "49 | 0.2948587603001237 | 0.3192488000000993 | 84.56585879873552 | 0.48253403388559135\n",
      "50 | 0.2938246631522062 | 0.3191589998872313 | 84.36037934668072 | 0.4820845181878516\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.68%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 01:34:00,467] Trial 9 finished with value: 0.31915900019341953 and parameters: {'embed_dim': 227, 'learning_rate': 0.005653143151624919, 'dropout': 0.4752355245603169}. Best is trial 9 with value: 0.31915900019341953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31915900019341953\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.41340945158666426 | 0.3577898818336121 | 82.22760800842993 | 0.10305535208202837\n",
      "2 | 0.3644219150578757 | 0.34230301508756533 | 83.17070600632245 | 0.27855980531242414\n",
      "3 | 0.35621167414049854 | 0.3365972923905882 | 83.66385669125395 | 0.3636208982952078\n",
      "4 | 0.3506168538550718 | 0.3334143878254172 | 83.77133825079031 | 0.3759174963954318\n",
      "5 | 0.34538906765959315 | 0.3310879758776051 | 83.96311907270811 | 0.4020008885449209\n",
      "6 | 0.34152533350916814 | 0.3295159674670598 | 84.19599578503689 | 0.4328816629131596\n",
      "7 | 0.34000010800972263 | 0.32814377056409233 | 84.19599578503689 | 0.420183527052328\n",
      "8 | 0.3371083024057591 | 0.32716242652641586 | 84.3740779768177 | 0.44509727027974083\n",
      "9 | 0.33573028057904786 | 0.3261852545076854 | 84.2781875658588 | 0.4277143154931872\n",
      "10 | 0.3330265077383511 | 0.32523145567472667 | 84.30558482613277 | 0.436365929769593\n",
      "11 | 0.33013892039337656 | 0.3246054194153172 | 84.41517386722866 | 0.46285643082001926\n",
      "12 | 0.3288870329088574 | 0.3238724651810241 | 84.30558482613277 | 0.4393689439190076\n",
      "13 | 0.3255460660346422 | 0.32320285659946807 | 84.48366701791359 | 0.4641790850056698\n",
      "14 | 0.3247481078729717 | 0.322820856350742 | 84.41517386722866 | 0.4410426033734576\n",
      "15 | 0.3235306595694217 | 0.3220889082510177 | 84.4699683877766 | 0.46386395301192546\n",
      "16 | 0.3192486641558303 | 0.3217869773507118 | 84.41517386722866 | 0.4517294795074787\n",
      "17 | 0.3183573539331783 | 0.321355451868005 | 84.41517386722866 | 0.4614900093344031\n",
      "18 | 0.31663475893096094 | 0.32105616031035983 | 84.44257112750263 | 0.4546781488436333\n",
      "19 | 0.31482043466459536 | 0.32061382658677556 | 84.51106427818756 | 0.474137752386223\n",
      "20 | 0.3142466190461901 | 0.32085085384649775 | 84.40147523709167 | 0.4463192325665265\n",
      "21 | 0.31184531461208237 | 0.3202733312771745 | 84.30558482613277 | 0.4523979343223796\n",
      "22 | 0.30989078256879743 | 0.3200216845494427 | 84.48366701791359 | 0.4843985074177598\n",
      "23 | 0.30889029042163024 | 0.320083947112299 | 84.3740779768177 | 0.45349129817993394\n",
      "24 | 0.30758626299941577 | 0.31977389344613844 | 84.38777660695469 | 0.4510391372110625\n",
      "25 | 0.30484157591055655 | 0.31948785902294397 | 84.41517386722866 | 0.47321444560894677\n",
      "26 | 0.30435768408922975 | 0.319273429476235 | 84.49736564805058 | 0.4849818252991156\n",
      "27 | 0.3012629183983311 | 0.3192997558476174 | 84.55216016859852 | 0.5012068132558657\n",
      "28 | 0.2992587171677238 | 0.3190719200527831 | 84.45626975763962 | 0.47611066115572925\n",
      "29 | 0.29850174953152825 | 0.3192840709465824 | 84.48366701791359 | 0.4637811264474268\n",
      "30 | 0.29859911162258107 | 0.31883750825303875 | 84.41517386722866 | 0.4787851797130791\n",
      "31 | 0.2958659838190137 | 0.31884181091230207 | 84.3740779768177 | 0.49007762309373726\n",
      "32 | 0.29441601405951223 | 0.3193266141700418 | 84.41517386722866 | 0.46012632079959626\n",
      "33 | 0.29340444743314287 | 0.3188657775929529 | 84.49736564805058 | 0.4798767246154083\n",
      "34 | 0.2914390642913656 | 0.3189330930783324 | 84.44257112750263 | 0.4746342927660161\n",
      "35 | 0.28960072789739943 | 0.31875288864112883 | 84.51106427818756 | 0.48886352761386914\n",
      "36 | 0.28859332262535525 | 0.319326774408556 | 84.45626975763962 | 0.4672010927526589\n",
      "37 | 0.28597681252257356 | 0.31895616507693514 | 84.51106427818756 | 0.5030801745856378\n",
      "38 | 0.28615274536067375 | 0.31897285825585664 | 84.52476290832455 | 0.49543699643688704\n",
      "39 | 0.2830161586210177 | 0.31907561839851617 | 84.60695468914648 | 0.48647815605087125\n",
      "40 | 0.2813118778009663 | 0.31919922812344276 | 84.44257112750263 | 0.49130625988229776\n",
      "41 | 0.2794529881772645 | 0.3194637161819902 | 84.44257112750263 | 0.48397795098341767\n",
      "42 | 0.2787443310683109 | 0.3195007696543654 | 84.53846153846153 | 0.5103371205338965\n",
      "43 | 0.27617373400644063 | 0.31968980231513716 | 84.49736564805058 | 0.48787321529945754\n",
      "44 | 0.273019091906852 | 0.3194766184442664 | 84.55216016859852 | 0.5063466624647474\n",
      "45 | 0.27262621356988903 | 0.3196961712347318 | 84.52476290832455 | 0.49105989889826634\n",
      "46 | 0.27120265250723663 | 0.32018807591640785 | 84.41517386722866 | 0.4751885681579753\n",
      "47 | 0.2695915894098694 | 0.3199218022700858 | 84.56585879873552 | 0.4876169764061968\n",
      "48 | 0.26721881793680907 | 0.32016819664468504 | 84.5795574288725 | 0.4904837963521213\n",
      "49 | 0.26617077473744705 | 0.3211399874050323 | 84.49736564805058 | 0.4720026895559423\n",
      "50 | 0.26546984656916117 | 0.32045569791369244 | 84.56585879873552 | 0.5034875504011718\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.61%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 02:05:22,335] Trial 10 finished with value: 0.32045569791369244 and parameters: {'embed_dim': 208, 'learning_rate': 0.009595240359410446, 'dropout': 0.3168367700694979}. Best is trial 9 with value: 0.31915900019341953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32045569791369244\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.4095498004911143 | 0.356544336431647 | 82.36459430979978 | 0.12918866820537797\n",
      "2 | 0.363499621265434 | 0.342440031570931 | 83.17070600632245 | 0.279084424374702\n",
      "3 | 0.3553546469442159 | 0.3373617638055592 | 83.33298208640674 | 0.3549148067178804\n",
      "4 | 0.3518435087863823 | 0.33451267393076256 | 83.68914646996839 | 0.39423823063085706\n",
      "5 | 0.3470877292096068 | 0.33233158606780716 | 83.64805057955743 | 0.36785559612174396\n",
      "6 | 0.34364514699951954 | 0.33031506591463744 | 83.8809272918862 | 0.41409447855178394\n",
      "7 | 0.33993279676462895 | 0.3289249353621104 | 83.90832455216017 | 0.4121331992159283\n",
      "8 | 0.3380443524193326 | 0.3276166672984215 | 83.99051633298208 | 0.4246596163894891\n",
      "9 | 0.33648734719745976 | 0.32682217075808406 | 84.07270811380401 | 0.44271725089304786\n",
      "10 | 0.33293255132591687 | 0.3259686872567216 | 84.15489989462593 | 0.4585321035829456\n",
      "11 | 0.33151597094462915 | 0.32498793661186137 | 84.03161222339304 | 0.42995158839452624\n",
      "12 | 0.3288446767682876 | 0.3240812475958916 | 84.26448893572181 | 0.4475679022794201\n",
      "13 | 0.3266497077676681 | 0.3235151244148816 | 84.19599578503689 | 0.4333198566695971\n",
      "14 | 0.3246676300677958 | 0.3227971021647323 | 84.33298208640674 | 0.45741651220123436\n",
      "15 | 0.3217971653019616 | 0.322506821931225 | 84.31928345626976 | 0.43343374556105985\n",
      "16 | 0.32078291474633625 | 0.3225771653121465 | 84.40147523709167 | 0.4222704595161134\n",
      "17 | 0.31864592739429315 | 0.32151811488278925 | 84.36037934668072 | 0.4515188801720548\n",
      "18 | 0.31666325677516627 | 0.3212301856646799 | 84.44257112750263 | 0.47327019543859267\n",
      "19 | 0.3158771334962196 | 0.3208124456952696 | 84.3740779768177 | 0.4762904331197136\n",
      "20 | 0.313576459303635 | 0.3203281123344212 | 84.52476290832455 | 0.4704316069568471\n",
      "21 | 0.31061329637349383 | 0.3201030236605096 | 84.45626975763962 | 0.4693083167122035\n",
      "22 | 0.3088352436296619 | 0.31983886791826927 | 84.4699683877766 | 0.46536478638754836\n",
      "23 | 0.3085873638840808 | 0.31954583158231764 | 84.49736564805058 | 0.475422186563683\n",
      "24 | 0.3068662264615024 | 0.3194589941468957 | 84.44257112750263 | 0.45370611204530525\n",
      "25 | 0.3058743353033102 | 0.3192306158885564 | 84.55216016859852 | 0.4678520684920124\n",
      "26 | 0.302783711417507 | 0.31911702045839124 | 84.62065331928346 | 0.4941505052685275\n",
      "27 | 0.30147988271731485 | 0.3189784714008031 | 84.73024236037935 | 0.4746015766932729\n",
      "28 | 0.30042282586478675 | 0.3192846411711549 | 84.66174920969442 | 0.4591578389519283\n",
      "29 | 0.2964686369950618 | 0.3186662742128111 | 84.64805057955743 | 0.49333148239846003\n",
      "30 | 0.296476916691989 | 0.3186787690405976 | 84.68914646996839 | 0.4743436574061903\n",
      "31 | 0.29643632366182426 | 0.31852666890784487 | 84.53846153846153 | 0.4928496460496227\n",
      "32 | 0.2952456856117974 | 0.3185677377328481 | 84.82613277133825 | 0.48451647800549125\n",
      "33 | 0.2921179495011357 | 0.31836723270889833 | 84.63435194942045 | 0.4913138406121463\n",
      "34 | 0.2904452318183235 | 0.31835877589166983 | 84.70284510010538 | 0.4938261802742756\n",
      "35 | 0.2866170628890623 | 0.31837909199195363 | 84.75763962065332 | 0.4861743711633913\n",
      "36 | 0.28719379833190267 | 0.31839126899634324 | 84.71654373024236 | 0.4918219344990769\n",
      "37 | 0.28507455276034843 | 0.3183529047320967 | 84.7850368809273 | 0.4889811500416964\n",
      "38 | 0.28478859079608676 | 0.3185021808906777 | 84.66174920969442 | 0.5067658581900999\n",
      "39 | 0.28047553030631595 | 0.31862732543520733 | 84.77133825079031 | 0.4850036358821488\n",
      "40 | 0.27954840031808487 | 0.318521293569101 | 84.74394099051634 | 0.49941456396682854\n",
      "41 | 0.27790384164894577 | 0.3190136447752992 | 84.64805057955743 | 0.5044791625671856\n",
      "42 | 0.27562137446522894 | 0.318900442490839 | 84.71654373024236 | 0.49590191926209043\n",
      "43 | 0.2740684922226343 | 0.3193157296882917 | 84.79873551106428 | 0.49942871132435973\n",
      "44 | 0.2720988518259394 | 0.31930479825767755 | 84.77133825079031 | 0.5121850841224322\n",
      "45 | 0.27041208849997694 | 0.3202307930547897 | 84.82613277133825 | 0.4862510913235167\n",
      "46 | 0.269462048876404 | 0.3199915070443937 | 84.74394099051634 | 0.5144670662139584\n",
      "47 | 0.26694960754922953 | 0.3200243880895719 | 84.77133825079031 | 0.5040894441250091\n",
      "48 | 0.26433944796704734 | 0.3203507301537958 | 84.81243414120127 | 0.5083895357559928\n",
      "49 | 0.2624932959709966 | 0.32046665884044073 | 84.8809272918862 | 0.5131392512300299\n",
      "50 | 0.26123456889905877 | 0.3206400641635673 | 84.85353003161222 | 0.49875269283775503\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.88%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 02:36:47,640] Trial 11 finished with value: 0.3206400637553163 and parameters: {'embed_dim': 207, 'learning_rate': 0.009845143875502508, 'dropout': 0.3212540163270328}. Best is trial 9 with value: 0.31915900019341953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3206400637553163\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.4234629359882359 | 0.3671821702833045 | 81.87144362486828 | 0.02170561554123198\n",
      "2 | 0.37049647776085304 | 0.34731249450004265 | 82.75974710221287 | 0.21480262356198387\n",
      "3 | 0.36016362420189274 | 0.33979601096617035 | 83.18440463645943 | 0.3355276554304291\n",
      "4 | 0.3536577749006245 | 0.335816379891683 | 83.25289778714438 | 0.3456246463296998\n",
      "5 | 0.35008762940901134 | 0.3333164594353062 | 83.38988408851424 | 0.361757127331321\n",
      "6 | 0.34651155684382545 | 0.3312829418541634 | 83.71654373024236 | 0.3916381444037809\n",
      "7 | 0.3445060017663951 | 0.32996179847276375 | 83.89462592202318 | 0.4157374826323268\n",
      "8 | 0.3415354242952774 | 0.3286839071406077 | 83.77133825079031 | 0.39536891608484737\n",
      "9 | 0.3393046912546369 | 0.32773864299875416 | 83.83983140147524 | 0.39572526658872836\n",
      "10 | 0.33719299447691403 | 0.32662817039718367 | 84.03161222339304 | 0.4152637207339301\n",
      "11 | 0.3364645389424187 | 0.3258909322831729 | 83.96311907270811 | 0.4177609004626229\n",
      "12 | 0.33357165103339637 | 0.3250692184657267 | 84.1822971548999 | 0.43628949396425426\n",
      "13 | 0.332005823560811 | 0.3245633754828205 | 84.23709167544784 | 0.4440182276694822\n",
      "14 | 0.3295543276297572 | 0.32406280026452183 | 84.22339304531086 | 0.44324685154393134\n",
      "15 | 0.3281733656148298 | 0.3235845580084683 | 84.26448893572181 | 0.4379696826638284\n",
      "16 | 0.32697461875275186 | 0.3231314420700073 | 84.10010537407798 | 0.427574400523907\n",
      "17 | 0.3249375361504905 | 0.3229632634822636 | 84.23709167544784 | 0.46307292384902987\n",
      "18 | 0.323213968631051 | 0.3226536731809786 | 84.29188619599579 | 0.4654966181571785\n",
      "19 | 0.32269807259013894 | 0.322124923234933 | 84.19599578503689 | 0.44229280374380575\n",
      "20 | 0.321632222423404 | 0.3221419916985786 | 84.1822971548999 | 0.4359922786382274\n",
      "21 | 0.31878095921072025 | 0.32175644119716673 | 84.25079030558483 | 0.45316763068470023\n",
      "22 | 0.31905744478835607 | 0.32141204586584277 | 84.22339304531086 | 0.4561438970118296\n",
      "23 | 0.31555136847979065 | 0.3213665403731882 | 84.16859852476291 | 0.44105607936768326\n",
      "24 | 0.31508107790410156 | 0.32083469129180253 | 84.30558482613277 | 0.4575467331642311\n",
      "25 | 0.31321568285835627 | 0.32060295761856317 | 84.1822971548999 | 0.45535263821144967\n",
      "26 | 0.3122434767265021 | 0.320539057765105 | 84.20969441517387 | 0.44472223166628183\n",
      "27 | 0.3119375090305594 | 0.32028612443437315 | 84.25079030558483 | 0.4568328428953747\n",
      "28 | 0.3095688119712955 | 0.3202278557909678 | 84.23709167544784 | 0.46335331897030885\n",
      "29 | 0.30797840318776415 | 0.32008422339615755 | 84.19599578503689 | 0.4615853750142386\n",
      "30 | 0.3070730870868726 | 0.3199574498281087 | 84.16859852476291 | 0.4606419967621322\n",
      "31 | 0.305557010792173 | 0.31992435536972463 | 84.2781875658588 | 0.44799227145591397\n",
      "32 | 0.303760305038491 | 0.3198058679699898 | 84.29188619599579 | 0.45422703519694246\n",
      "33 | 0.30175914244572505 | 0.32019697446120926 | 84.20969441517387 | 0.4384291299484841\n",
      "34 | 0.30138751074375547 | 0.3198087402198413 | 84.12750263435196 | 0.44279861583685104\n",
      "35 | 0.2993452566467932 | 0.31941029266135335 | 84.33298208640674 | 0.4663450536783967\n",
      "36 | 0.2984941514488025 | 0.3194440288902962 | 84.22339304531086 | 0.47066682141091437\n",
      "37 | 0.2971459459114694 | 0.31933404372571267 | 84.23709167544784 | 0.4692383002409218\n",
      "38 | 0.29722864530547677 | 0.31937893459649935 | 84.22339304531086 | 0.4673629851553234\n",
      "39 | 0.295923259086474 | 0.3194754219626727 | 84.20969441517387 | 0.4620238752428304\n",
      "40 | 0.2941022643175785 | 0.31970519878684656 | 84.26448893572181 | 0.45090508509595334\n",
      "41 | 0.29335255551775663 | 0.3193506516822397 | 84.19599578503689 | 0.4696364714388997\n",
      "42 | 0.2913901777225988 | 0.31934097878736994 | 84.1822971548999 | 0.4577353996903483\n",
      "43 | 0.28909250423995725 | 0.3194292692901337 | 84.23709167544784 | 0.45707495430564626\n",
      "44 | 0.2883550724817799 | 0.31938401395327426 | 84.2781875658588 | 0.4810058397794616\n",
      "45 | 0.2857962640839706 | 0.31944461962948106 | 84.12750263435196 | 0.4675815175576879\n",
      "46 | 0.28669940370394187 | 0.3194907618916198 | 84.22339304531086 | 0.46310054893155034\n",
      "47 | 0.285847329947562 | 0.3197213573610946 | 84.15489989462593 | 0.45260511657674196\n",
      "48 | 0.2840564384464095 | 0.31966296022068963 | 84.2781875658588 | 0.48324049104905464\n",
      "49 | 0.28324275094708173 | 0.3195322487656384 | 84.1822971548999 | 0.46702230707998715\n",
      "50 | 0.28074135720752613 | 0.31988979375933946 | 84.26448893572181 | 0.46932054197143447\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.33%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 03:08:14,390] Trial 12 finished with value: 0.31988979375933946 and parameters: {'embed_dim': 206, 'learning_rate': 0.00777533922649327, 'dropout': 0.4030019137652875}. Best is trial 9 with value: 0.31915900019341953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31988979375933946\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.495676605196083 | 0.43274161525785104 | 81.69336143308746 | 0.0\n",
      "2 | 0.42567483631933867 | 0.40106224564656817 | 81.69336143308746 | 0.0\n",
      "3 | 0.4020102369835435 | 0.38218974587443755 | 81.69336143308746 | 0.0\n",
      "4 | 0.3884731852634602 | 0.3697956284023311 | 81.72075869336143 | 0.0074347266128088054\n",
      "5 | 0.3787079260026643 | 0.36140944308614076 | 82.0632244467861 | 0.05348046728989517\n",
      "6 | 0.3728595758682909 | 0.35538375459305227 | 82.30769230769232 | 0.11670692121304954\n",
      "7 | 0.3685086094389815 | 0.35096940439041346 | 82.59536354056902 | 0.18119909122125075\n",
      "8 | 0.36639004034369 | 0.34775116386478894 | 82.86933614330876 | 0.2169297910434091\n",
      "9 | 0.36300299365631666 | 0.34531388656325535 | 82.97892518440464 | 0.23895251458655337\n",
      "10 | 0.36061663807167554 | 0.3432789685791486 | 83.03371970495259 | 0.26233513230576294\n",
      "11 | 0.35827243543409426 | 0.341596588696519 | 83.25289778714438 | 0.2975012192943126\n",
      "12 | 0.35744707154206906 | 0.340264120024361 | 83.25289778714438 | 0.3107548272216538\n",
      "13 | 0.3562873152030535 | 0.33921376095242695 | 83.26659641728136 | 0.30568052284979835\n",
      "14 | 0.3556190553585506 | 0.33815310854617864 | 83.25289778714438 | 0.3223862372354444\n",
      "15 | 0.35435959115761134 | 0.3373246137818245 | 83.19810326659642 | 0.3249392277252643\n",
      "16 | 0.3536928375037985 | 0.3365503942517385 | 83.19810326659642 | 0.32999336508834176\n",
      "17 | 0.3527205497459352 | 0.3358559628872022 | 83.40358271865122 | 0.3485001804016605\n",
      "18 | 0.3528265860900055 | 0.33530533456639067 | 83.33508956796629 | 0.3403890438319379\n",
      "19 | 0.3506493455294622 | 0.33469805866479874 | 83.40358271865122 | 0.3480957200402906\n",
      "20 | 0.34953109817493944 | 0.3341903641615828 | 83.40358271865122 | 0.3504126956555401\n",
      "21 | 0.3492211055591566 | 0.3337093371234528 | 83.41728134878821 | 0.3523069201525043\n",
      "22 | 0.3485192001344414 | 0.33318987815347434 | 83.5268703898841 | 0.36728197121931083\n",
      "23 | 0.3480221448118716 | 0.3328467925933942 | 83.47207586933615 | 0.35868856875445493\n",
      "24 | 0.3476912312391883 | 0.3323910013861852 | 83.58166491043204 | 0.3705158378093418\n",
      "25 | 0.34682381322852335 | 0.3321787199541314 | 83.55426765015807 | 0.3606019343800221\n",
      "26 | 0.3453962238511179 | 0.331631710574235 | 83.622760800843 | 0.37903217337039935\n",
      "27 | 0.34536756298274074 | 0.33133822679519653 | 83.58166491043204 | 0.37462101123520314\n",
      "28 | 0.3445040933582761 | 0.33103692317253924 | 83.622760800843 | 0.3772040494850423\n",
      "29 | 0.34358753536226916 | 0.3306652044188486 | 83.66385669125395 | 0.3844728524808694\n",
      "30 | 0.343826134680608 | 0.3303788198183661 | 83.69125395152793 | 0.38430981521886837\n",
      "31 | 0.3432060969532083 | 0.33013975385525457 | 83.63645943097998 | 0.3805816470668485\n",
      "32 | 0.3423960908609428 | 0.3298278471378431 | 83.67755532139094 | 0.3844355557152777\n",
      "33 | 0.3417398273853716 | 0.32956348094221666 | 83.63645943097998 | 0.38722497874039774\n",
      "34 | 0.3408675479335249 | 0.3293176638345196 | 83.66385669125395 | 0.38604112476645513\n",
      "35 | 0.3424140518867277 | 0.3291001413782982 | 83.70495258166491 | 0.3886909684512239\n",
      "36 | 0.3397189659900986 | 0.32884279465022154 | 83.84193888303479 | 0.3945166887478061\n",
      "37 | 0.34020621221888503 | 0.3286057831081626 | 83.70495258166491 | 0.39835770058574244\n",
      "38 | 0.3393770869058024 | 0.32838540983526676 | 83.77344573234986 | 0.396419663021194\n",
      "39 | 0.33843695681075076 | 0.3281325141656889 | 83.73234984193888 | 0.40534604314270173\n",
      "40 | 0.3396045720312938 | 0.32797023409033477 | 83.67755532139094 | 0.4004950996621551\n",
      "41 | 0.337715701726963 | 0.32777955964820027 | 83.65015806111697 | 0.3964797891012955\n",
      "42 | 0.33796362863676993 | 0.327574661333267 | 83.70495258166491 | 0.41086550302069963\n",
      "43 | 0.337221320547642 | 0.3274135832508949 | 83.74604847207587 | 0.4030296541369954\n",
      "44 | 0.3366252995959116 | 0.32720773973285333 | 83.74604847207587 | 0.4056006745080696\n",
      "45 | 0.33580058350566694 | 0.3270472364678775 | 83.81454162276081 | 0.4054910462096757\n",
      "46 | 0.33575467423561517 | 0.32683043212515034 | 83.80084299262383 | 0.412735184966103\n",
      "47 | 0.33455064560682585 | 0.3266304306787987 | 83.85563751317177 | 0.41865827727591653\n",
      "48 | 0.33535032781461876 | 0.3264744892104031 | 83.85563751317177 | 0.41865886154587184\n",
      "49 | 0.3342186208267551 | 0.3263058186802146 | 83.88303477344574 | 0.41671630440695356\n",
      "50 | 0.33457516238594637 | 0.3261614175487871 | 83.95152792413067 | 0.4174693945436054\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.95%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 03:39:39,494] Trial 13 finished with value: 0.3261614175487871 and parameters: {'embed_dim': 204, 'learning_rate': 0.0016890814081679895, 'dropout': 0.4302629637283488}. Best is trial 9 with value: 0.31915900019341953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3261614175487871\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.4274975248592527 | 0.37127349709403024 | 81.77555321390938 | 0.013095732273814466\n",
      "2 | 0.3717788454566709 | 0.34835931709776186 | 82.91043203371972 | 0.21929087701013994\n",
      "3 | 0.3594714974819338 | 0.34023689152034997 | 83.12750263435196 | 0.29229498693489314\n",
      "4 | 0.3538747606428756 | 0.3362858120913375 | 83.29188619599579 | 0.32867753455890103\n",
      "5 | 0.35076590966923155 | 0.3340646102003855 | 83.4699683877766 | 0.34906317477523063\n",
      "6 | 0.34701341201497143 | 0.33228313933088355 | 83.7850368809273 | 0.3827088187136323\n",
      "7 | 0.34572747268853565 | 0.33097787762749686 | 83.82613277133825 | 0.38206313306027584\n",
      "8 | 0.34176116178114113 | 0.32974857347060554 | 83.90832455216017 | 0.3973001915601113\n",
      "9 | 0.34047889793206243 | 0.32898480149164594 | 83.89462592202318 | 0.3845812689584506\n",
      "10 | 0.33655002660798733 | 0.3280177107208396 | 84.01791359325605 | 0.3968258955247884\n",
      "11 | 0.3358846426534179 | 0.327027800760857 | 84.20969441517387 | 0.4294034651021869\n",
      "12 | 0.3343576467809601 | 0.32651720757353797 | 84.03161222339304 | 0.4096981247001899\n",
      "13 | 0.3329260552057068 | 0.32593334302918553 | 84.05900948366701 | 0.4183595203033327\n",
      "14 | 0.32980303813785955 | 0.32540115856961027 | 84.10010537407798 | 0.4164816775496046\n",
      "15 | 0.32976596237983363 | 0.324774309686602 | 84.36037934668072 | 0.4534402950694517\n",
      "16 | 0.32837823607506006 | 0.3243718469796115 | 84.30558482613277 | 0.4525633143809777\n",
      "17 | 0.32772134457478463 | 0.3240883133999289 | 84.25079030558483 | 0.4340119242067848\n",
      "18 | 0.32473568165452654 | 0.3236091400254263 | 84.42887249736565 | 0.4612660434033795\n",
      "19 | 0.32364431547644673 | 0.3232068064278119 | 84.38777660695469 | 0.45978983664222856\n",
      "20 | 0.3222854536294436 | 0.3229500747298541 | 84.30558482613277 | 0.44566404433456125\n",
      "21 | 0.32036208231514746 | 0.3227016466529402 | 84.44257112750263 | 0.4543146463160933\n",
      "22 | 0.31913215435745157 | 0.32232130257642433 | 84.41517386722866 | 0.45979824698767896\n",
      "23 | 0.31770115041541397 | 0.32215527251158677 | 84.44257112750263 | 0.45902248765928805\n",
      "24 | 0.31746196024552764 | 0.32180828560296804 | 84.4699683877766 | 0.4631832027865596\n",
      "25 | 0.31558666821421105 | 0.32164214408560976 | 84.44257112750263 | 0.4680639430649116\n",
      "26 | 0.3142459710279554 | 0.3214271599299287 | 84.48366701791359 | 0.48235528157992846\n",
      "27 | 0.3134781993093932 | 0.32130467779424093 | 84.49736564805058 | 0.4801355464036868\n",
      "28 | 0.31120533737658723 | 0.3211419031228105 | 84.53846153846153 | 0.46420715655514655\n",
      "29 | 0.3095566583996701 | 0.3211385775102328 | 84.53846153846153 | 0.45836306263459325\n",
      "30 | 0.3091563802747916 | 0.32065379782898784 | 84.51106427818756 | 0.47733464704448403\n",
      "31 | 0.30789881281623055 | 0.32048578907365666 | 84.5795574288725 | 0.4800574210381574\n",
      "32 | 0.30652984546708223 | 0.32042654246500096 | 84.60695468914648 | 0.4682899020207503\n",
      "33 | 0.30393878697483184 | 0.32013341325194866 | 84.44257112750263 | 0.4779426424944682\n",
      "34 | 0.30439654946190503 | 0.3198769704936302 | 84.48366701791359 | 0.47881344331794884\n",
      "35 | 0.30257760446849036 | 0.31992796298167475 | 84.55216016859852 | 0.4786500575419971\n",
      "36 | 0.3021592159587491 | 0.3199238106608391 | 84.55216016859852 | 0.4733252828256247\n",
      "37 | 0.30061153684734204 | 0.31972985306423 | 84.45626975763962 | 0.4764561263637162\n",
      "38 | 0.29939287693584365 | 0.3198799716487323 | 84.48366701791359 | 0.46572558964175736\n",
      "39 | 0.29665630858744685 | 0.3195323916534855 | 84.4699683877766 | 0.49228644006071687\n",
      "40 | 0.2958419528883805 | 0.31955858442473084 | 84.49736564805058 | 0.48474004810766225\n",
      "41 | 0.29432971385344636 | 0.31948312443413146 | 84.4699683877766 | 0.480866811607913\n",
      "42 | 0.294288906077954 | 0.31928237170389256 | 84.52476290832455 | 0.47926496492776804\n",
      "43 | 0.2924767196383498 | 0.3194032109355273 | 84.56585879873552 | 0.4787267738980379\n",
      "44 | 0.29179939539926497 | 0.3192114612623437 | 84.42887249736565 | 0.48408906448032435\n",
      "45 | 0.2909798240716304 | 0.3192610764136053 | 84.52476290832455 | 0.491621210294372\n",
      "46 | 0.2876312584824154 | 0.31941816363840886 | 84.42887249736565 | 0.4877231036513613\n",
      "47 | 0.2873517531867421 | 0.3193115241927643 | 84.51106427818756 | 0.48457399002469853\n",
      "48 | 0.2884861545734085 | 0.3193338556240683 | 84.51106427818756 | 0.48655743712375604\n",
      "49 | 0.2857226712361479 | 0.31938403497820034 | 84.49736564805058 | 0.49824200274551617\n",
      "50 | 0.2844150324753665 | 0.319273660648359 | 84.52476290832455 | 0.4859322081577688\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.61%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 04:14:48,826] Trial 14 finished with value: 0.31927366105661 and parameters: {'embed_dim': 234, 'learning_rate': 0.007040969716062686, 'dropout': 0.4141115968359264}. Best is trial 9 with value: 0.31915900019341953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31927366105661\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.4773563849269797 | 0.4201071511392724 | 81.69336143308746 | 0.0\n",
      "2 | 0.41234669345644637 | 0.38986925107159026 | 81.69336143308746 | 0.0\n",
      "3 | 0.3913952346651926 | 0.37270307214292764 | 81.7481559536354 | 0.004566210045662101\n",
      "4 | 0.3796985515646796 | 0.3619642367918197 | 82.13171759747102 | 0.0885835927435444\n",
      "5 | 0.3702818548665681 | 0.35492375850269237 | 82.58166491043204 | 0.17097640079689785\n",
      "6 | 0.365873048885153 | 0.3501874593226877 | 82.85563751317177 | 0.2228753345250016\n",
      "7 | 0.3623889988282587 | 0.3467797644130171 | 82.96522655426766 | 0.24566855714275032\n",
      "8 | 0.35930103282316017 | 0.3441342594076509 | 82.97892518440464 | 0.2745284819651841\n",
      "9 | 0.3579790694724529 | 0.3421275273781933 | 83.15700737618546 | 0.29895971427416596\n",
      "10 | 0.3549618002608282 | 0.3403803446929749 | 83.38988408851424 | 0.32138987452007794\n",
      "11 | 0.354312648865699 | 0.3390250735699314 | 83.37618545837725 | 0.32153127145841276\n",
      "12 | 0.35229358476919864 | 0.3378766179084778 | 83.36248682824026 | 0.32468999644309937\n",
      "13 | 0.34979535667766853 | 0.3367490399986097 | 83.29399367755533 | 0.3351224870076851\n",
      "14 | 0.34990211514704816 | 0.33580844053258635 | 83.45837723919917 | 0.35754750499061483\n",
      "15 | 0.34952495855475785 | 0.33504059104478523 | 83.56585879873552 | 0.36718379241434024\n",
      "16 | 0.3476401727521274 | 0.3342913508619348 | 83.64805057955743 | 0.3652795576137782\n",
      "17 | 0.3478730233665271 | 0.3336954753692836 | 83.6754478398314 | 0.37565906333235677\n",
      "18 | 0.3462718914772757 | 0.33310300693528294 | 83.60695468914648 | 0.3685829849873527\n",
      "19 | 0.3446333989022522 | 0.33256506031914934 | 83.66174920969442 | 0.36815373649815963\n",
      "20 | 0.34374510181287377 | 0.33196939446338236 | 83.75763962065332 | 0.3828351653081747\n",
      "21 | 0.3428661386377039 | 0.33148752867359005 | 83.75763962065332 | 0.38222605158707573\n",
      "22 | 0.3421511019748103 | 0.3310583227709548 | 83.75763962065332 | 0.388947129492258\n",
      "23 | 0.34243873226533245 | 0.330636396800002 | 83.79873551106428 | 0.38508874610919175\n",
      "24 | 0.34044069393512305 | 0.3302487109417785 | 83.86722866174921 | 0.38810600925093003\n",
      "25 | 0.33968572324250085 | 0.32980775139103197 | 83.82613277133825 | 0.39870189441528714\n",
      "26 | 0.3384102227377053 | 0.3294734655790133 | 83.83983140147524 | 0.38791156987066855\n",
      "27 | 0.3392700402675601 | 0.3290633423483535 | 83.93572181243414 | 0.39899224104082603\n",
      "28 | 0.3387615197659997 | 0.3287515872961854 | 83.96311907270811 | 0.413064188083531\n",
      "29 | 0.33705415393279964 | 0.32841478441267796 | 83.9768177028451 | 0.4093644206414505\n",
      "30 | 0.33704533860862074 | 0.3281584916865989 | 84.00421496311907 | 0.4053919418720126\n",
      "31 | 0.3354724920397505 | 0.32780368248485536 | 84.01791359325605 | 0.41126404234365077\n",
      "32 | 0.33438340160961544 | 0.327497199177742 | 83.99051633298208 | 0.4102718060244286\n",
      "33 | 0.33425565003574076 | 0.3273162257997957 | 84.01791359325605 | 0.40513061643945425\n",
      "34 | 0.33323857966320597 | 0.32696857854519806 | 84.16859852476291 | 0.4270307032855275\n",
      "35 | 0.3342631743018963 | 0.32669836415411674 | 84.16859852476291 | 0.42564761878311425\n",
      "36 | 0.3322501540844776 | 0.32645278495468505 | 84.086406743941 | 0.42357162593959524\n",
      "37 | 0.3329534285647698 | 0.3262846370878285 | 84.16859852476291 | 0.4304406500999868\n",
      "38 | 0.33143871792503815 | 0.3259981550174217 | 84.14120126448894 | 0.42831586804168337\n",
      "39 | 0.3316089712447894 | 0.3258194356952628 | 84.20969441517387 | 0.43243790971207313\n",
      "40 | 0.3294556263822846 | 0.3256512789489472 | 84.22339304531086 | 0.4396227517598172\n",
      "41 | 0.33027043550158497 | 0.32543353277118237 | 84.23709167544784 | 0.4337348283026364\n",
      "42 | 0.3299667284644525 | 0.3252300741533711 | 84.03161222339304 | 0.4192105106748433\n",
      "43 | 0.32860377912811184 | 0.3250010416728176 | 84.14120126448894 | 0.42672827977312255\n",
      "44 | 0.3276686880521089 | 0.32480188438745394 | 84.2781875658588 | 0.44127900884799875\n",
      "45 | 0.3261174532023774 | 0.3246064641296047 | 84.34668071654373 | 0.4429339093922712\n",
      "46 | 0.32750351796502003 | 0.32442089302899085 | 84.26448893572181 | 0.43940266661689165\n",
      "47 | 0.3257523568899624 | 0.32424373414418467 | 84.25079030558483 | 0.43840913959596745\n",
      "48 | 0.32630793596489716 | 0.3240970488484592 | 84.26448893572181 | 0.43751743236864377\n",
      "49 | 0.3264630708946 | 0.32394876902642317 | 84.25079030558483 | 0.44001412657406186\n",
      "50 | 0.32449662685394287 | 0.3237747111140865 | 84.22339304531086 | 0.43968600849521494\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.35%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 04:51:40,512] Trial 15 finished with value: 0.3237747111140865 and parameters: {'embed_dim': 246, 'learning_rate': 0.0021418399044712223, 'dropout': 0.4503782010868066}. Best is trial 9 with value: 0.31915900019341953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3237747111140865\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.43139344584823386 | 0.37105677011486604 | 81.73445732349842 | 0.008671351479570658\n",
      "2 | 0.3719491769619491 | 0.34950357889884137 | 82.95152792413067 | 0.2242827145664416\n",
      "3 | 0.3610411339214453 | 0.34198476292499125 | 82.97892518440464 | 0.30009981114243467\n",
      "4 | 0.35511881757857966 | 0.33793488828694984 | 83.25289778714438 | 0.3268296528497988\n",
      "5 | 0.34935927394469946 | 0.3351326703208767 | 83.55216016859852 | 0.37726394337496333\n",
      "6 | 0.3481162935381453 | 0.3331730184081483 | 83.62065331928346 | 0.3817101938288901\n",
      "7 | 0.34525484716468446 | 0.3315984608784114 | 83.75763962065332 | 0.38217554055776903\n",
      "8 | 0.34331737332251094 | 0.33023361259535566 | 83.82613277133825 | 0.38494988697794436\n",
      "9 | 0.34027703688551164 | 0.3289886564016342 | 83.86722866174921 | 0.39954696193722444\n",
      "10 | 0.3386649558668107 | 0.32820660284120745 | 83.92202318229715 | 0.43274260068828535\n",
      "11 | 0.33632279237567103 | 0.32703982273193255 | 83.94942044257112 | 0.4122835993836234\n",
      "12 | 0.33451461943510113 | 0.3262830766504758 | 83.89462592202318 | 0.42577634497388517\n",
      "13 | 0.33314730691800426 | 0.32557004383982047 | 84.11380400421497 | 0.4280962074292542\n",
      "14 | 0.3299331133245328 | 0.32491725673006006 | 84.07270811380401 | 0.41752888952243516\n",
      "15 | 0.3292958235043451 | 0.3243920084548323 | 84.12750263435196 | 0.42118100179874696\n",
      "16 | 0.3280721703367678 | 0.3237572267447432 | 84.2781875658588 | 0.4388314606231617\n",
      "17 | 0.3261906973653066 | 0.3232737865350018 | 84.29188619599579 | 0.4494095411832616\n",
      "18 | 0.3248418015897821 | 0.32274852254211084 | 84.29188619599579 | 0.44490749479436664\n",
      "19 | 0.3225672314063125 | 0.3226262803559434 | 84.20969441517387 | 0.43179959821534303\n",
      "20 | 0.3209699698202654 | 0.32227560296042324 | 84.29188619599579 | 0.44306437071030935\n",
      "21 | 0.31984436401487853 | 0.3219691179182431 | 84.2781875658588 | 0.43759701200596013\n",
      "22 | 0.31803432384944474 | 0.32133671304542727 | 84.36037934668072 | 0.4597024755469217\n",
      "23 | 0.3169006153325969 | 0.32112352611267403 | 84.33298208640674 | 0.45337109750289684\n",
      "24 | 0.316626136740869 | 0.32093949962968693 | 84.51106427818756 | 0.4828336144436356\n",
      "25 | 0.31540863508706063 | 0.3207247529740203 | 84.31928345626976 | 0.44519102302296454\n",
      "26 | 0.31296306290302073 | 0.32030484978466817 | 84.42887249736565 | 0.4764150936968772\n",
      "27 | 0.3107603248835339 | 0.3200650445810736 | 84.44257112750263 | 0.47574920011663185\n",
      "28 | 0.31119161680773677 | 0.32007823466029883 | 84.31928345626976 | 0.47894685674840437\n",
      "29 | 0.30963403279309243 | 0.31979940984755345 | 84.40147523709167 | 0.47773676066017995\n",
      "30 | 0.3082867511049688 | 0.3195135093306842 | 84.36037934668072 | 0.46917898313831863\n",
      "31 | 0.3073242018711312 | 0.31939631394327506 | 84.41517386722866 | 0.47920499119989535\n",
      "32 | 0.30547169867094137 | 0.3192172636316247 | 84.38777660695469 | 0.4700051142617768\n",
      "33 | 0.3041019496203198 | 0.319113104412817 | 84.45626975763962 | 0.4636844205734033\n",
      "34 | 0.3034114870969035 | 0.31903118027807914 | 84.48366701791359 | 0.46483812526032514\n",
      "35 | 0.3017034796536516 | 0.318891691528771 | 84.3740779768177 | 0.4885322065885599\n",
      "36 | 0.2996604307198652 | 0.3187143405618733 | 84.36037934668072 | 0.47944880664212813\n",
      "37 | 0.29843805903506204 | 0.3186602610839556 | 84.33298208640674 | 0.4822979900706279\n",
      "38 | 0.2985865444355055 | 0.3185866086246216 | 84.41517386722866 | 0.4654805476157862\n",
      "39 | 0.2975367713970238 | 0.3186185530399623 | 84.26448893572181 | 0.49308595670000493\n",
      "40 | 0.2938610903622543 | 0.31863585308398285 | 84.5795574288725 | 0.469865831776188\n",
      "41 | 0.29338133321442733 | 0.318396574320042 | 84.34668071654373 | 0.4874213976794651\n",
      "42 | 0.29270852668729214 | 0.31826210042385206 | 84.34668071654373 | 0.4838050351717057\n",
      "43 | 0.291960162084994 | 0.3182041351109335 | 84.30558482613277 | 0.4887051544987218\n",
      "44 | 0.29002808404670577 | 0.31821121418312803 | 84.26448893572181 | 0.4909631872261339\n",
      "45 | 0.2884277940047079 | 0.3181152429482708 | 84.42887249736565 | 0.49375994892074354\n",
      "46 | 0.28880345792330187 | 0.31814115423045747 | 84.29188619599579 | 0.4974725613167267\n",
      "47 | 0.28557756524931766 | 0.3183278383253372 | 84.51106427818756 | 0.4800235314762771\n",
      "48 | 0.2854180277555908 | 0.31800139838293806 | 84.48366701791359 | 0.49971768067628275\n",
      "49 | 0.28377803139365776 | 0.317929365761476 | 84.51106427818756 | 0.48892688863479933\n",
      "50 | 0.2840350417004448 | 0.3181364592419912 | 84.5795574288725 | 0.5076895972091652\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.58%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 05:32:01,490] Trial 16 finished with value: 0.3181364592419912 and parameters: {'embed_dim': 280, 'learning_rate': 0.006282648339273991, 'dropout': 0.49965871600800255}. Best is trial 16 with value: 0.3181364592419912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3181364592419912\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.46985621930353505 | 0.41140261557820723 | 81.69336143308746 | 0.0\n",
      "2 | 0.4054618546061377 | 0.38425847285822645 | 81.69336143308746 | 0.0\n",
      "3 | 0.38692823550250915 | 0.36886550475881524 | 81.7481559536354 | 0.016834839437579165\n",
      "4 | 0.3754071337526909 | 0.3592259631785628 | 82.2550052687039 | 0.09572225019232951\n",
      "5 | 0.36911952440891793 | 0.3528712823578756 | 82.67966280295047 | 0.1877683456752756\n",
      "6 | 0.36490379636167386 | 0.348559291599548 | 83.00632244467862 | 0.2559656672829411\n",
      "7 | 0.3613596381078437 | 0.34556575231764414 | 83.04741833508957 | 0.26501216650236964\n",
      "8 | 0.35875851797674774 | 0.3432233663250322 | 83.15700737618546 | 0.28230912313778056\n",
      "9 | 0.35609808973170565 | 0.3413335266586852 | 83.25289778714438 | 0.3021498907116176\n",
      "10 | 0.3551306658245008 | 0.33977083616877257 | 83.19810326659642 | 0.3241050741272337\n",
      "11 | 0.3536514490273203 | 0.33847642587880566 | 83.28029504741835 | 0.31926139899325784\n",
      "12 | 0.35259693063544933 | 0.3373318360480544 | 83.45837723919917 | 0.35081303777580125\n",
      "13 | 0.3506646194817094 | 0.3363917803723518 | 83.34878819810328 | 0.3394472386306221\n",
      "14 | 0.34879833258209975 | 0.335440895328783 | 83.60695468914648 | 0.3583157927989779\n",
      "15 | 0.34802697859818416 | 0.33462480976156994 | 83.6754478398314 | 0.36923676048613585\n",
      "16 | 0.3467733932698903 | 0.33397441020567126 | 83.75763962065332 | 0.37791692938317056\n",
      "17 | 0.34638423428212833 | 0.3332917023807356 | 83.82613277133825 | 0.3865797334485574\n",
      "18 | 0.34475954844180595 | 0.33269109679003284 | 83.75763962065332 | 0.38367374465417214\n",
      "19 | 0.3436557540060548 | 0.3322204125866498 | 83.63435194942045 | 0.373119436102175\n",
      "20 | 0.34249661584877455 | 0.33164946438923276 | 83.75763962065332 | 0.38408166745331174\n",
      "21 | 0.3425075796515388 | 0.3311480130643061 | 83.77133825079031 | 0.3892976540020081\n",
      "22 | 0.34171353241810376 | 0.3307428905000425 | 83.7850368809273 | 0.3837759906998424\n",
      "23 | 0.340330380638805 | 0.33038601289465 | 83.94942044257112 | 0.4186073639509521\n",
      "24 | 0.33991043859756687 | 0.32987641471706025 | 83.81243414120127 | 0.3974900011179737\n",
      "25 | 0.33766098429850483 | 0.3294071920522272 | 83.86722866174921 | 0.40272927238103734\n",
      "26 | 0.3378870385023979 | 0.32903926360280544 | 83.92202318229715 | 0.39968209176774006\n",
      "27 | 0.3373032981808944 | 0.3288023123798305 | 83.81243414120127 | 0.3897767562580532\n",
      "28 | 0.3356987988285789 | 0.3283313001467757 | 84.01791359325605 | 0.4095078690184505\n",
      "29 | 0.3359485152960735 | 0.32803172495675414 | 84.03161222339304 | 0.4119585292097068\n",
      "30 | 0.3349347513522212 | 0.327723585681556 | 84.00421496311907 | 0.41640931466318537\n",
      "31 | 0.3341449183334998 | 0.3274280994314037 | 83.99051633298208 | 0.4211992675280983\n",
      "32 | 0.33313890095424215 | 0.32712478996956185 | 84.03161222339304 | 0.4231841968021407\n",
      "33 | 0.3323019112172659 | 0.32683301747661747 | 84.03161222339304 | 0.41993678166914944\n",
      "34 | 0.3322476752626422 | 0.3265631785743857 | 84.03161222339304 | 0.4207186012462954\n",
      "35 | 0.33029351333321416 | 0.32627849974860884 | 84.07270811380401 | 0.42483798317909177\n",
      "36 | 0.3297709796053645 | 0.32604205465480074 | 84.086406743941 | 0.4190926084422416\n",
      "37 | 0.33014183350203596 | 0.3258265907021418 | 84.086406743941 | 0.41925153887334216\n",
      "38 | 0.32826156507528154 | 0.3255058641303076 | 84.12750263435196 | 0.4292776803315842\n",
      "39 | 0.3279850673078579 | 0.3252988386235825 | 84.19599578503689 | 0.4305334827165693\n",
      "40 | 0.32721379600943773 | 0.3251133034490559 | 84.14120126448894 | 0.4302594234472176\n",
      "41 | 0.32752280525111277 | 0.3249239826651469 | 84.10010537407798 | 0.42729130865396187\n",
      "42 | 0.3265613912966456 | 0.324716157496792 | 84.07270811380401 | 0.4321423026915202\n",
      "43 | 0.3254896551142774 | 0.32458800893940337 | 84.07270811380401 | 0.4218273799733997\n",
      "44 | 0.32427828735031117 | 0.32437673037591047 | 84.07270811380401 | 0.4247244748676107\n",
      "45 | 0.324091925579109 | 0.32415219192227274 | 84.20969441517387 | 0.4454409916896558\n",
      "46 | 0.3240724103439839 | 0.3239983064058709 | 84.1822971548999 | 0.4368023753612625\n",
      "47 | 0.32425876951773597 | 0.32381122977766275 | 84.1822971548999 | 0.44978707144198443\n",
      "48 | 0.32301202676165 | 0.32364637190348483 | 84.12750263435196 | 0.44603478659159973\n",
      "49 | 0.32227445607976446 | 0.32362694405529596 | 84.23709167544784 | 0.43315842887887923\n",
      "50 | 0.32106018067545483 | 0.32343904704672016 | 84.19599578503689 | 0.4606229014756969\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.24%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 06:14:38,326] Trial 17 finished with value: 0.32343904704672016 and parameters: {'embed_dim': 296, 'learning_rate': 0.0022570457779730667, 'dropout': 0.49153640606299986}. Best is trial 16 with value: 0.3181364592419912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32343904704672016\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.4336033802285836 | 0.3723946041236185 | 81.7481559536354 | 0.007588264095113411\n",
      "2 | 0.3721082544221973 | 0.3501238589213319 | 82.73234984193888 | 0.20175452177869582\n",
      "3 | 0.35867834650549685 | 0.3417155961059544 | 83.29188619599579 | 0.28905781847804224\n",
      "4 | 0.35487353677550953 | 0.33766469073622196 | 83.59325605900949 | 0.3240924831261986\n",
      "5 | 0.3513901022687235 | 0.33527534863311953 | 83.74394099051634 | 0.38364080846368886\n",
      "6 | 0.34864633407910844 | 0.3333013421052123 | 83.75763962065332 | 0.3525007638286173\n",
      "7 | 0.3454601122493591 | 0.3316996537455141 | 83.96311907270811 | 0.38758192066593794\n",
      "8 | 0.34261559027415167 | 0.3304529862656985 | 84.07270811380401 | 0.40442387972929156\n",
      "9 | 0.3415343760304859 | 0.32956291827028744 | 84.05900948366701 | 0.4078530522326632\n",
      "10 | 0.3387836530485649 | 0.3287168613647761 | 84.07270811380401 | 0.4116711545303448\n",
      "11 | 0.33771496549795527 | 0.3278734529875729 | 84.14120126448894 | 0.41604513490256156\n",
      "12 | 0.33475707772128077 | 0.3271376373220796 | 84.11380400421497 | 0.4207774350842022\n",
      "13 | 0.332102805170991 | 0.326401476480373 | 84.05900948366701 | 0.42504703187267534\n",
      "14 | 0.3319856294342502 | 0.3259133607761501 | 84.12750263435196 | 0.43297023135014\n",
      "15 | 0.33017363777492387 | 0.3254317769857302 | 84.1822971548999 | 0.4208663883257086\n",
      "16 | 0.3284415260914269 | 0.32490671232138596 | 84.33298208640674 | 0.4494395843838651\n",
      "17 | 0.32729753339235934 | 0.3244359991321825 | 84.10010537407798 | 0.4305741640518417\n",
      "18 | 0.32645446115631943 | 0.32414771249033003 | 84.31928345626976 | 0.4504437822608099\n",
      "19 | 0.3241686886034063 | 0.3237944319844246 | 84.34668071654373 | 0.46410690724468084\n",
      "20 | 0.3221483983851354 | 0.32331888755298643 | 84.36037934668072 | 0.45646664568273604\n",
      "21 | 0.32153942396106705 | 0.3229609214483875 | 84.42887249736565 | 0.45618179326570807\n",
      "22 | 0.319506678855565 | 0.32273739515102073 | 84.48366701791359 | 0.45877328109086013\n",
      "23 | 0.31843251433075387 | 0.32228638211341754 | 84.41517386722866 | 0.45647672896740543\n",
      "24 | 0.3168805708897952 | 0.32202172728434003 | 84.4699683877766 | 0.464753895245591\n",
      "25 | 0.31605645148396855 | 0.3219678122295092 | 84.51106427818756 | 0.46974701454747014\n",
      "26 | 0.31400745496472815 | 0.3218170180916786 | 84.45626975763962 | 0.4572740905513217\n",
      "27 | 0.31386229473334204 | 0.32156682218590826 | 84.42887249736565 | 0.46758571860647763\n",
      "28 | 0.31242862448597536 | 0.3213794551891823 | 84.53846153846153 | 0.477180870439956\n",
      "29 | 0.31007621879903 | 0.32113826427965947 | 84.51106427818756 | 0.4771127329550493\n",
      "30 | 0.3110002377369535 | 0.3210891734247338 | 84.5795574288725 | 0.4790674379996238\n",
      "31 | 0.30867898625973894 | 0.32084943997125104 | 84.49736564805058 | 0.4712553702208634\n",
      "32 | 0.3084826874988159 | 0.32078033726509303 | 84.41517386722866 | 0.46320212291174495\n",
      "33 | 0.30625273796533226 | 0.3206005954783257 | 84.51106427818756 | 0.4689083688619581\n",
      "34 | 0.3059722948192821 | 0.32061944065028675 | 84.44257112750263 | 0.45873634869507085\n",
      "35 | 0.30465192193398966 | 0.32029237781893716 | 84.4699683877766 | 0.4739274306661323\n",
      "36 | 0.30207088598212517 | 0.3201553781759249 | 84.48366701791359 | 0.46624223761132494\n",
      "37 | 0.3026664664382143 | 0.3200981746593567 | 84.51106427818756 | 0.4734332650741701\n",
      "38 | 0.3005629678396366 | 0.3199085045146616 | 84.48366701791359 | 0.480830139505672\n",
      "39 | 0.2980836008416221 | 0.32005477788513653 | 84.4699683877766 | 0.4638959627026005\n",
      "40 | 0.2976527447592957 | 0.32001108421038277 | 84.52476290832455 | 0.47006569249539115\n",
      "41 | 0.29741508105843806 | 0.31987764318920164 | 84.44257112750263 | 0.4784979488493221\n",
      "42 | 0.29549701115976595 | 0.320065890579191 | 84.38777660695469 | 0.49247730940585477\n",
      "43 | 0.29365226024881413 | 0.319928973402879 | 84.38777660695469 | 0.4929077956134095\n",
      "44 | 0.29399512230848684 | 0.319793566755236 | 84.51106427818756 | 0.4669018710521685\n",
      "45 | 0.2921053744989251 | 0.3196636644536502 | 84.55216016859852 | 0.4803695246477685\n",
      "46 | 0.2908149561773564 | 0.3197157924918279 | 84.45626975763962 | 0.49336587021205275\n",
      "47 | 0.2913707373578341 | 0.31983296634399727 | 84.48366701791359 | 0.4827618171590448\n",
      "48 | 0.2899558204838533 | 0.31991873722370356 | 84.52476290832455 | 0.48747069588989766\n",
      "49 | 0.28584384050687334 | 0.32001659223069884 | 84.4699683877766 | 0.48508763476739086\n",
      "50 | 0.28633851786386166 | 0.3200413852113567 | 84.53846153846153 | 0.491207305102657\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.58%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 06:52:42,474] Trial 18 finished with value: 0.3200413852113567 and parameters: {'embed_dim': 278, 'learning_rate': 0.00571744110071781, 'dropout': 0.3686760639572524}. Best is trial 16 with value: 0.3181364592419912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3200413852113567\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.43884990354060033 | 0.3793899393449091 | 81.69336143308746 | 0.0\n",
      "2 | 0.3784381795093554 | 0.35396053080689416 | 82.39199157007377 | 0.1540565704705405\n",
      "3 | 0.36483315058303173 | 0.34499844296337806 | 82.54267650158062 | 0.22220386934981906\n",
      "4 | 0.3595437413303677 | 0.34019251769944414 | 83.11380400421497 | 0.30481686663059826\n",
      "5 | 0.3543876403834477 | 0.33721736645045347 | 83.5795574288725 | 0.35292086003543505\n",
      "6 | 0.35138380044461753 | 0.3354039766812978 | 83.53846153846153 | 0.35151867671894244\n",
      "7 | 0.3494893891867877 | 0.3339319537355475 | 83.59325605900949 | 0.38991095641332035\n",
      "8 | 0.34749354792343 | 0.33275897382465125 | 83.44257112750263 | 0.3674854901585992\n",
      "9 | 0.3438259349959341 | 0.3316926399934782 | 83.75763962065332 | 0.40938564696396834\n",
      "10 | 0.3423625600811903 | 0.3308028474654237 | 83.7850368809273 | 0.4048932278186801\n",
      "11 | 0.34073733414212865 | 0.33008948039926894 | 83.73024236037935 | 0.3897206890896928\n",
      "12 | 0.33995028176211073 | 0.32931375779109456 | 83.89462592202318 | 0.4127784001856349\n",
      "13 | 0.3373847568595628 | 0.32883482007947684 | 83.74394099051634 | 0.3906915367191137\n",
      "14 | 0.33606043999120366 | 0.32823143981090963 | 83.92202318229715 | 0.4084814166568759\n",
      "15 | 0.336423895273369 | 0.32792483800894595 | 83.8809272918862 | 0.4025011205318755\n",
      "16 | 0.33317967585148434 | 0.3273421117704209 | 83.92202318229715 | 0.4154014621993404\n",
      "17 | 0.33285714211996176 | 0.3269256965754783 | 84.05900948366701 | 0.4323906737422805\n",
      "18 | 0.33047400843295116 | 0.3265076631756678 | 84.04531085353003 | 0.42530751508590564\n",
      "19 | 0.33049929851740145 | 0.3262864280849287 | 84.11380400421497 | 0.4211991081190229\n",
      "20 | 0.32960287068005 | 0.3259743077501859 | 84.19599578503689 | 0.42833141515025897\n",
      "21 | 0.3274678760849008 | 0.3256099686434824 | 84.10010537407798 | 0.43333675517404757\n",
      "22 | 0.3273554579539948 | 0.3253195824688428 | 84.10010537407798 | 0.4295123354758631\n",
      "23 | 0.3249949671510345 | 0.3250868108174572 | 84.10010537407798 | 0.4431977361710007\n",
      "24 | 0.32503771616732674 | 0.32477813806027583 | 84.1822971548999 | 0.44397136678898436\n",
      "25 | 0.32338859250239277 | 0.3245768022455581 | 84.10010537407798 | 0.4487320708817212\n",
      "26 | 0.32211770228927655 | 0.32444348523061567 | 84.19599578503689 | 0.44204908547389504\n",
      "27 | 0.3216020257328993 | 0.32455701380968094 | 84.19599578503689 | 0.4254525007600524\n",
      "28 | 0.32047760788499396 | 0.3239456773416637 | 84.19599578503689 | 0.4536678399532063\n",
      "29 | 0.3179608166605143 | 0.3237836077196957 | 84.23709167544784 | 0.46287255722354753\n",
      "30 | 0.31820685647724234 | 0.32358324058251836 | 84.19599578503689 | 0.4508274186712028\n",
      "31 | 0.31808022344741255 | 0.32346518623502285 | 84.23709167544784 | 0.45338412917280535\n",
      "32 | 0.3171762894577025 | 0.3236360627494446 | 84.36037934668072 | 0.44115364256928136\n",
      "33 | 0.31622279230334344 | 0.3232606705533315 | 84.34668071654373 | 0.4552444578661948\n",
      "34 | 0.31407923987334657 | 0.32313038102568015 | 84.30558482613277 | 0.46725598561449533\n",
      "35 | 0.31481985905028265 | 0.32319675824821814 | 84.34668071654373 | 0.4759822517666765\n",
      "36 | 0.3127793436727575 | 0.3228408227840515 | 84.38777660695469 | 0.45521248904539946\n",
      "37 | 0.3129963103442564 | 0.32276720073941634 | 84.40147523709167 | 0.45775653524428683\n",
      "38 | 0.31128581632099567 | 0.3226362305552992 | 84.36037934668072 | 0.4640127621183472\n",
      "39 | 0.31177492252912725 | 0.32255738534747735 | 84.42887249736565 | 0.46931872788251533\n",
      "40 | 0.30971364581256833 | 0.3223861723321758 | 84.34668071654373 | 0.4687598348032651\n",
      "41 | 0.30962299962815704 | 0.322403610977408 | 84.45626975763962 | 0.4597914881996224\n",
      "42 | 0.3079957089782764 | 0.3222029529613991 | 84.53846153846153 | 0.4697555067334211\n",
      "43 | 0.3074882540065761 | 0.32204802334308624 | 84.52476290832455 | 0.46695096357782406\n",
      "44 | 0.3066155769113189 | 0.3220168715470458 | 84.49736564805058 | 0.4657201799446125\n",
      "45 | 0.305096836720767 | 0.32189730330281063 | 84.60695468914648 | 0.47881206737972193\n",
      "46 | 0.304408182558071 | 0.32178683793299817 | 84.52476290832455 | 0.47742610125781665\n",
      "47 | 0.30389665524028125 | 0.3217122138363041 | 84.49736564805058 | 0.47746361810902355\n",
      "48 | 0.3037400364203646 | 0.32161653603184714 | 84.60695468914648 | 0.4767809330822753\n",
      "49 | 0.30267375820979975 | 0.32160847879027665 | 84.52476290832455 | 0.484623352802943\n",
      "50 | 0.3023091759418767 | 0.32174533309593595 | 84.48366701791359 | 0.4731174596935543\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.61%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 07:19:22,321] Trial 19 finished with value: 0.32174533289181045 and parameters: {'embed_dim': 187, 'learning_rate': 0.0055696053369551635, 'dropout': 0.498486389699978}. Best is trial 16 with value: 0.3181364592419912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32174533289181045\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.47285467885229565 | 0.40708067992778674 | 81.69336143308746 | 0.0\n",
      "2 | 0.4022347271920891 | 0.3780317168733845 | 81.69336143308746 | 0.0\n",
      "3 | 0.382522043148312 | 0.3624499691267536 | 81.99473129610116 | 0.0589658058836141\n",
      "4 | 0.37272933474101055 | 0.3534617340319777 | 82.47418335089569 | 0.15647758768226133\n",
      "5 | 0.36633014562753363 | 0.34780093017098024 | 82.72075869336143 | 0.2204173605237927\n",
      "6 | 0.36236243736033047 | 0.3439697857997189 | 83.15700737618546 | 0.28278514470280536\n",
      "7 | 0.35908925746331155 | 0.3413717426257591 | 83.25289778714438 | 0.2931414058648683\n",
      "8 | 0.3578461565601352 | 0.3394127815757712 | 83.36248682824026 | 0.31240417042217467\n",
      "9 | 0.35518452547923507 | 0.3377535541171897 | 83.53846153846153 | 0.3286240408852045\n",
      "10 | 0.3529276949572818 | 0.3364321300632333 | 83.52476290832455 | 0.33499117985878957\n",
      "11 | 0.35181089559006035 | 0.3352224023578918 | 83.70284510010538 | 0.36045064654026293\n",
      "12 | 0.3510253132130119 | 0.33420234581787295 | 83.75763962065332 | 0.3664138099615459\n",
      "13 | 0.3500660220873101 | 0.33328641971496686 | 83.79873551106428 | 0.3701419568705953\n",
      "14 | 0.3477325913373848 | 0.33247049499864445 | 83.90832455216017 | 0.38162669277515393\n",
      "15 | 0.34685939865967186 | 0.3317964795517595 | 83.92202318229715 | 0.37628602222338453\n",
      "16 | 0.3452063122428156 | 0.3310724819359714 | 83.90832455216017 | 0.3904397681091479\n",
      "17 | 0.34429712432923665 | 0.33076078975445605 | 83.92202318229715 | 0.36331280859545695\n",
      "18 | 0.3440423910976003 | 0.32992276722845965 | 84.00421496311907 | 0.40018084536784393\n",
      "19 | 0.3425612518569563 | 0.329392550130413 | 84.07270811380401 | 0.39798438018259635\n",
      "20 | 0.3405250925513765 | 0.3288693425998296 | 84.05900948366701 | 0.39833509491687275\n",
      "21 | 0.34076888101682384 | 0.3284069203758893 | 84.16859852476291 | 0.415982599607403\n",
      "22 | 0.3401916881265626 | 0.3279678999152902 | 84.1822971548999 | 0.41036177696309595\n",
      "23 | 0.3391458611787277 | 0.3276808809948294 | 84.26448893572181 | 0.4004173590482382\n",
      "24 | 0.33924329145284604 | 0.3272228668609711 | 84.25079030558483 | 0.415025211077709\n",
      "25 | 0.3383981417010137 | 0.32688671279035203 | 84.30558482613277 | 0.42338867729377566\n",
      "26 | 0.33689136905681105 | 0.3265454013870187 | 84.26448893572181 | 0.4259764334053241\n",
      "27 | 0.3357340910839378 | 0.3261616580086212 | 84.31928345626976 | 0.42458853212822056\n",
      "28 | 0.3345324850501635 | 0.3258462507430821 | 84.34668071654373 | 0.42891893970940215\n",
      "29 | 0.3345628285991307 | 0.32556072265318 | 84.40147523709167 | 0.4188792943943714\n",
      "30 | 0.33366180719403316 | 0.3252168361046543 | 84.41517386722866 | 0.43158937984201845\n",
      "31 | 0.3327570807314065 | 0.324925314992258 | 84.4699683877766 | 0.4353094910629779\n",
      "32 | 0.3320179839336544 | 0.32470913557973624 | 84.63435194942045 | 0.4546287540818691\n",
      "33 | 0.3309013004605559 | 0.3243674599144557 | 84.51106427818756 | 0.43654114423729334\n",
      "34 | 0.3317042123127603 | 0.3241982120153022 | 84.59325605900949 | 0.44960785890277194\n",
      "35 | 0.32970100872514807 | 0.3239551556028732 | 84.63435194942045 | 0.45156009347990583\n",
      "36 | 0.3293401674347551 | 0.3237758172701483 | 84.56585879873552 | 0.43735763681446277\n",
      "37 | 0.3292197877319034 | 0.32348811922416293 | 84.60695468914648 | 0.444693771950357\n",
      "38 | 0.3279036669465015 | 0.3233217045868913 | 84.66174920969442 | 0.450934785845311\n",
      "39 | 0.3286435449729454 | 0.32319731969539434 | 84.71654373024236 | 0.44561049249884727\n",
      "40 | 0.32708155842275793 | 0.3230446288234567 | 84.70284510010538 | 0.44464778256692744\n",
      "41 | 0.32660579535574724 | 0.32290278354736224 | 84.77133825079031 | 0.46173203148201053\n",
      "42 | 0.3266140686672762 | 0.32275902234936416 | 84.68914646996839 | 0.4445713690129491\n",
      "43 | 0.3256691002289819 | 0.32255903498767174 | 84.77133825079031 | 0.45700177597529495\n",
      "44 | 0.32402686531504543 | 0.32250623527455 | 84.77133825079031 | 0.44628466418811713\n",
      "45 | 0.32388771079275586 | 0.322254235205585 | 84.74394099051634 | 0.4586120293270343\n",
      "46 | 0.3232325919496539 | 0.3220505882820038 | 84.73024236037935 | 0.4585717423689391\n",
      "47 | 0.32283048296421313 | 0.3218901263524408 | 84.71654373024236 | 0.4590780961488627\n",
      "48 | 0.3218135196225723 | 0.3217728353322369 | 84.74394099051634 | 0.4624156020518883\n",
      "49 | 0.3215936768105088 | 0.3216686103850195 | 84.70284510010538 | 0.4502853149758177\n",
      "50 | 0.32122404329957216 | 0.3215570101795131 | 84.71654373024236 | 0.470601826923709\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.77%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 07:51:56,471] Trial 20 finished with value: 0.3215570099753876 and parameters: {'embed_dim': 232, 'learning_rate': 0.0026843529160480648, 'dropout': 0.45347959625114853}. Best is trial 16 with value: 0.3181364592419912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3215570099753876\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.43661052470497036 | 0.3766314750664855 | 81.70706006322445 | 0.0009132420091324201\n",
      "2 | 0.3767260561158898 | 0.3523962083336425 | 82.50158061116966 | 0.14760570256138345\n",
      "3 | 0.36310114825446305 | 0.3431710753947088 | 83.14330874604848 | 0.2810698642808368\n",
      "4 | 0.357870997240146 | 0.3386311463705481 | 83.62065331928346 | 0.3500349013991819\n",
      "5 | 0.35365625616698454 | 0.33567386059320137 | 83.74394099051634 | 0.36259537109091894\n",
      "6 | 0.35074347820394997 | 0.33358257294517674 | 83.85353003161222 | 0.38649444429752355\n",
      "7 | 0.3477549379640216 | 0.33195696281243675 | 83.93572181243414 | 0.3752116704858367\n",
      "8 | 0.3464437086670588 | 0.33077563161719337 | 83.99051633298208 | 0.37759834175689666\n",
      "9 | 0.3435997106590585 | 0.3295204293442099 | 83.99051633298208 | 0.38559852552683316\n",
      "10 | 0.34099100214580147 | 0.32863918987855517 | 84.10010537407798 | 0.38434388712665996\n",
      "11 | 0.33952830382124366 | 0.3275894481841832 | 84.20969441517387 | 0.41340743481828623\n",
      "12 | 0.338372758415952 | 0.326724734718669 | 84.23709167544784 | 0.416715027905026\n",
      "13 | 0.33700570599702884 | 0.3259521394151531 | 84.44257112750263 | 0.4243353893388167\n",
      "14 | 0.3342427646325451 | 0.3253837061459071 | 84.38777660695469 | 0.43650952562392037\n",
      "15 | 0.3332102194041105 | 0.3248591973152879 | 84.48366701791359 | 0.43679804634594915\n",
      "16 | 0.3312426324398328 | 0.3243062468628361 | 84.64805057955743 | 0.4547730501394998\n",
      "17 | 0.32990905293904316 | 0.32424942771457643 | 84.33298208640674 | 0.40658763996720854\n",
      "18 | 0.3294505031625612 | 0.32356539276772983 | 84.75763962065332 | 0.46618845998130964\n",
      "19 | 0.32749462561733134 | 0.3229999414657893 | 84.7850368809273 | 0.4600265718382637\n",
      "20 | 0.3251182698966531 | 0.322601124031903 | 84.7850368809273 | 0.452755179029344\n",
      "21 | 0.3249889531312368 | 0.32233453501168996 | 84.70284510010538 | 0.46353761224852547\n",
      "22 | 0.3241604843065826 | 0.3220618408020229 | 84.74394099051634 | 0.4709367172106308\n",
      "23 | 0.3226688511282297 | 0.3219917286952881 | 84.70284510010538 | 0.4378041632496868\n",
      "24 | 0.3226011830413378 | 0.32151594186482363 | 84.86722866174921 | 0.47083150932820733\n",
      "25 | 0.32112097850678895 | 0.3212859207024313 | 84.79873551106428 | 0.4713447727422511\n",
      "26 | 0.3196744549256217 | 0.32106483635837085 | 84.81243414120127 | 0.47755757538867427\n",
      "27 | 0.3177479151465477 | 0.3206145220423398 | 84.85353003161222 | 0.4745214188110848\n",
      "28 | 0.31643331206947656 | 0.3204373490728744 | 84.89462592202318 | 0.4788592793667444\n",
      "29 | 0.3161298755886722 | 0.3202652705654706 | 84.86722866174921 | 0.459551075212856\n",
      "30 | 0.3145930651028495 | 0.3199017604124056 | 84.93572181243414 | 0.47671162553266916\n",
      "31 | 0.3136606754148407 | 0.31973438567086443 | 84.9768177028451 | 0.4820416878153405\n",
      "32 | 0.312411119665848 | 0.319751952915159 | 84.75763962065332 | 0.4946598861674476\n",
      "33 | 0.3110381945830966 | 0.31938188635323145 | 85.07270811380401 | 0.4823660585968886\n",
      "34 | 0.31143422053403447 | 0.3193492080046706 | 84.89462592202318 | 0.4902544580062409\n",
      "35 | 0.30893519925187124 | 0.319185259713702 | 84.86722866174921 | 0.49215831162984525\n",
      "36 | 0.3081262041771904 | 0.3190057698177965 | 84.85353003161222 | 0.47635080196467267\n",
      "37 | 0.30781548250956453 | 0.3187109079875358 | 84.92202318229715 | 0.48620839794857695\n",
      "38 | 0.30610025054362205 | 0.3187693025763721 | 84.77133825079031 | 0.4894544376638313\n",
      "39 | 0.3062288895568352 | 0.31871022284030914 | 84.81243414120127 | 0.47730644752596163\n",
      "40 | 0.304507269002839 | 0.3187146996186204 | 84.86722866174921 | 0.4762194466096634\n",
      "41 | 0.3033471356860906 | 0.3186184552638498 | 84.77133825079031 | 0.48883350649386226\n",
      "42 | 0.3029141657293663 | 0.3185557270907376 | 84.75763962065332 | 0.47260737216885784\n",
      "43 | 0.3020320842868509 | 0.3184136687076255 | 84.83983140147524 | 0.49135910226079577\n",
      "44 | 0.29957514522819345 | 0.31864441451552794 | 84.81243414120127 | 0.4727600156145383\n",
      "45 | 0.2988073610350353 | 0.31826798964853154 | 84.82613277133825 | 0.49312342392976094\n",
      "46 | 0.2976589456872747 | 0.31818088460458466 | 84.79873551106428 | 0.4854687504086724\n",
      "47 | 0.2968957905888284 | 0.3181285317221733 | 84.83983140147524 | 0.48845158047340287\n",
      "48 | 0.295493101758102 | 0.31812271884042925 | 84.75763962065332 | 0.4940101025116683\n",
      "49 | 0.29427299655358724 | 0.3183325051444851 | 84.85353003161222 | 0.4797828252595021\n",
      "50 | 0.29407395497078376 | 0.3181242602941108 | 84.68914646996839 | 0.4991502064139153\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 85.07%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 08:25:04,804] Trial 21 finished with value: 0.3181242602941108 and parameters: {'embed_dim': 232, 'learning_rate': 0.005672788661459371, 'dropout': 0.4200814213636909}. Best is trial 21 with value: 0.3181242602941108.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3181242602941108\n",
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143122/3687490577.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/tmp/ipykernel_143122/3687490577.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 0.42557176403040553 | 0.3690834725148057 | 81.87144362486828 | 0.02951048190774218\n",
      "2 | 0.37102971166736853 | 0.34894371338903085 | 83.00632244467862 | 0.2655796899509294\n",
      "3 | 0.35983116319830266 | 0.3412897102024457 | 83.23709167544784 | 0.29970446506790105\n",
      "4 | 0.35360261800183435 | 0.33729796591278627 | 83.38777660695469 | 0.3660688780209937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-05 08:28:48,752] Trial 22 failed with parameters: {'embed_dim': 268, 'learning_rate': 0.006426567586391541, 'dropout': 0.4609735941332162} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xuxa/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_143122/3687490577.py\", line 19, in objective\n",
      "    train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=50)\n",
      "  File \"/tmp/ipykernel_143122/3709412887.py\", line 46, in train\n",
      "    logits = model(b_input_ids)\n",
      "  File \"/home/xuxa/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/xuxa/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_143122/1224269538.py\", line 78, in forward\n",
      "    x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
      "  File \"/tmp/ipykernel_143122/1224269538.py\", line 78, in <listcomp>\n",
      "    x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
      "  File \"/home/xuxa/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/xuxa/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/xuxa/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 310, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/xuxa/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 306, in _conv_forward\n",
      "    return F.conv1d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n",
      "[W 2023-12-05 08:28:48,754] Trial 22 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create a study object and optimize the objective function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Print the best hyperparameters and their corresponding validation accuracy\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# print('Best trial:')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# trial = study.best_trial\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# print('Validation Accuracy: {:.3f}'.format(trial.value))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# print('Best Hyperparameters:', trial.params)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     13\u001b[0m cnn_rand, optimizer \u001b[39m=\u001b[39m initialize_model(vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(word2idx),\n\u001b[1;32m     14\u001b[0m                                        embed_dim\u001b[39m=\u001b[39membed_dim,\n\u001b[1;32m     15\u001b[0m                                        learning_rate\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m     16\u001b[0m                                        dropout\u001b[39m=\u001b[39mdropout)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Evaluate the model and return the validation accuracy as the objective value\u001b[39;00m\n\u001b[1;32m     22\u001b[0m val_loss, val_accuracy, f1_value \u001b[39m=\u001b[39m evaluate(cnn_rand, val_dataloader)\n",
      "Cell \u001b[0;32mIn[20], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     45\u001b[0m \u001b[39m# Perform a forward pass. This will return logits.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m logits \u001b[39m=\u001b[39m model(b_input_ids)\n\u001b[1;32m     48\u001b[0m \u001b[39m# Compute loss and accumulate the loss values\u001b[39;00m\n\u001b[1;32m     49\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, b_labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 78\u001b[0m, in \u001b[0;36mCNN_NLP.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     75\u001b[0m x_reshaped \u001b[39m=\u001b[39m x_embed\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[39m# Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m x_conv_list \u001b[39m=\u001b[39m [F\u001b[39m.\u001b[39mrelu(conv1d(x_reshaped)) \u001b[39mfor\u001b[39;00m conv1d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1d_list]\n\u001b[1;32m     80\u001b[0m \u001b[39m# Max pooling. Output shape: (b, num_filters[i], 1)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m x_pool_list \u001b[39m=\u001b[39m [F\u001b[39m.\u001b[39mmax_pool1d(x_conv, kernel_size\u001b[39m=\u001b[39mx_conv\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m])\n\u001b[1;32m     82\u001b[0m     \u001b[39mfor\u001b[39;00m x_conv \u001b[39min\u001b[39;00m x_conv_list]\n",
      "Cell \u001b[0;32mIn[18], line 78\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m x_reshaped \u001b[39m=\u001b[39m x_embed\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[39m# Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m x_conv_list \u001b[39m=\u001b[39m [F\u001b[39m.\u001b[39mrelu(conv1d(x_reshaped)) \u001b[39mfor\u001b[39;00m conv1d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1d_list]\n\u001b[1;32m     80\u001b[0m \u001b[39m# Max pooling. Output shape: (b, num_filters[i], 1)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m x_pool_list \u001b[39m=\u001b[39m [F\u001b[39m.\u001b[39mmax_pool1d(x_conv, kernel_size\u001b[39m=\u001b[39mx_conv\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m])\n\u001b[1;32m     82\u001b[0m     \u001b[39mfor\u001b[39;00m x_conv \u001b[39min\u001b[39;00m x_conv_list]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters and their corresponding validation accuracy\n",
    "# print('Best trial:')\n",
    "# trial = study.best_trial\n",
    "# print('Validation Accuracy: {:.3f}'.format(trial.value))\n",
    "# print('Best Hyperparameters:', trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters and their corresponding validation accuracy\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Validation Accuracy: {:.3f}'.format(trial.value))\n",
    "print('Best Hyperparameters:', trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Epoch | Train Loss | Val Loss | Val Acc | F1-score\n",
      "------------------------------------------------------------\n",
      "1 | 0.3589555164087074 | 0.3397639045772487 | 83.05900948366701 | 0.22782265152276604\n",
      "2 | 0.33430641900394853 | 0.3305757246809463 | 83.7850368809273 | 0.31662686787832695\n",
      "3 | 0.3213110337438295 | 0.32292095469693616 | 84.19599578503689 | 0.41895430079075435\n",
      "4 | 0.3134392347152627 | 0.32153395042844013 | 84.71654373024236 | 0.4273816042405493\n",
      "5 | 0.30223780927968863 | 0.320953333112475 | 84.70284510010538 | 0.4786553577790299\n",
      "6 | 0.2913446164482048 | 0.3220095121084827 | 84.77133825079031 | 0.480933064966218\n",
      "7 | 0.2819221992535511 | 0.3240414533304842 | 84.62065331928346 | 0.5093598484627935\n",
      "8 | 0.27080093700131147 | 0.32725794695011556 | 84.81243414120127 | 0.47227554815107375\n",
      "9 | 0.26123465762248643 | 0.33292579732529104 | 84.42887249736565 | 0.5030228731537973\n",
      "10 | 0.2484275270538228 | 0.3363199178894905 | 84.53846153846153 | 0.5255026088724173\n",
      "11 | 0.2365801274115704 | 0.3411375588341935 | 84.26448893572181 | 0.49041176607046744\n",
      "12 | 0.2231805183830748 | 0.3526176609200974 | 84.44257112750263 | 0.4861107963176148\n",
      "13 | 0.21282414822562937 | 0.36017731424063854 | 83.9768177028451 | 0.5464745293051927\n",
      "14 | 0.2000255609950797 | 0.3639679856292189 | 84.25079030558483 | 0.4877234912063067\n",
      "15 | 0.19037476069769546 | 0.3664744063599469 | 83.90832455216017 | 0.4964939114130592\n",
      "16 | 0.17683904903972186 | 0.3792520096040752 | 84.41517386722866 | 0.4716989665431536\n",
      "17 | 0.16672066029229568 | 0.3924513746001949 | 84.05900948366701 | 0.47764599166654603\n",
      "18 | 0.15799149898214077 | 0.3969649889901893 | 83.73024236037935 | 0.5036724670905508\n",
      "19 | 0.1490884842066271 | 0.4094927950877033 | 83.92202318229715 | 0.4622339921748743\n",
      "20 | 0.14133609490560237 | 0.4179585644439475 | 84.086406743941 | 0.49512092961137416\n",
      "21 | 0.13501188021996244 | 0.42641537013935715 | 84.16859852476291 | 0.4945773818932921\n",
      "22 | 0.1282530347432155 | 0.4337130861535464 | 83.94942044257112 | 0.48594229548948376\n",
      "23 | 0.12226539472721813 | 0.4402260359835951 | 83.70284510010538 | 0.4920949601799088\n",
      "24 | 0.1147259499205544 | 0.466189967544928 | 83.83983140147524 | 0.45153125547139106\n",
      "25 | 0.11211842387937443 | 0.4630326688902019 | 83.62065331928346 | 0.46566369764696924\n",
      "26 | 0.10727402658646054 | 0.46100900363023967 | 83.83983140147524 | 0.513479110714655\n",
      "27 | 0.10171988651808636 | 0.4809750447126284 | 83.3740779768177 | 0.5074330554283469\n",
      "28 | 0.0985944630985436 | 0.48864197332973347 | 83.68914646996839 | 0.47342442718894595\n",
      "29 | 0.0960060542724195 | 0.4971021142316191 | 83.75763962065332 | 0.45372628337323806\n",
      "30 | 0.09184273700543112 | 0.49976466228700667 | 83.2781875658588 | 0.4778378536119303\n",
      "31 | 0.08985074343507261 | 0.5069447131189582 | 83.29188619599579 | 0.4891084817702772\n",
      "32 | 0.08677358853072498 | 0.5137617758998315 | 83.48366701791359 | 0.5038321132829583\n",
      "33 | 0.08304810261322522 | 0.5237842012349874 | 83.086406743941 | 0.49486411114589596\n",
      "34 | 0.07967263308676979 | 0.5273996484402108 | 83.25079030558483 | 0.49075859734606736\n",
      "35 | 0.07773563497634291 | 0.5304279023245589 | 83.40147523709167 | 0.5115068427903424\n",
      "36 | 0.07354016981830974 | 0.5441863305152279 | 83.74394099051634 | 0.49961514449543615\n",
      "37 | 0.07217368134226229 | 0.5519218707125481 | 83.49736564805058 | 0.5001962663434956\n",
      "38 | 0.06930180801974399 | 0.5629475729106224 | 83.2781875658588 | 0.5097514163051293\n",
      "39 | 0.068625172490855 | 0.5670155584812164 | 83.34668071654373 | 0.4943055616290734\n",
      "40 | 0.06613133332139559 | 0.5725015883066066 | 83.48366701791359 | 0.49070261086649164\n",
      "41 | 0.06413271948759265 | 0.5826855721334888 | 83.41517386722866 | 0.5063579350763622\n",
      "42 | 0.062109732744604386 | 0.5877553019927789 | 83.1822971548999 | 0.4868950033129769\n",
      "43 | 0.061154325950153125 | 0.5986757684141806 | 83.25079030558483 | 0.4881385513876797\n",
      "44 | 0.05916795205274691 | 0.6072668740398264 | 83.45626975763962 | 0.4748281265451757\n",
      "45 | 0.05851538456187495 | 0.6078622279918358 | 83.23709167544784 | 0.4992215766715205\n",
      "46 | 0.056380206622562115 | 0.6059815131434022 | 83.42887249736565 | 0.48504777155421824\n",
      "47 | 0.05455497935854343 | 0.6298763569291324 | 83.60695468914648 | 0.47353357350183917\n",
      "48 | 0.05437036339885666 | 0.6279351679123428 | 83.12750263435196 | 0.4766029837438902\n",
      "49 | 0.053952540191415904 | 0.6249688116654958 | 83.41517386722866 | 0.5094133038766286\n",
      "50 | 0.05348439320308848 | 0.6364269730162947 | 82.94942044257112 | 0.5074291490461951\n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.81%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "set_seed(42)\n",
    "cnn_rand, optimizer = initialize_model(vocab_size=len(word2idx),\n",
    "                                      embed_dim=300,\n",
    "                                      learning_rate=0.1,\n",
    "                                      dropout=0.5)\n",
    "train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model=cnn_rand.to(\"cpu\"), max_len=62):\n",
    "    \"\"\"Predict probability that a review is positive.\"\"\"\n",
    "\n",
    "    # Tokenize, pad and encode text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
    "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
    "\n",
    "    # Compute logits\n",
    "    logits = model.forward(input_id)\n",
    "\n",
    "    #  Compute probability\n",
    "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
    "\n",
    "    print(f\"This review is {probs[1] * 100:.2f}% important.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labels\n",
    "with open(\"training_labels.json\", \"r\") as json_file:\n",
    "    labels = json.load(json_file)\n",
    "\n",
    "# read nodes and edges\n",
    "nodes, edges = custom_utils.gather_dataset(\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data from all dialogs\n",
    "X, y, edge_idx = [], [], [] \n",
    "count = 0\n",
    "for id in nodes.keys():\n",
    "        X += nodes[id]\n",
    "        y += labels[id]\n",
    "        edge_idx += [[e[0] + count, e[1] + count] for e in edges[id]]\n",
    "        count += len(labels[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# stratified split (we do it every epoch)\n",
    "def split(spliter, X, y):\n",
    "    train_idx, val_idx = list(spliter.split(X, y))[0]\n",
    "    train_mask, val_mask = [False]*len(X), [False]*len(X)\n",
    "    for idx in train_idx: train_mask[idx] = True\n",
    "    for idx in val_idx: val_mask[idx] = True\n",
    "    train_mask = torch.Tensor(train_mask).bool()\n",
    "    val_mask = torch.Tensor(val_mask).bool()\n",
    "    return train_mask, val_mask\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state=42)\n",
    "train_mask, val_mask = split(sss, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting\n",
    "y = torch.Tensor(y).long()\n",
    "edge_idx = torch.Tensor(edge_idx).long().transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ME: <vocalsound> Okay .',\n",
       " 'ME: Oh',\n",
       " 'ME: I totally <disfmarker>',\n",
       " 'ME: Yeah',\n",
       " \"ME: 'cause I moved it . <vocalsound>\",\n",
       " \"ME: 'S put it over here .\",\n",
       " \"ME: Then we don't have to worry about it .\",\n",
       " 'UI: <vocalsound> Ready for this ?',\n",
       " 'PM: All set ?',\n",
       " 'PM: Cool .',\n",
       " 'PM: Alright ,',\n",
       " 'PM: it is PowerPoint time .',\n",
       " \"PM: I've done more PowerPoints in this particular experiment than I've ever done in my life before this experiment <vocalsound>\",\n",
       " 'ID: <vocalsound> Yeah <vocalsound> .',\n",
       " 'PM: which is kind of fun .',\n",
       " 'UI: <vocalsound> Oh man . <vocalsound>',\n",
       " 'PM: So uh',\n",
       " 'PM: here we have our detailed design meeting',\n",
       " 'PM: where we will um look at the prototype',\n",
       " 'PM: and',\n",
       " 'PM: um <vocalsound> <disfmarker> right so um ,',\n",
       " 'PM: I finally figured out what this whole second bullet point is about in my <disfmarker>',\n",
       " 'PM: that my coach was sending to me .',\n",
       " \"PM: It means I'm supposed to read the minutes from the previous meeting .\",\n",
       " 'ID: Oh',\n",
       " 'ID: really ?',\n",
       " 'ID: Okay .',\n",
       " 'PM: I think . <vocalsound>',\n",
       " 'ME: Huh .',\n",
       " \"PM: I don't know .\",\n",
       " \"PM: Otherwise it's just saying I'm the secretary\",\n",
       " \"PM: and I'm <disfmarker> therefore I'm taking the minutes ,\",\n",
       " 'PM: s so just to go',\n",
       " 'PM: um <vocalsound> just real briefly to go over minutes from last meeting ,',\n",
       " 'PM: uh , I will open them slowly , no ?',\n",
       " 'PM: Wait for it , wait for it .',\n",
       " 'ME: Yeah',\n",
       " \"ME: that's not you . <vocalsound>\",\n",
       " 'PM: No .',\n",
       " \"PM: That's how the <disfmarker>\",\n",
       " 'PM: <vocalsound> Wait .',\n",
       " 'PM: This is , this is very high-powered stuff here ,',\n",
       " 'PM: double-clicking ,',\n",
       " 'PM: there we go .',\n",
       " 'PM: basically the moral of the story from our last minute uh <disfmarker> last meeting was that <vocalsound> um we that we had meetings from <disfmarker> uh we had presentations done by the Industrial Designer , uh or from Nathan , and Ron and from Sarah about what we can do here',\n",
       " \"PM: um and what sort of limitations we're operating with\",\n",
       " 'PM: um <disfmarker> uh excuse me',\n",
       " \"PM: what limitations we're operating under ,\",\n",
       " \"PM: what kind of risk we'd be looking at with some of the various approaches we were discussing\",\n",
       " 'PM: and we essentially came to the conclusion that we should develop a remote with uh voice recognition ,',\n",
       " 'PM: I_E_ that had a vaguely non-remote like shape',\n",
       " \"PM: um because you didn't really need to use it as a remote\",\n",
       " 'PM: since you could just use your voice .',\n",
       " 'PM: That would include some <disfmarker> mostly just the simple design features for a television operation',\n",
       " 'PM: but with a slide or a fold-out bay for more advanced functions for users .',\n",
       " 'PM: Um , and uh the uh uh',\n",
       " 'PM: the U_I_D_ and the I_D_ were asked to go ahead and start developing a prototype for us to look at .',\n",
       " 'PM: <vocalsound> So .',\n",
       " \"PM: That's sorted ,\",\n",
       " 'PM: back to the main <vocalsound> meet here ,',\n",
       " 'PM: um , go ahead and take it away guys .',\n",
       " 'ID: Well . Uh ,',\n",
       " 'ID: we have assembled our prototype , um .',\n",
       " \"ID: What's to be said about it ?\",\n",
       " 'ID: Um , we took into account a lot of the things',\n",
       " 'ID: that we went over in the last meeting , um .',\n",
       " 'ID: Some of the most important things to consider are that we decided not to go for the touch screen',\n",
       " 'ID: which you can see',\n",
       " 'ID: and opted for some very large buttons for the primary functions , um .',\n",
       " 'ID: This is going to be the on off button',\n",
       " 'ID: and we have these buttons to go through the channels',\n",
       " 'ID: um and then two volume buttons down here ,',\n",
       " 'ID: d uh we decided those were the most important uh buttons .',\n",
       " 'ID: And then , for the more advanced uh functions there is a slide out panel here',\n",
       " 'ID: um and you can see that there are lots of other things going on .',\n",
       " 'ID: But this actually can slide back in',\n",
       " \"ID: and provides a very nice aesthetic when it's all put away , um .\",\n",
       " 'ID: As far as the uh whole visible light thing , we decided to go with the multiple colours coming out ,',\n",
       " 'PM: <vocalsound> Nice . <vocalsound>',\n",
       " 'ID: why not ?',\n",
       " 'ME: <vocalsound> Fair enough .',\n",
       " \"ID: Of course , if that's annoying for some people that function can be turned off . Um .\",\n",
       " 'PM: <vocalsound> Perfect .',\n",
       " 'UI: No',\n",
       " 'ID: Go ahead .',\n",
       " \"UI: it's important to <vocalsound>\",\n",
       " 'UI: we talked a quite a bit about uh you know the the interchangeable uh faces',\n",
       " \"UI: and what we've done here is come up with a bit of a natural look here\",\n",
       " 'UI: um f we call it fruity',\n",
       " 'ME: Right .',\n",
       " 'UI: if you will .',\n",
       " 'ME: Appropriate , okay .',\n",
       " 'UI: Um . Right , um ,',\n",
       " \"UI: of course that's uh interchangeable\",\n",
       " 'UI: and uh I think it would be desirable for the uh for the regular product in the in the in the in the first packaging to be something a little bit more subdued',\n",
       " \"ME: Mm 'kay .\",\n",
       " 'UI: but this is kind of something that can be done',\n",
       " 'ME: It is an option .',\n",
       " 'UI: um and as you can see on the television there',\n",
       " 'UI: uh we have the uh voice detector device um on the top there .',\n",
       " 'PM: <vocalsound> Oh , right .',\n",
       " \"ID: That's this here .\",\n",
       " 'ME: Ah . I see .',\n",
       " 'UI: Um . So that that will work quite well with with regard to finding this uh contraption .',\n",
       " 'UI: Um , what other things do we see here , well , um',\n",
       " 'UI: if you give it a touch it does have actually a bit of a spongy feel ,',\n",
       " 'UI: um , so',\n",
       " 'UI: I think that will work well with regards to our market .',\n",
       " 'UI: Um and uh',\n",
       " \"UI: let's see ,\",\n",
       " 'UI: well',\n",
       " \"UI: clearly there's gonna be some more colours and what not available .\",\n",
       " 'UI: <vocalsound> Um uh',\n",
       " 'UI: do you have anything else to add to that ?',\n",
       " 'ID: Um I worried about the materials ,',\n",
       " 'ID: it is',\n",
       " 'ID: uh <disfmarker> the entire thing is covered in a rubber coating',\n",
       " \"ID: so it's very durable uh ,\",\n",
       " \"ID: it's not gonna break like some types of plastic that's dropped .\",\n",
       " 'ID: Um , and of course as you can see',\n",
       " 'ID: and if you touch it it does have that nice squishy feel .',\n",
       " \"UI: It's actually important to note that the television , uh you know if there's an earthquake or anything like that , that i it actually is edible inside .\",\n",
       " 'ID: Fact ,',\n",
       " 'ID: I dunno if you noticed ,',\n",
       " \"ID: but I wrote the uh the company's name on the telephone screen ,\",\n",
       " 'PM: Oh',\n",
       " 'PM: well done yeah , yeah oh ok',\n",
       " 'ME: Nice .',\n",
       " 'ID: I thought that was kinda nice .',\n",
       " 'ID: This was actually an apple on the inside .',\n",
       " 'ME: Do we need to worry about um rot factors ? <vocalsound>',\n",
       " 'ID: This <disfmarker>',\n",
       " \"UI: Um <vocalsound> it's encased in a new uh type of uh\",\n",
       " 'ME: Oh okay ,',\n",
       " \"ME: there's preservatives involved ,\",\n",
       " 'UI: polymer',\n",
       " 'ID: Yeah .',\n",
       " \"ME: we don't need to worry ,\",\n",
       " 'UI: yeah .',\n",
       " \"UI: It's fine .\",\n",
       " 'ID: We got a bit ahead of ourselves ,',\n",
       " 'ME: okay .',\n",
       " \"ID: I know we're not talking about making televisions at this point or anything like that , but <disfmarker>\",\n",
       " 'ME: Fair enough .',\n",
       " \"PM: <vocalsound> Edible televisions , it's a wave of the future . <vocalsound>\",\n",
       " 'ME: No but <disfmarker>',\n",
       " \"ME: <vocalsound> It's a couple years off at least .\",\n",
       " \"UI: It's pos a possible new product .\",\n",
       " 'ME: Okay .',\n",
       " \"UI: Um , but I think that's <disfmarker> I think that sums up the main features of our <disfmarker> of the remote ,\",\n",
       " 'UI: um I dunno if you guys have any questions',\n",
       " 'PM: Brilliant .',\n",
       " 'ID: Right .',\n",
       " 'UI: or f whether that uh <disfmarker> whether we need to worry about any uh other marketing areas or anything of that nature .',\n",
       " 'UI: Um , did we come in under budget ?',\n",
       " 'ID: Uh we did , yeah .',\n",
       " 'ID: This cost <disfmarker>',\n",
       " 'ID: well',\n",
       " \"ID: to put this into um production , we're looking at about <disfmarker>\",\n",
       " 'ID: what was our goal ?',\n",
       " 'ID: It was twelve fifty Euro',\n",
       " 'ID: um and this actually came in at about eleven ninety nine .',\n",
       " 'ID: Um , so I was quite pleased with that .',\n",
       " \"ID: One thing that we didn't do\",\n",
       " 'ID: um <disfmarker> obviously we had a choice with the buttons',\n",
       " 'ID: whether to use scroll buttons or standard rubber buttons ,',\n",
       " 'ID: but we just went for a classic rubber button',\n",
       " 'ID: and um since we did that',\n",
       " \"ID: we didn't have to use as many microchips\",\n",
       " 'ID: which was quite nice',\n",
       " \"ID: and that's what helped keep the cost down .\",\n",
       " 'PM: Brilliant .',\n",
       " 'ID: So even though it has a lot of modern technology , um for example the voice recognition ,',\n",
       " \"ID: in a lot of ways it's just a simple remote\",\n",
       " 'ME: Okay .',\n",
       " 'ID: and um I think if we shopped around for other manufacturers um we might be able to get even cheaper .',\n",
       " 'UI: <vocalsound> Did we talk about the voice recognition uh option ?',\n",
       " 'ID: And <disfmarker>',\n",
       " \"ID: Oh no , we haven't talked about that yet have we ?\",\n",
       " 'UI: So uh so uh yeah',\n",
       " 'UI: on the back here you all noticed this area here which is actually the voice recognition uh uh console',\n",
       " 'ME: Okay .',\n",
       " \"UI: and uh I think it's nicely designed into the into the overall look .\",\n",
       " 'ME: Yeah .',\n",
       " 'UI: Um , but basically the voice recognition uh incorporates um the latest designs that our research team has been able to cufw uh come up with .',\n",
       " 'UI: Basically uh quite similar to the coffee maker um design that we were talking about earlier',\n",
       " \"ME: Mm 'kay .\",\n",
       " 'UI: and um , I think that uh has given a proven um <vocalsound> ease of use and what not .',\n",
       " 'ID: Hmm . Yeah .',\n",
       " 'UI: And uh allows features like the remote actually talking back to the user um , so .',\n",
       " 'ME: Right .',\n",
       " 'PM: Cool .',\n",
       " 'ID: Any questions ?',\n",
       " 'ME: <vocalsound> Do we have um other , for lack of a better word , skins ? Covers ?',\n",
       " 'PM: No , no',\n",
       " \"PM: I think that's <disfmarker>\",\n",
       " 'ME: In play now',\n",
       " 'ME: or are those ones gonna be developed later once we see how the couple we have g go or ?',\n",
       " 'ID: Um , do you wanna answer this one',\n",
       " 'ME: Do we know where we stand on that yet ?',\n",
       " 'ID: or do you want me to answer it ?',\n",
       " 'UI: Well',\n",
       " \"UI: we didn't quite have enough material uh <vocalsound> .\",\n",
       " 'ME: Oh',\n",
       " \"ME: I wasn't expecting a prototype\",\n",
       " 'ID: Yeah , yeah .',\n",
       " \"ME: I just didn't know <disfmarker> if you guys had any in mind yet . <vocalsound>\",\n",
       " 'UI: Oh I see ,',\n",
       " 'UI: right , um .',\n",
       " 'ID: Um , well <vocalsound>',\n",
       " 'ID: as you can see this is just a most superficial layer',\n",
       " \"ID: and um it'd be very easy to put another layer of something else like <disfmarker>\",\n",
       " 'ME: Okay .',\n",
       " 'ME: Just veneer really ,',\n",
       " 'ME: yeah . Okay .',\n",
       " 'UI: Right .',\n",
       " 'UI: Actually this bottom red ring here just unclips',\n",
       " 'UI: and then you put a a new a new uh a new plate on top of that .',\n",
       " 'ME: And the whole thing <disfmarker>',\n",
       " 'ME: Okay',\n",
       " 'ME: Right',\n",
       " 'UI: So',\n",
       " 'UI: I mean there are <disfmarker> I <disfmarker>',\n",
       " 'UI: we definitely priced out a spongy <disfmarker> even spongier non-natural look um materials',\n",
       " 'ME: Yeah .',\n",
       " \"ME: There's <disfmarker>\",\n",
       " 'ME: Okay .',\n",
       " 'UI: which I think worked out fine .',\n",
       " \"UI: We also continued on with the ideas that f following uh Apple's colour schemes with the kind of the uh light orange and the green .\",\n",
       " \"ME: Mm 'kay .\",\n",
       " 'ME: Okay , very cool .',\n",
       " \"ID: It's not it's not quite a a face plate ,\",\n",
       " \"ID: it's more like a pseudo-face plate\",\n",
       " 'ME: Okay .',\n",
       " \"ID: because it's simple enough\",\n",
       " 'ID: that in the factory it could <disfmarker> we could very easily put a different one on it ,',\n",
       " \"ID: it locks into place such that , you know , it's pretty permanent\",\n",
       " \"ID: but at the same time , if we wanna go the other way it's just a matter of a couple of adjustments\",\n",
       " 'ID: and we could go the face plate way',\n",
       " 'ME: Okay .',\n",
       " 'ID: if you know what I mean .',\n",
       " 'ME: Yep .',\n",
       " \"ME: It's still an option if we need it .\",\n",
       " 'ID: Yeah .',\n",
       " 'ME: Very cool , nice job .',\n",
       " 'PM: Right , yeah',\n",
       " 'PM: thanks guys',\n",
       " \"PM: that's very , very good work .\",\n",
       " 'PM: I like it ,',\n",
       " 'PM: brilliant .',\n",
       " 'PM: <vocalsound> Um , <vocalsound> what we need to discuss now is the finance of it , um',\n",
       " \"PM: I got me <disfmarker> you've got <disfmarker>\",\n",
       " 'PM: you provided a number',\n",
       " 'PM: that actually sounds quite nice .',\n",
       " 'PM: Um the trouble is I was just given this by finance .',\n",
       " 'ID: Oh .',\n",
       " \"PM: Um , it's a spreadsheet of the parts <vocalsound>\",\n",
       " \"PM: and I've just tentatively put in what it's going to look like um .\",\n",
       " 'ID: Ooh .',\n",
       " \"PM: I'm just gonna clear this out real quickly ,\",\n",
       " 'PM: but it looks like <disfmarker>',\n",
       " 'PM: So',\n",
       " \"PM: we'll just <disfmarker>\",\n",
       " \"PM: if we can just itemize what's in here ,\",\n",
       " \"PM: we've got this <disfmarker>\",\n",
       " \"PM: it's a solar cell thing\",\n",
       " 'PM: right ?',\n",
       " 'ID: Right',\n",
       " \"ID: uh we didn't really touch on that\",\n",
       " 'PM: With a back-up battery ?',\n",
       " \"ID: but it it's in there ,\",\n",
       " 'ID: yep .',\n",
       " 'PM: With the ba okay . Um',\n",
       " 'PM: and <disfmarker>',\n",
       " 'UI: The voice recognition area actually doubles as uh as the solar cell area . Yeah .',\n",
       " 'PM: Clever , clever ,',\n",
       " 'PM: well done .',\n",
       " \"PM: Um so I guess that would mean we've got a bit of a um <disfmarker>\",\n",
       " \"PM: <vocalsound> It's a s a speaker and a sensor at the same time\",\n",
       " \"PM: isn't it ?\",\n",
       " 'ID: Yeah , yeah .',\n",
       " \"ID: It's just making use of the same space and the same materials ,\",\n",
       " 'PM: Okay .',\n",
       " \"PM: Um and the case , it's more of a single-curved case ,\",\n",
       " 'PM: I guess would be that <disfmarker> be the general <disfmarker>',\n",
       " 'ID: Yeah ,',\n",
       " 'ID: one big curve I guess you could say .',\n",
       " \"PM: <vocalsound> Um and we've got a rubber skin material basically throughout .\",\n",
       " 'PM: Um . Push button interface um with this other drop-down',\n",
       " \"PM: so maybe we've got two push button interfaces\",\n",
       " \"PM: don't we ?\",\n",
       " 'ID: Yeah , mm-hmm .',\n",
       " 'PM: And um a special <disfmarker>',\n",
       " \"PM: I guess it's uh <disfmarker> we've got a sort of a wood materi a rubbery type material that <disfmarker> throughout ,\",\n",
       " 'ID: Yeah ,',\n",
       " 'ID: special .',\n",
       " 'PM: yeah .',\n",
       " 'ID: And s I guess you have to mark special colour and special form as well ,',\n",
       " \"ID: don't you ?\",\n",
       " \"ID: 'Cause it i it is very unconventional ,\",\n",
       " 'PM: Yeah ,',\n",
       " \"PM: it's it's quite unique .\",\n",
       " 'ID: I like to think of it as unconventional .',\n",
       " 'PM: I like it , yeah',\n",
       " \"PM: it's <disfmarker>\",\n",
       " 'PM: So',\n",
       " 'PM: it looks like',\n",
       " 'ME: M come in at sixteen ?',\n",
       " 'PM: a bit over budget , um .',\n",
       " 'ID: Oh . Huh ,',\n",
       " \"ID: doesn't match up\",\n",
       " 'ID: does it ?',\n",
       " 'PM: So',\n",
       " 'PM: what we could do perhaps , a simple fix would maybe to switch away from the solar cells',\n",
       " 'PM: <vocalsound> um or take out the back-up battery . Uh <disfmarker>',\n",
       " 'ID: How do you feel about that ?',\n",
       " 'UI: I mean',\n",
       " \"UI: I think that uh if we're talking about it being one of our main selling features , being environmental and without the batteries and what not ,\",\n",
       " 'UI: although it does still have a battery',\n",
       " \"UI: so I'm not sure that <disfmarker> you know what the sell is on that .\",\n",
       " 'ME: Yeah .',\n",
       " 'ID: I mean we could take we could take the battery out of it you see',\n",
       " \"ID: and it'd probably work ninety nine per cent of the time\",\n",
       " \"ID: but you're gonna have to set up a call centre for that one per cent of the time\",\n",
       " 'ID: when people are calling',\n",
       " \"ID: and saying oh look my remote isn't working\",\n",
       " 'ID: what am I gonna do ?',\n",
       " 'PM: Mm-hmm . Mm-hmm .',\n",
       " 'ME: Mm k .',\n",
       " \"ID: People'd be real upset .\",\n",
       " \"ID: I think in the long-run it's better to keep the battery ,\",\n",
       " 'ME: True .',\n",
       " \"ID: it's hard to scrap the whole cell battery idea\",\n",
       " \"ID: 'cause that's so integral to the theme that we have .\",\n",
       " \"PM: <vocalsound> What's difficult , we have all these things integral to the um to the design of it that we just can't back out of now ,\",\n",
       " 'ME: Nah .',\n",
       " 'PM: it would have to be <disfmarker>',\n",
       " \"PM: seems like we'd have to go back to\",\n",
       " 'PM: square one in a way .',\n",
       " \"PM: Um if we were gonna try to undo one bit we'd probably have to undo most of it , um <disfmarker>\",\n",
       " 'ID: Yeah .',\n",
       " \"ME: <vocalsound> Although we don't wanna get rid of the whole environmental <disfmarker>\",\n",
       " \"ME: I mean obviously the solar cell is a big piece of the way we're marketing this as like a natural , new thing ,\",\n",
       " \"ME: but honestly if we cut that one piece out we're actually coming in under budget\",\n",
       " \"ME: if I've done my math correctly .\",\n",
       " 'UI: I mean you might be able to sway me on the idea that <disfmarker> we <disfmarker> our main selling point could be already this voice recognition thing',\n",
       " 'ME: I mean <disfmarker>',\n",
       " \"UI: I mean that's what sets us apart\",\n",
       " \"ME: Which , it's <disfmarker>\",\n",
       " 'UI: right ?',\n",
       " 'ME: yeah',\n",
       " \"ME: that's what setting us into this young market ,\",\n",
       " \"ME: I mean that's where we started from , so <disfmarker>\",\n",
       " \"ME: <vocalsound> I don't know ,\",\n",
       " 'ME: and I mean you know',\n",
       " 'ME: perhaps when the cell technology comes down in price we can bring that back into the game',\n",
       " 'ME: but it looks like at this point that may be out of our league .',\n",
       " \"UI: And the reality is you know , for me from an ideological stand point , I'd like to stick with the uh the solar cell ,\",\n",
       " 'ID: Right .',\n",
       " 'UI: but I h kind of have to throw myself in the in the business structure model here',\n",
       " 'ME: Right .',\n",
       " 'ME: Yeah .',\n",
       " 'UI: and uh you know I think I think that I think that we need to come to a compromise here',\n",
       " 'ID: Right .',\n",
       " \"PM: It's either or .\",\n",
       " 'ME: Yeah .',\n",
       " 'UI: and maybe move ahead with the project , without the solar cell .',\n",
       " 'ID: Yeah . I guess we might have to do that .',\n",
       " \"ME: I think unfortunately that's our best option .\",\n",
       " \"ID: It's the only way we're gonna get below our uh goal\",\n",
       " \"ID: isn't it ?\",\n",
       " 'ID: Of twelve fifty .',\n",
       " \"PM: 'Cause we can't remove the push buttons\",\n",
       " \"PM: 'cause they're <vocalsound>\",\n",
       " 'ME: <vocalsound> It <disfmarker> kind of <disfmarker>',\n",
       " 'ID: Yeah , <vocalsound> .',\n",
       " \"PM: um and we can't get rid of the uh <disfmarker> I mean removing the <disfmarker>\",\n",
       " 'UI: <vocalsound> Savings .',\n",
       " 'ME: yeah .',\n",
       " \"PM: changing the case wouldn't be so much of a\",\n",
       " 'PM: <vocalsound> mm-mm ,',\n",
       " 'PM: um , nor would changing the case materials .',\n",
       " 'ME: Mm-mm .',\n",
       " 'PM: Um . So yeah',\n",
       " 'PM: that looks like to be the only thing .',\n",
       " 'ME: Yeah .',\n",
       " 'PM: So that would be the <disfmarker>',\n",
       " \"PM: it's a major change but <disfmarker>\",\n",
       " 'PM: Yeah .',\n",
       " 'ID: Gotta do what you gotta do .',\n",
       " 'PM: Alright , so',\n",
       " \"PM: we're in agreement on that .\",\n",
       " 'ME: Unfortunately I think we are .',\n",
       " 'UI: No ,',\n",
       " 'UI: I think that was a good compromise you brought forward Sarah .',\n",
       " 'PM: <vocalsound> Right .',\n",
       " 'PM: Moving along swiftly .',\n",
       " 'PM: Um , so',\n",
       " 'PM: I guess now we just go to the project evaluation',\n",
       " 'PM: which I will allow Sarah to take over .',\n",
       " 'ME: That would be me .',\n",
       " 'ME: Um cord ?',\n",
       " 'PM: Ah',\n",
       " 'PM: of course ,',\n",
       " 'PM: sorry .',\n",
       " 'ME: No problem .',\n",
       " 'PM: Whoosh .',\n",
       " 'ME: Can you reach ,',\n",
       " 'ME: that would be great ,',\n",
       " 'UI: Yep .',\n",
       " 'ME: thank you .',\n",
       " \"PM: <vocalsound> That'd be great <vocalsound>\",\n",
       " \"ME: <vocalsound> I didn't even do that one on purpose either ,\",\n",
       " 'ME: damn .',\n",
       " 'ME: Okay , um ,',\n",
       " \"ME: basically I was just evaluating um <vocalsound> from what we know of how our product's working right now with the criteria that we set at the beginning of <disfmarker>\",\n",
       " 'ME: these are the things we needed to do ,',\n",
       " \"ME: these are the things that look like we feel they're important .\",\n",
       " 'ME: Um so I was looking at basic design things ,',\n",
       " 'ME: does it fulfil its functions as a remote ?',\n",
       " 'ME: Is the design what we wanted it to do ?',\n",
       " 'ME: I',\n",
       " 'ME: are technologies up to where we hoped they would be',\n",
       " 'ME: and does it fulfil the aesthetic qualities that our original market research was looking for ? Um .',\n",
       " 'ME: Basic questions like , you know , does it turn on ?',\n",
       " 'ME: Does it respond to voice recognition ?',\n",
       " \"ME: And overall , in general , it looks like it's coming up to par .\",\n",
       " 'ME: Um , the only thing is with with the pull-out panel , that is , can it take some adjusting',\n",
       " \"ME: because it's a new sort of interface ,\",\n",
       " 'ME: um that looked like it was coming up rough ,',\n",
       " 'ME: but then , once you get used to it , it does make a lot of sense .',\n",
       " 'ID: <vocalsound> Really good .',\n",
       " \"ME: So I think overall we're headed in the right direction .\",\n",
       " 'ME: <vocalsound> So .',\n",
       " 'UI: They like that spongy feel .',\n",
       " 'ME: Yeah .',\n",
       " \"ME: It looks like it's going over well , so\",\n",
       " 'UI: And the paging function works well ,',\n",
       " 'ID: Six ?',\n",
       " \"UI: that's good to hear ,\",\n",
       " \"ME: we're we're good yeah .\",\n",
       " 'UI: we worked hard on that one .',\n",
       " 'ID: We did .',\n",
       " 'ME: Yeah .',\n",
       " 'PM: <vocalsound> Brilliant .',\n",
       " \"ME: It's <disfmarker>\",\n",
       " 'ME: I think eventually if we do um branch out with this product maybe we do have a higher budget options',\n",
       " 'ME: and if it goes over with this model we can look into um wider range voice recognition like from other rooms of the house and stuff ,',\n",
       " \"ME: but for now , what we've got is working in the range we need it for ,\",\n",
       " 'ME: so',\n",
       " \"ME: it's all good .\",\n",
       " 'ID: I am bit disappointed about losing the solar panel',\n",
       " \"ME: That's everything from me .\",\n",
       " \"ID: but it's okay .\",\n",
       " 'ME: Yeah .',\n",
       " 'ME: Yeah ,',\n",
       " 'ME: it is a set-back ,',\n",
       " 'ME: but <disfmarker>',\n",
       " 'ME: Okay ,',\n",
       " 'ME: do you need the cord back ?',\n",
       " 'UI: W we might have uh we might have lost that granola market again',\n",
       " 'PM: Um yeah ,',\n",
       " 'PM: I was just <disfmarker>',\n",
       " 'PM: go on .',\n",
       " \"UI: that we're <disfmarker>\",\n",
       " 'ID: I know .',\n",
       " 'PM: Well',\n",
       " \"PM: they don't own tellys anyway\",\n",
       " 'PM: do they ?',\n",
       " \"UI: I guess that's true .\",\n",
       " 'ID: True .',\n",
       " 'PM: Right . So , um , <vocalsound>',\n",
       " \"PM: this one's a bit unclear to me to be perfectly fair , um .\",\n",
       " 'PM: I got this slide from the coach',\n",
       " \"PM: and I'm not sure what it's connected to .\",\n",
       " 'PM: Um so I guess we are going to discuss um <vocalsound> our project process um',\n",
       " 'PM: and that is gonna go into my report .',\n",
       " 'PM: So',\n",
       " 'PM: I guess this is the point where we go um <vocalsound> uh out of role it looks like',\n",
       " 'PM: and talk about our satisfaction for room for creativity and so forth',\n",
       " 'PM: and how that all worked , I guess , um .',\n",
       " 'ID: Okay .',\n",
       " 'ME: As in within the team or ?',\n",
       " 'PM: I think so yeah .',\n",
       " 'ID: Right so',\n",
       " \"ID: it's just kind of a open mic kind of thing or <disfmarker>\",\n",
       " 'ME: Okay .',\n",
       " \"PM: I think it's <disfmarker> I\",\n",
       " 'PM: mm-hmm , I think so .',\n",
       " \"ID: 'Kay .\",\n",
       " \"PM: I think <vocalsound> hope I'm not screwing up an experiment <vocalsound> .\",\n",
       " 'ME: It is now ,',\n",
       " \"ME: you're in charge\",\n",
       " 'ME: so',\n",
       " 'ME: there you go <vocalsound> .',\n",
       " 'PM: But I trust that she would jump in if I was',\n",
       " 'PM: so okay',\n",
       " 'PM: fair enough .',\n",
       " 'ID: Yeah .',\n",
       " 'ME: Whatever <vocalsound> .',\n",
       " 'PM: Um right , um so',\n",
       " 'PM: any thoughts ?',\n",
       " 'ID: Are we considering these points here ?',\n",
       " 'PM: Yeah .',\n",
       " 'ID: Okay .',\n",
       " \"ME: I think they're starting blocks\",\n",
       " 'ME: yeah .',\n",
       " 'PM: What do you guys feel about the process ?',\n",
       " 'ME: Um , you know',\n",
       " \"ME: I think in general , for a day's worth of work we actually were <vocalsound> relatively productive ,\",\n",
       " 'PM: <vocalsound> Mm-hmm .',\n",
       " 'ME: considering the little amount of input we had going in .',\n",
       " 'ME: Um , and the technology has definitely been a help ,',\n",
       " \"ME: it's really been interesting to try out all this new stuff .\",\n",
       " \"UI: We didn't use the whiteboard at all .\",\n",
       " \"ME: <vocalsound> No , we didn't .\",\n",
       " 'ID: No .',\n",
       " 'PM: No , no whiteboard . <vocalsound>',\n",
       " \"ME: We could now if that'd make up for it\",\n",
       " 'UI: <vocalsound> And <disfmarker>',\n",
       " 'ME: but really <disfmarker>',\n",
       " 'ME: and I feel like if you guys had been designing in here perhaps that would have changed',\n",
       " \"ME: but because of room constraints , doesn't really matter .\",\n",
       " \"UI: Um , also had I not been intrigued about the pen , I don't think I woulda used it at all ,\",\n",
       " \"UI: I didn't write barely anything .\",\n",
       " 'ME: Yeah',\n",
       " 'ME: I think I was taking <vocalsound> notes more often than usual',\n",
       " \"ME: just 'cause I liked the pen , yeah .\",\n",
       " \"ID: Yeah , it's true .\",\n",
       " 'PM: Was pretty cool tack though .',\n",
       " 'UI: Yeah .',\n",
       " 'ID: Definitely .',\n",
       " \"ME: I am disappointed I didn't get a note back from my personal coach . <vocalsound>\",\n",
       " 'ID: As you write your personal coach .',\n",
       " 'ME: Yeah ,',\n",
       " \"ME: but I didn't get a response <vocalsound>\",\n",
       " 'ID: <vocalsound> What if you get a response two or three months from now ?',\n",
       " 'ME: so <vocalsound>',\n",
       " \"ME: we'll see <vocalsound> .\",\n",
       " 'ME: Okay',\n",
       " \"ID: That'd be weird .\",\n",
       " 'ME: that would be kinda creepy .',\n",
       " 'PM: Attempts to contact coach ineffective <vocalsound> .',\n",
       " 'ME: <vocalsound> Well',\n",
       " 'ME: what kind of coaching is that really ?',\n",
       " 'ME: What if I really needed something . <vocalsound>',\n",
       " 'UI: <vocalsound> I <disfmarker> so',\n",
       " \"UI: I don't\",\n",
       " 'UI: n I think there was a lot of room for creativity ,',\n",
       " 'UI: we could do whatever <disfmarker> basically what we wanted until the budget came down on us , um .',\n",
       " 'ME: I think so .',\n",
       " 'ME: And even then we did get a decent product turned out',\n",
       " \"ME: although it's not everything we wanted it to be .\",\n",
       " 'ID: Yeah .',\n",
       " 'UI: With the natural look .',\n",
       " \"ID: That's very natural .\",\n",
       " 'PM: <vocalsound> Very natural look .',\n",
       " 'ME: <vocalsound> Organic , really . <vocalsound>',\n",
       " \"PM: <vocalsound> That's the brilliance of <disfmarker> they had a p they had a peeler in here .\",\n",
       " 'ME: And highly resourceful team mates might I add',\n",
       " 'ID: Yeah .',\n",
       " 'ME: which is always a plus .',\n",
       " 'PM: Yeah ,',\n",
       " 'PM: I think , yeah re I thought it was like really creative actually , I mean .',\n",
       " 'UI: <vocalsound> I think the teamwork was good as well .',\n",
       " 'ME: Mm yeah ,',\n",
       " \"ME: I'm impressed .\",\n",
       " \"ID: And to prove that we weren't wasteful we didn't waste a single bit of Play-Doh ,\",\n",
       " 'ID: we used every bit .',\n",
       " 'ME: <vocalsound> Nice .',\n",
       " 'ME: All four of those little containers . <vocalsound>',\n",
       " 'PM: <vocalsound> Including the s the multi-coloured wave pattern .',\n",
       " 'ID: Yeah ,',\n",
       " 'ID: I guess <disfmarker>',\n",
       " \"ID: My one my one criticism is that we didn't have enough colours to work with ,\",\n",
       " 'ME: <vocalsound> Yeah .',\n",
       " 'ID: we only had four ,',\n",
       " \"ID: wasn't enough .\",\n",
       " 'ME: You could have developed multiple skins really had you had more colours .',\n",
       " 'ID: I know',\n",
       " 'ID: it could have been amazing .',\n",
       " 'ME: Oh well .',\n",
       " 'PM: What did you guys think about the the the roles ?',\n",
       " 'ME: They were good <vocalsound> .',\n",
       " 'ID: Yeah',\n",
       " \"ID: it's f kind of fun ,\",\n",
       " 'ME: Yeah .',\n",
       " 'ID: it was <disfmarker>',\n",
       " 'ID: I think it was pretty clever',\n",
       " \"ID: 'cause we were never able to get too far off track\",\n",
       " 'ID: because the information came in at the right time',\n",
       " 'ID: and kind of filled in the gaps enough .',\n",
       " 'ME: True .',\n",
       " 'ID: At the same time you had enough room to kind of just make things up ,',\n",
       " 'ME: Do your own .',\n",
       " 'ID: which was kind of fun .',\n",
       " 'ME: Though I did feel like th the level of information dropped off severely over the course of the day .',\n",
       " 'ID: Yeah .',\n",
       " \"ME: I mean maybe it's just me\",\n",
       " \"ME: but I didn't actually get any information for the last presentation at all .\",\n",
       " \"PM: That's true ,\",\n",
       " \"ME: Nothing , I didn't even get an email ,\",\n",
       " 'PM: I I got this spreadsheet <vocalsound> .',\n",
       " 'ME: like',\n",
       " 'ME: that was it .',\n",
       " 'ME: So , yeah ,',\n",
       " 'ME: I feel like that was slightly lacking',\n",
       " 'ME: but then you know , fill in the blanks on your own , level of creativity upped .',\n",
       " 'UI: Well',\n",
       " 'ME: Whatever .',\n",
       " 'UI: I think that was I think that was an issue I kept finding with regard to <disfmarker>',\n",
       " 'PM: Of what to do .',\n",
       " 'UI: well',\n",
       " 'UI: no',\n",
       " 'UI: but also <disfmarker>',\n",
       " 'UI: yeah',\n",
       " 'UI: when I was reporting about what each of us was doing I was often confused as to what you were doing <vocalsound>',\n",
       " 'ME: Uh-huh ,',\n",
       " \"ME: that wasn't very much . <vocalsound>\",\n",
       " 'ID: You know <disfmarker>',\n",
       " 'UI: um and then I also felt like you know a lot of our discussion would centre around n specifically what my task was',\n",
       " 'UI: because that was kind of the interface portion',\n",
       " 'ID: Yeah .',\n",
       " 'UI: which was what the whole project was about',\n",
       " 'ME: Yes .',\n",
       " 'PM: Mm , mm .',\n",
       " 'ME: Hmm ,',\n",
       " 'ME: very much so .',\n",
       " 'UI: but <disfmarker>',\n",
       " 'UI: and then in the end I think our jobs kind of melded together a little bit more ,',\n",
       " 'ID: Yeah .',\n",
       " 'ID: That was fun .',\n",
       " 'UI: which was fine .',\n",
       " 'ID: I think the most helpful thing out of everything was getting the the PowerPoint slides already put together for you',\n",
       " 'ME: Yeah , already having the formatted stuff helped a lot .',\n",
       " \"ID: 'cause if we didn't have that there's no way we could have got all that done in time .\",\n",
       " 'ME: Very much so .',\n",
       " 'PM: Cool .',\n",
       " 'UI: And I think your leadership was quite good .',\n",
       " 'ID: It was really good yeah .',\n",
       " 'PM: She said I I I',\n",
       " 'PM: she actually made a comment off <gap>',\n",
       " \"PM: boy you're getting into this\",\n",
       " \"PM: and I really I think it's true\",\n",
       " 'PM: I did get <disfmarker> I I felt like I got way too into it .',\n",
       " 'ME: Yeah .',\n",
       " \"ME: That's kind of a good thing though ,\",\n",
       " 'PM: I felt like I slipped into it a lot .',\n",
       " \"ID: It's kinda fun .\",\n",
       " 'ME: you know ,',\n",
       " 'ME: give the rest of us some structure to work with so hey .',\n",
       " 'PM: I dunno .',\n",
       " 'UI: An',\n",
       " 'UI: so',\n",
       " \"UI: is that the first time you've taken on that kind of role ?\",\n",
       " \"PM: The first time I've ever done anything like yeah project project management .\",\n",
       " 'PM: I usually organise crap ,',\n",
       " \"PM: it's one thing to do , you know <disfmarker> set up a party with your friends ,\",\n",
       " 'ID: Yeah .',\n",
       " 'PM: you know ?',\n",
       " 'ME: Yeah .',\n",
       " 'ID: Little different .',\n",
       " 'PM: But you guys felt that you could keep the , yeah , suspension of disbelief kind of like like the role and the <disfmarker> okay ?',\n",
       " 'ME: Yeah .',\n",
       " 'ID: Yeah .',\n",
       " 'ME: I',\n",
       " 'ME: except for a couple moments where it just got out of hand',\n",
       " 'ME: and I knew we were all lying through our teeth ,',\n",
       " 'ID: Yeah .',\n",
       " 'ME: other than that <vocalsound> <disfmarker>',\n",
       " 'UI: <vocalsound> I had to admit , as soon as w we started <disfmarker> I mean as soon as we got the Play-Doh , <vocalsound> th <vocalsound> you know the whole concept of really trying to stick with reality went out the window <vocalsound> .',\n",
       " 'ME: <vocalsound> I could only imagine .',\n",
       " 'PM: Yeah , yeah .',\n",
       " 'PM: Maybe in in Legos you know ? Be fun with Legos too ,',\n",
       " 'UI: Possibly .',\n",
       " 'PM: like make a remote control or spaceship ,',\n",
       " 'PM: we used to have spaceship Legos <disfmarker>',\n",
       " 'PM: did you guys ever used to build spaceships with Legos <disfmarker>',\n",
       " 'UI: Oh yeah ,',\n",
       " 'ME: Yeah . Totally .',\n",
       " \"UI: still have 'em .\",\n",
       " 'PM: everybody knows <disfmarker> best spaceships ever .',\n",
       " 'PM: Um you guys felt like there was enough teamwork in all ?',\n",
       " 'ID: Yeah ?',\n",
       " 'ME: I think so .',\n",
       " 'UI: Yep .',\n",
       " \"ID: <vocalsound> You don't .\",\n",
       " 'PM: No I , no I dunno , I d I I dunno ,',\n",
       " \"PM: I don't <disfmarker> I I was just <disfmarker> I <disfmarker>\",\n",
       " \"ME: Though we didn't actually <disfmarker>\",\n",
       " \"ME: I mean other than minor discussion at meetings there wasn't <disfmarker> except for the actual building ,\",\n",
       " 'ID: Yeah .',\n",
       " 'ME: but I feel like if this was a team project there actually would have been much more of the collaborative like brainstorming , use the board <disfmarker>',\n",
       " \"PM: It's true huh ?\",\n",
       " 'ME: well',\n",
       " \"ME: and this would have been six months' worth of work , not like three hours' worth of meetings .\",\n",
       " 'PM: Yeah .',\n",
       " 'UI: I mean',\n",
       " 'UI: I think had the issue been more serious we probably woulda brainstormed more during our meetings as a team .',\n",
       " 'ID: Yeah .',\n",
       " \"ME: That's true .\",\n",
       " 'PM: Yeah .',\n",
       " \"PM: Course I'm I'm conscious of the idea of the Project Manager asking if you guys feel like there's a team <vocalsound> you know it's like , kind of like , like hmm .\",\n",
       " 'PM: It d',\n",
       " 'ME: Yeah that is kind of <disfmarker>',\n",
       " 'PM: But yeah .',\n",
       " \"PM: Interesting . It's kind of fascinating\",\n",
       " \"PM: wasn't it ?\",\n",
       " 'PM: I mean the whole process of <disfmarker>',\n",
       " 'ID: Wonder why <disfmarker>',\n",
       " 'ID: is there anything about the way that we got so much inform what was it that kept us from going to the the board ?',\n",
       " \"PM: I don't know . <vocalsound>\",\n",
       " \"PM: I I don't know if there was a ri I th\",\n",
       " 'ME: Mine was the mics .',\n",
       " \"ME: I didn't feel like getting up and down and dealing with all these wires ,\",\n",
       " \"ID: Yeah , that's it\",\n",
       " 'ME: I was afraid I was gonna break something actually .',\n",
       " \"ID: 'cause the mics are loose\",\n",
       " \"ID: and each time you get up it's s a possibility of tripping over something or getting tangled or .\",\n",
       " 'ME: Yeah .',\n",
       " 'PM: Mm . <vocalsound>',\n",
       " 'ME: Yeah .',\n",
       " 'ME: Yeah .',\n",
       " 'UI: Well',\n",
       " 'UI: I dunno what I woulda shown on that board .',\n",
       " 'ME: True ,',\n",
       " \"ME: but it didn't even occur to me as an option ,\",\n",
       " \"ME: I mean I don't know that I would have\",\n",
       " 'PM: Nor I .',\n",
       " \"ME: but I know that I consciously didn't .\",\n",
       " 'UI: I mean',\n",
       " \"UI: it's just like the paper I don't know what I really needed the paper for .\",\n",
       " 'ME: True .',\n",
       " \"UI: Um , because I've got this laptop . Standard ,\",\n",
       " 'ME: Yeah .',\n",
       " 'UI: I just used it',\n",
       " \"UI: 'cause it's literally right in front of me .\",\n",
       " 'ME: Yeah .',\n",
       " 'PM: I wanna see the output files from these um , from the digital paper .',\n",
       " 'ME: Well',\n",
       " 'ME: it looks really professional .',\n",
       " 'PM: I wanna see wh wh what my my handwriting looks like digitized',\n",
       " 'ME: I know .',\n",
       " 'PM: because my handwriting is crap .',\n",
       " 'ID: Yeah ,',\n",
       " 'PM: I mean ,',\n",
       " \"ID: that's it .\",\n",
       " 'PM: just to see what it looks like in P_D_F_ format or something .',\n",
       " 'ID: Usually I would do a lot more doodling too',\n",
       " \"ID: but I didn't\",\n",
       " 'ID: because <disfmarker>',\n",
       " 'ME: I know ,',\n",
       " 'ME: I felt like I needed to be professional',\n",
       " \"ME: so I didn't like draw all over my paper and stuff .\",\n",
       " 'ME: <vocalsound> Okay , well',\n",
       " 'ME: not entirely ,',\n",
       " 'ME: but still , I doodled less than I usually do .',\n",
       " 'PM: T',\n",
       " \"PM: I I'm curious about what the de-briefing is gonna be like .\",\n",
       " 'PM: You know , like',\n",
       " 'PM: what is the uh',\n",
       " \"PM: what exactly we're looking for here .\",\n",
       " 'UI: So',\n",
       " 'UI: is this all we need to get through ?',\n",
       " 'PM: I dunno ,',\n",
       " \"PM: I'm not sure what the new ideas found i is about .\",\n",
       " 'ME: I guess .',\n",
       " 'ID: New ideas .',\n",
       " 'ME: It <disfmarker> did it just say in an email that we need to discuss that ?',\n",
       " 'UI: Is it <disfmarker>',\n",
       " 'PM: Well ,',\n",
       " \"PM: that's the thing I got <disfmarker> i in the email I got this PowerPoint file\",\n",
       " 'PM: but this slide was just there ,',\n",
       " 'ME: That slide was like that ?',\n",
       " 'PM: mm-hmm .',\n",
       " \"PM: I didn't change this one at all .\",\n",
       " 'ME: Well .',\n",
       " 'PM: Um ch',\n",
       " \"ID: I guess we're on the right track .\",\n",
       " 'PM: Yeah well . <vocalsound>',\n",
       " 'UI: Any new ideas with regard to remote control concepts ? <vocalsound>',\n",
       " 'PM: W <gap> <vocalsound> I kinda like th',\n",
       " 'ID: No , none .',\n",
       " 'ME: Uh I think they still do their job .',\n",
       " \"ID: I think they're fine actually .\",\n",
       " 'PM: Yeah',\n",
       " \"PM: you can't <disfmarker>\",\n",
       " 'ME: I am thinking outside the little square box though , with literally in like form',\n",
       " 'PM: Yeah .',\n",
       " \"ME: I don't <disfmarker>\",\n",
       " 'ID: Yeah ,',\n",
       " 'ID: maybe a s a circle would be alright , different .',\n",
       " 'ME: Yeah .',\n",
       " 'PM: Does kinda make you wonder ,',\n",
       " 'PM: I mean ,',\n",
       " 'PM: how much can you do with a remote control ?',\n",
       " \"PM: It's like inventing a new car .\",\n",
       " 'PM: Yeah yeah ,',\n",
       " \"ME: It's still gotta be technically car shaped\",\n",
       " 'PM: you can <disfmarker>',\n",
       " \"ME: or it won't fit on the road ,\",\n",
       " 'PM: <vocalsound> Yeah .',\n",
       " 'ME: you know ?',\n",
       " \"ME: <vocalsound> Don't know .\",\n",
       " \"PM: <vocalsound> Um . 'Kay .\",\n",
       " 'UI: What is that ? Our limited ability to think outside the box ?',\n",
       " 'ME: Kind of .',\n",
       " 'PM: So this was other costs .',\n",
       " 'ID: Are we back into project mood ?',\n",
       " 'PM: I dunno .',\n",
       " 'PM: I think this is',\n",
       " 'ME: Oh ,',\n",
       " 'ME: how long was our meeting supposed to be ?',\n",
       " 'ME: How much time do we have left ?',\n",
       " 'PM: forty ish',\n",
       " 'PM: I I I mm we should go on a bit <disfmarker> yeah <disfmarker> about the project eval , um .',\n",
       " 'PM: I dunno about you guys but I felt like a bit under-stimulated on the whole thing .',\n",
       " 'PM: Like ,',\n",
       " 'PM: what like you know what am I really doing ,',\n",
       " 'PM: you know',\n",
       " 'PM: what is <disfmarker>',\n",
       " 'ME: Yeah ,',\n",
       " 'ME: at the beginning it started out',\n",
       " 'ME: and I felt actually like under pressure',\n",
       " 'ME: like the first couple were taking a lot of work',\n",
       " 'PM: Yep .',\n",
       " 'ME: and I was like <disfmarker> had like all this brainstorming I was doing',\n",
       " 'ID: Yeah .',\n",
       " \"ME: and then suddenly I was like well it's just another two minute presentation\",\n",
       " \"ME: that you guys don't really care about anyway\",\n",
       " 'UI: <vocalsound> Why ?',\n",
       " 'ME: so type away . <vocalsound>',\n",
       " 'PM: Hey .',\n",
       " 'UI: <vocalsound> Huh',\n",
       " 'ME: You know , you know what I mean',\n",
       " 'UI: I think it was the real <gap> .',\n",
       " 'ME: like we all sort of knew where we were headed with it',\n",
       " 'ID: Yeah .',\n",
       " \"ME: so it didn't feel like it mattered anymore .\",\n",
       " 'UI: Yeah .',\n",
       " 'ID: Definitely when <vocalsound> when I first filled out the questionnaire I was marking it probably higher in terms of how much I had to <disfmarker> how much I stressed over it',\n",
       " 'ID: and then by the time I got to the last one I was like , you know , not very much .',\n",
       " 'ME: Whatever . <vocalsound> Yeah .',\n",
       " \"UI: Think it was also realisation of you basically just copy and paste what's given to you into your presentation\",\n",
       " 'ID: Yeah <vocalsound> .',\n",
       " 'ME: Very much , yeah .',\n",
       " \"UI: which uh wasn't so clear to me at the beginning .\",\n",
       " \"PM: <vocalsound> I actually didn't do that at all though ,\",\n",
       " 'PM: every single one I <disfmarker> a all the presentations I either added slides',\n",
       " \"PM: or edited 'em .\",\n",
       " 'PM: I di',\n",
       " 'UI: Oh',\n",
       " 'UI: I added like five slides too ,',\n",
       " 'ME: <vocalsound> See',\n",
       " 'PM: Oh .',\n",
       " 'ME: I only got blank ones .',\n",
       " 'ID: Did you really ?',\n",
       " 'UI: but I <disfmarker>',\n",
       " 'ID: I just got blank ones',\n",
       " 'ID: and <disfmarker>',\n",
       " 'PM: What ?',\n",
       " 'PM: Really ?',\n",
       " 'ME: My slides were all blank ,',\n",
       " \"ME: they'd have a title maybe\",\n",
       " 'UI: Yeah , mine too .',\n",
       " 'ME: and they were just empty .',\n",
       " 'ID: Me too .',\n",
       " \"PM: Did they not have <disfmarker> they didn't like <disfmarker> uh mine <disfmarker>\",\n",
       " 'PM: yeah',\n",
       " \"PM: they didn't come like this ?\",\n",
       " 'PM: Like with <disfmarker>',\n",
       " 'PM: this was what it looks like .',\n",
       " 'ME: Like with those words already on it ?',\n",
       " 'PM: This is what that looked like ,',\n",
       " 'PM: literally , just like that .',\n",
       " 'ME: No .',\n",
       " 'UI: No .',\n",
       " 'PM: Interesting .',\n",
       " 'ME: I wondered why yours always looked so more complicated . <vocalsound>',\n",
       " 'PM: Uh-huh huh huh .',\n",
       " 'ID: I deleted slides <vocalsound> .',\n",
       " 'ME: I think I added a slide one time .',\n",
       " 'UI: I added many slides every time <vocalsound>',\n",
       " 'ME: <vocalsound> Hey',\n",
       " 'ME: with the whole new background being innovative , yeah that was class .',\n",
       " 'UI: Yeah <vocalsound> .',\n",
       " 'ID: That was pretty cool ,',\n",
       " 'ID: it was a high moment of the whole experiment .',\n",
       " 'UI: <vocalsound> Um .',\n",
       " 'PM: Interesting .',\n",
       " 'PM: Any other thoughts come to mind ?',\n",
       " 'ME: I wanna know how our product would fare .',\n",
       " \"ME: I can't just leave it there .\",\n",
       " 'ID: I think it would fail , I think',\n",
       " \"ID: it'd be a huge disaster , especially if it looks like that . <vocalsound>\",\n",
       " 'ME: <vocalsound> I think it would take extensive marketing ,',\n",
       " 'ME: okay ,',\n",
       " 'ME: an apple with a red button on top , even I am sceptical . <vocalsound>',\n",
       " 'ME: But you know the whole <vocalsound> <disfmarker>',\n",
       " 'UI: Even you <vocalsound> .',\n",
       " 'ME: Yeah <vocalsound> .',\n",
       " \"ID: I don't <disfmarker>\",\n",
       " 'ID: we kind of designed it to look little bit like a face .',\n",
       " 'PM: <vocalsound> Yeah but <vocalsound> <disfmarker>',\n",
       " 'ME: I know it is <vocalsound> .',\n",
       " \"ID: It's a happy face .\",\n",
       " 'UI: Actually that looked a lot more like a tongue from previous to uh fr some other design uh modifications <vocalsound> .',\n",
       " 'PM: Builds .',\n",
       " 'UI: I hope you appreciate the uh incorporation of some tin foil from a uh random Kit-Kat bar',\n",
       " 'ME: <vocalsound> I I noticed that . <vocalsound> By accident .',\n",
       " 'UI: that happened to be consumed . <vocalsound>',\n",
       " 'PM: <vocalsound> Interesting .',\n",
       " 'ME: Well huh .',\n",
       " 'ME: An interesting day all in all I would say .',\n",
       " 'ID: <vocalsound> Yeah ,',\n",
       " \"PM: Uh , yeah , I'd say so .\",\n",
       " \"ID: it's uh <disfmarker>\",\n",
       " 'UI: So',\n",
       " 'UI: again I reiterate my question',\n",
       " 'UI: of how different we are comp compared to the other groups , especially between culture groups and what not .',\n",
       " 'ME: I know .',\n",
       " 'PM: Mm , I know .',\n",
       " 'PM: It seemed like everything flowed pretty logically . You know from the the the basics to the conce',\n",
       " 'ID: I wanna see a <disfmarker>',\n",
       " 'ME: I',\n",
       " 'ME: Yeah .',\n",
       " \"PM: although the whole concepts thing , the whole concepts phase , I don't think I really understood like the concept .\",\n",
       " 'PM: Well',\n",
       " 'PM: the id',\n",
       " 'PM: okay',\n",
       " \"ME: 'Cause it's such a functional item .\",\n",
       " 'PM: the notion of <disfmarker>',\n",
       " 'PM: yeah I mean',\n",
       " \"PM: i it's not like I have a concept of a mug's material ,\",\n",
       " \"PM: it's just it is what it is .\",\n",
       " 'ID: Yeah .',\n",
       " 'PM: You know ,',\n",
       " 'PM: maybe i rather than concepts i it should be th thought of <disfmarker> we sh I I thought of I thought of <disfmarker>',\n",
       " 'PM: rather than in terms of concepts I thought of it in terms of',\n",
       " 'PM: proposed idea .',\n",
       " 'PM: And then the final would be like th the actual specified prototype or whatever ,',\n",
       " 'PM: I dunno .',\n",
       " 'PM: But .',\n",
       " \"PM: All in all it's kinda interesting .\",\n",
       " 'ID: So',\n",
       " 'ID: we have more slides or ?',\n",
       " 'PM: No',\n",
       " 'PM: just this closing one .',\n",
       " 'PM: No',\n",
       " \"PM: we've established that the costs weren't really within budget ,\",\n",
       " 'PM: but we could s you know do it <disfmarker>',\n",
       " 'ME: We got it to be .',\n",
       " \"PM: We did the project evaluation based on um <vocalsound> Sarah's evaluation of on off switches\",\n",
       " 'ME: Like cutting corners .',\n",
       " 'ME: Kind of ,',\n",
       " 'ME: though it was really technically an evaluation of the product , not the project in general .',\n",
       " 'PM: and <disfmarker>',\n",
       " \"ME: Which I'm not sure is the same thing ,\",\n",
       " 'PM: True .',\n",
       " 'ME: at the time that just i made more sense ,',\n",
       " 'ME: but I could see if they were really asking about us .',\n",
       " 'PM: Yeah .',\n",
       " \"PM: 'Cause we di we had a thu think about it . Um .\",\n",
       " 'PM: Yeah .',\n",
       " \"PM: And it's all recorded ,\",\n",
       " 'PM: woo-hoo .',\n",
       " 'PM: Yeah',\n",
       " 'ME: Yay .',\n",
       " \"PM: what I'm gonna <disfmarker> I'm gonna put um <vocalsound>\",\n",
       " \"PM: I'm supposed to do this final report thing at the end\",\n",
       " \"PM: so I'll put all that into the final report as well , or as much as seems like <disfmarker>\",\n",
       " \"PM: maybe not like the articles and stuff , like because and if and so forth , but I'll put most of it in the reports .\",\n",
       " 'ID: I',\n",
       " 'ME: <vocalsound> Make it sound eloquent .',\n",
       " \"ID: <vocalsound> It'd be so cool if we get a copy of the recording .\",\n",
       " 'ME: Oh ,',\n",
       " \"ME: I have to done <disfmarker> I've <vocalsound>\",\n",
       " \"ME: I've done transcription before\",\n",
       " 'PM: <vocalsound> Nice .',\n",
       " \"ME: and it's really ridiculous how many words people say like just in the middle of their sentences like that\",\n",
       " 'PM: Oh yeah .',\n",
       " 'ME: that mean nothing .',\n",
       " \"PM: There's a whole branch of psychology that looks into that , psycholinguistics .\",\n",
       " 'UI: What the uhs and the <disfmarker>',\n",
       " 'ID: Really .',\n",
       " \"ME: There's a guy studying it here , yeah , he's studying ums and ahs or something .\",\n",
       " 'ID: Filler words or ?',\n",
       " 'PM: Yep ,',\n",
       " \"PM: they're called um disfluencies .\",\n",
       " 'ID: Disfluencies .',\n",
       " \"ME: That's a good word for it .\",\n",
       " 'PM: Yeah',\n",
       " 'PM: we like our fancy phrases and terminologies for things .',\n",
       " 'ME: Just add some prefixes ,',\n",
       " 'ME: sounds classier .',\n",
       " 'PM: Exactly',\n",
       " 'PM: uh I will save this into the project documents .',\n",
       " 'ID: I find myself hitting the send and receive button on the email a lot , just out of boredom ,',\n",
       " 'ME: I ,',\n",
       " 'PM: Yeah . Oh yeah . <vocalsound> Yeah I know .',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1fba3e1cc64084b97c29309ecf9aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[39m=\u001b[39m bert\u001b[39m.\u001b[39;49mencode(X, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pytorch_utils.py:241\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> 551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 451\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    452\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = bert.encode(X, show_progress_bar=True, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.shape[1]\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, 300)\n",
    "        self.lin2 = Linear(300, 600)\n",
    "        self.lin3 = Linear(600, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(X)  # Perform a single forward pass.\n",
    "      # out = model(X, edge_idx)  # Perform a single forward pass.\n",
    "      loss = criterion(out[train_mask], y[train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(X)\n",
    "      # out = model(X, edge_idx)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "\n",
    "      # TP, FP, FN\n",
    "      TP = ((pred[val_mask] == 1) & (y[val_mask] == 1)).sum()\n",
    "      FP = ((pred[val_mask] == 1) & (y[val_mask] == 0)).sum()\n",
    "      FN = ((pred[val_mask] == 0) & (y[val_mask] == 1)).sum()\n",
    "\n",
    "      # Calculate precision, recall, and F1 score\n",
    "      precision = TP / max((TP + FP), 1e-10)  # Avoid division by zero\n",
    "      recall = TP / max((TP + FN), 1e-10)  # Avoid division by zero\n",
    "      f1_score = 2 * (precision * recall) / max((precision + recall), 1e-10)  # Avoid division by zero\n",
    "\n",
    "      # Calculate accuracy\n",
    "      test_correct = pred[val_mask] == y[val_mask]\n",
    "      test_acc = int(test_correct.sum()) / int(val_mask.sum())\n",
    "\n",
    "      # return criterion(model(X, edge_idx)[val_mask], y[val_mask]), f1_score\n",
    "      return criterion(model(X)[val_mask], y[val_mask]), f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [72623, 384]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m201\u001b[39m):\n\u001b[1;32m      2\u001b[0m     train_mask, val_mask \u001b[39m=\u001b[39m split(sss, X, y)\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m      4\u001b[0m     val_loss, f1_score \u001b[39m=\u001b[39m test()\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, F1-score: \u001b[39m\u001b[39m{\u001b[39;00mf1_score\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m out \u001b[39m=\u001b[39m model(X)  \u001b[39m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# out = model(X, edge_idx)  # Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[train_mask], y[train_mask])  \u001b[39m# Compute the loss solely based on the training nodes.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [72623, 384]"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    train_mask, val_mask = split(sss, X, y)\n",
    "    train_loss = train()\n",
    "    val_loss, f1_score = test()\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, F1-score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "# model = GCN(hidden_channels=500)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(X)  # Perform a single forward pass.\n",
    "      # out = model(X, edge_idx)  # Perform a single forward pass.\n",
    "      loss = criterion(out[train_mask], y[train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(X)\n",
    "      # out = model(X, edge_idx)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "\n",
    "      # TP, FP, FN\n",
    "      TP = ((pred[val_mask] == 1) & (y[val_mask] == 1)).sum()\n",
    "      FP = ((pred[val_mask] == 1) & (y[val_mask] == 0)).sum()\n",
    "      FN = ((pred[val_mask] == 0) & (y[val_mask] == 1)).sum()\n",
    "\n",
    "      # Calculate precision, recall, and F1 score\n",
    "      precision = TP / max((TP + FP), 1e-10)  # Avoid division by zero\n",
    "      recall = TP / max((TP + FN), 1e-10)  # Avoid division by zero\n",
    "      f1_score = 2 * (precision * recall) / max((precision + recall), 1e-10)  # Avoid division by zero\n",
    "\n",
    "      # Calculate accuracy\n",
    "      test_correct = pred[val_mask] == y[val_mask]\n",
    "      test_acc = int(test_correct.sum()) / int(val_mask.sum())\n",
    "\n",
    "      # return criterion(model(X, edge_idx)[val_mask], y[val_mask]), f1_score\n",
    "      return criterion(model(X)[val_mask], y[val_mask]), f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.6664, Val loss: 0.4499, F1-score: 0.0000\n",
      "Epoch: 002, Train Loss: 0.4510, Val loss: 0.6845, F1-score: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m201\u001b[39m):\n\u001b[1;32m      2\u001b[0m     train_mask, val_mask \u001b[39m=\u001b[39m split(sss, X, y)\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m      4\u001b[0m     val_loss, f1_score \u001b[39m=\u001b[39m test()\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, F1-score: \u001b[39m\u001b[39m{\u001b[39;00mf1_score\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# out = model(X, edge_idx)  # Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[train_mask], y[train_mask])  \u001b[39m# Compute the loss solely based on the training nodes.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    train_mask, val_mask = split(sss, X, y)\n",
    "    train_loss = train()\n",
    "    val_loss, f1_score = test()\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, F1-score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3364, F1-score: 0.5692\n"
     ]
    }
   ],
   "source": [
    "test_acc, f1_score = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}, F1-score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nodes, test_edges = custom_utils.gather_dataset(\"test\")\n",
    "\n",
    "test_labels = {}\n",
    "model.eval()\n",
    "for id, sentences in test_nodes.items():\n",
    "    X_test = bert.encode(sentences, convert_to_tensor=True)\n",
    "    out = model(X_test)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_labels[id] = pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to generate test labels\n",
    "# with open(\"test_labels.json\", \"w\") as json_file:\n",
    "#     json.dump(test_labels, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
