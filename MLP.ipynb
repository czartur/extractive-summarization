{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import custom_utils\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labels\n",
    "with open(\"training_labels.json\", \"r\") as json_file:\n",
    "    labels = json.load(json_file)\n",
    "\n",
    "# read nodes and edges\n",
    "nodes, edges = custom_utils.gather_dataset(\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data from all dialogs\n",
    "X, y, edge_idx = [], [], [] \n",
    "count = 0\n",
    "for id in nodes.keys():\n",
    "        X += nodes[id]\n",
    "        y += labels[id]\n",
    "        edge_idx += [[e[0] + count, e[1] + count] for e in edges[id]]\n",
    "        count += len(labels[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# stratified split (we do it every epoch)\n",
    "def split(spliter, X, y):\n",
    "    train_idx, val_idx = list(spliter.split(X, y))[0]\n",
    "    train_mask, val_mask = [False]*len(X), [False]*len(X)\n",
    "    for idx in train_idx: train_mask[idx] = True\n",
    "    for idx in val_idx: val_mask[idx] = True\n",
    "    train_mask = torch.Tensor(train_mask).bool()\n",
    "    val_mask = torch.Tensor(val_mask).bool()\n",
    "    return train_mask, val_mask\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state=42)\n",
    "train_mask, val_mask = split(sss, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting\n",
    "y = torch.Tensor(y).long()\n",
    "edge_idx = torch.Tensor(edge_idx).long().transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2270/2270 [01:30<00:00, 25.11it/s]\n"
     ]
    }
   ],
   "source": [
    "X = bert.encode(X, show_progress_bar=True, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.shape[1]\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, 300)\n",
    "        self.lin2 = Linear(300, 600)\n",
    "        self.lin3 = Linear(600, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels, aggr=\"lstm\")\n",
    "        self.conv2 = SAGEConv(hidden_channels, num_classes, aggr=\"lstm\")\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "# model = GCN(hidden_channels=500)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(X)  # Perform a single forward pass.\n",
    "      # out = model(X, edge_idx)  # Perform a single forward pass.\n",
    "      loss = criterion(out[train_mask], y[train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(X)\n",
    "      # out = model(X, edge_idx)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "\n",
    "      # TP, FP, FN\n",
    "      TP = ((pred[val_mask] == 1) & (y[val_mask] == 1)).sum()\n",
    "      FP = ((pred[val_mask] == 1) & (y[val_mask] == 0)).sum()\n",
    "      FN = ((pred[val_mask] == 0) & (y[val_mask] == 1)).sum()\n",
    "\n",
    "      # Calculate precision, recall, and F1 score\n",
    "      precision = TP / max((TP + FP), 1e-10)  # Avoid division by zero\n",
    "      recall = TP / max((TP + FN), 1e-10)  # Avoid division by zero\n",
    "      f1_score = 2 * (precision * recall) / max((precision + recall), 1e-10)  # Avoid division by zero\n",
    "\n",
    "      # Calculate accuracy\n",
    "      test_correct = pred[val_mask] == y[val_mask]\n",
    "      test_acc = int(test_correct.sum()) / int(val_mask.sum())\n",
    "\n",
    "      # return criterion(model(X, edge_idx)[val_mask], y[val_mask]), f1_score\n",
    "      return criterion(model(X)[val_mask], y[val_mask]), f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.6664, Val loss: 0.4495, F1-score: 0.0000\n",
      "Epoch: 002, Train Loss: 0.4509, Val loss: 0.6792, F1-score: 0.0000\n",
      "Epoch: 003, Train Loss: 0.6797, Val loss: 0.4001, F1-score: 0.0000\n",
      "Epoch: 004, Train Loss: 0.4008, Val loss: 0.4560, F1-score: 0.0000\n",
      "Epoch: 005, Train Loss: 0.4542, Val loss: 0.4858, F1-score: 0.0000\n",
      "Epoch: 006, Train Loss: 0.4836, Val loss: 0.4671, F1-score: 0.0000\n",
      "Epoch: 007, Train Loss: 0.4653, Val loss: 0.4245, F1-score: 0.0000\n",
      "Epoch: 008, Train Loss: 0.4234, Val loss: 0.3854, F1-score: 0.0000\n",
      "Epoch: 009, Train Loss: 0.3845, Val loss: 0.3831, F1-score: 0.0000\n",
      "Epoch: 010, Train Loss: 0.3809, Val loss: 0.3942, F1-score: 0.0000\n",
      "Epoch: 011, Train Loss: 0.3906, Val loss: 0.3895, F1-score: 0.0000\n",
      "Epoch: 012, Train Loss: 0.3850, Val loss: 0.3745, F1-score: 0.0000\n",
      "Epoch: 013, Train Loss: 0.3691, Val loss: 0.3618, F1-score: 0.0000\n",
      "Epoch: 014, Train Loss: 0.3569, Val loss: 0.3602, F1-score: 0.0000\n",
      "Epoch: 015, Train Loss: 0.3564, Val loss: 0.3622, F1-score: 0.0000\n",
      "Epoch: 016, Train Loss: 0.3577, Val loss: 0.3586, F1-score: 0.0000\n",
      "Epoch: 017, Train Loss: 0.3548, Val loss: 0.3543, F1-score: 0.0000\n",
      "Epoch: 018, Train Loss: 0.3504, Val loss: 0.3529, F1-score: 0.0000\n",
      "Epoch: 019, Train Loss: 0.3488, Val loss: 0.3528, F1-score: 0.0000\n",
      "Epoch: 020, Train Loss: 0.3489, Val loss: 0.3511, F1-score: 0.0000\n",
      "Epoch: 021, Train Loss: 0.3471, Val loss: 0.3477, F1-score: 0.0000\n",
      "Epoch: 022, Train Loss: 0.3447, Val loss: 0.3448, F1-score: 0.0000\n",
      "Epoch: 023, Train Loss: 0.3426, Val loss: 0.3438, F1-score: 0.0000\n",
      "Epoch: 024, Train Loss: 0.3410, Val loss: 0.3423, F1-score: 0.0000\n",
      "Epoch: 025, Train Loss: 0.3398, Val loss: 0.3407, F1-score: 0.0090\n",
      "Epoch: 026, Train Loss: 0.3383, Val loss: 0.3401, F1-score: 0.1105\n",
      "Epoch: 027, Train Loss: 0.3374, Val loss: 0.3387, F1-score: 0.2934\n",
      "Epoch: 028, Train Loss: 0.3359, Val loss: 0.3373, F1-score: 0.3853\n",
      "Epoch: 029, Train Loss: 0.3339, Val loss: 0.3364, F1-score: 0.4524\n",
      "Epoch: 030, Train Loss: 0.3332, Val loss: 0.3354, F1-score: 0.4676\n",
      "Epoch: 031, Train Loss: 0.3320, Val loss: 0.3348, F1-score: 0.4572\n",
      "Epoch: 032, Train Loss: 0.3312, Val loss: 0.3339, F1-score: 0.4585\n",
      "Epoch: 033, Train Loss: 0.3296, Val loss: 0.3326, F1-score: 0.4769\n",
      "Epoch: 034, Train Loss: 0.3287, Val loss: 0.3318, F1-score: 0.4774\n",
      "Epoch: 035, Train Loss: 0.3280, Val loss: 0.3315, F1-score: 0.4326\n",
      "Epoch: 036, Train Loss: 0.3270, Val loss: 0.3307, F1-score: 0.4669\n",
      "Epoch: 037, Train Loss: 0.3254, Val loss: 0.3305, F1-score: 0.4769\n",
      "Epoch: 038, Train Loss: 0.3247, Val loss: 0.3308, F1-score: 0.4393\n",
      "Epoch: 039, Train Loss: 0.3244, Val loss: 0.3305, F1-score: 0.5017\n",
      "Epoch: 040, Train Loss: 0.3239, Val loss: 0.3317, F1-score: 0.3720\n",
      "Epoch: 041, Train Loss: 0.3244, Val loss: 0.3292, F1-score: 0.4678\n",
      "Epoch: 042, Train Loss: 0.3218, Val loss: 0.3296, F1-score: 0.4932\n",
      "Epoch: 043, Train Loss: 0.3229, Val loss: 0.3302, F1-score: 0.4181\n",
      "Epoch: 044, Train Loss: 0.3214, Val loss: 0.3290, F1-score: 0.4603\n",
      "Epoch: 045, Train Loss: 0.3205, Val loss: 0.3292, F1-score: 0.5051\n",
      "Epoch: 046, Train Loss: 0.3206, Val loss: 0.3298, F1-score: 0.4028\n",
      "Epoch: 047, Train Loss: 0.3207, Val loss: 0.3279, F1-score: 0.4648\n",
      "Epoch: 048, Train Loss: 0.3188, Val loss: 0.3280, F1-score: 0.4947\n",
      "Epoch: 049, Train Loss: 0.3185, Val loss: 0.3292, F1-score: 0.4233\n",
      "Epoch: 050, Train Loss: 0.3192, Val loss: 0.3274, F1-score: 0.4909\n",
      "Epoch: 051, Train Loss: 0.3175, Val loss: 0.3268, F1-score: 0.4830\n",
      "Epoch: 052, Train Loss: 0.3162, Val loss: 0.3281, F1-score: 0.4131\n",
      "Epoch: 053, Train Loss: 0.3175, Val loss: 0.3283, F1-score: 0.5217\n",
      "Epoch: 054, Train Loss: 0.3189, Val loss: 0.3293, F1-score: 0.3854\n",
      "Epoch: 055, Train Loss: 0.3176, Val loss: 0.3265, F1-score: 0.4848\n",
      "Epoch: 056, Train Loss: 0.3156, Val loss: 0.3264, F1-score: 0.4911\n",
      "Epoch: 057, Train Loss: 0.3150, Val loss: 0.3287, F1-score: 0.4083\n",
      "Epoch: 058, Train Loss: 0.3157, Val loss: 0.3272, F1-score: 0.5248\n",
      "Epoch: 059, Train Loss: 0.3154, Val loss: 0.3270, F1-score: 0.4278\n",
      "Epoch: 060, Train Loss: 0.3146, Val loss: 0.3258, F1-score: 0.4650\n",
      "Epoch: 061, Train Loss: 0.3126, Val loss: 0.3260, F1-score: 0.4957\n",
      "Epoch: 062, Train Loss: 0.3128, Val loss: 0.3293, F1-score: 0.4012\n",
      "Epoch: 063, Train Loss: 0.3140, Val loss: 0.3305, F1-score: 0.5435\n",
      "Epoch: 064, Train Loss: 0.3173, Val loss: 0.3346, F1-score: 0.3450\n",
      "Epoch: 065, Train Loss: 0.3187, Val loss: 0.3264, F1-score: 0.4866\n",
      "Epoch: 066, Train Loss: 0.3127, Val loss: 0.3265, F1-score: 0.5004\n",
      "Epoch: 067, Train Loss: 0.3130, Val loss: 0.3315, F1-score: 0.3966\n",
      "Epoch: 068, Train Loss: 0.3151, Val loss: 0.3260, F1-score: 0.5153\n",
      "Epoch: 069, Train Loss: 0.3112, Val loss: 0.3250, F1-score: 0.5012\n",
      "Epoch: 070, Train Loss: 0.3092, Val loss: 0.3295, F1-score: 0.3944\n",
      "Epoch: 071, Train Loss: 0.3126, Val loss: 0.3266, F1-score: 0.5225\n",
      "Epoch: 072, Train Loss: 0.3117, Val loss: 0.3250, F1-score: 0.4487\n",
      "Epoch: 073, Train Loss: 0.3077, Val loss: 0.3266, F1-score: 0.4340\n",
      "Epoch: 074, Train Loss: 0.3082, Val loss: 0.3272, F1-score: 0.5285\n",
      "Epoch: 075, Train Loss: 0.3109, Val loss: 0.3295, F1-score: 0.3954\n",
      "Epoch: 076, Train Loss: 0.3106, Val loss: 0.3249, F1-score: 0.5020\n",
      "Epoch: 077, Train Loss: 0.3073, Val loss: 0.3240, F1-score: 0.4633\n",
      "Epoch: 078, Train Loss: 0.3059, Val loss: 0.3265, F1-score: 0.4253\n",
      "Epoch: 079, Train Loss: 0.3056, Val loss: 0.3265, F1-score: 0.5317\n",
      "Epoch: 080, Train Loss: 0.3082, Val loss: 0.3307, F1-score: 0.4012\n",
      "Epoch: 081, Train Loss: 0.3094, Val loss: 0.3266, F1-score: 0.5272\n",
      "Epoch: 082, Train Loss: 0.3081, Val loss: 0.3261, F1-score: 0.4106\n",
      "Epoch: 083, Train Loss: 0.3043, Val loss: 0.3240, F1-score: 0.4702\n",
      "Epoch: 084, Train Loss: 0.3027, Val loss: 0.3242, F1-score: 0.5118\n",
      "Epoch: 085, Train Loss: 0.3031, Val loss: 0.3289, F1-score: 0.4209\n",
      "Epoch: 086, Train Loss: 0.3051, Val loss: 0.3299, F1-score: 0.5517\n",
      "Epoch: 087, Train Loss: 0.3095, Val loss: 0.3392, F1-score: 0.3160\n",
      "Epoch: 088, Train Loss: 0.3145, Val loss: 0.3295, F1-score: 0.5350\n",
      "Epoch: 089, Train Loss: 0.3098, Val loss: 0.3234, F1-score: 0.4738\n",
      "Epoch: 090, Train Loss: 0.3007, Val loss: 0.3327, F1-score: 0.3975\n",
      "Epoch: 091, Train Loss: 0.3081, Val loss: 0.3289, F1-score: 0.5463\n",
      "Epoch: 092, Train Loss: 0.3081, Val loss: 0.3248, F1-score: 0.4480\n",
      "Epoch: 093, Train Loss: 0.3009, Val loss: 0.3266, F1-score: 0.4170\n",
      "Epoch: 094, Train Loss: 0.3018, Val loss: 0.3272, F1-score: 0.5310\n",
      "Epoch: 095, Train Loss: 0.3043, Val loss: 0.3260, F1-score: 0.4315\n",
      "Epoch: 096, Train Loss: 0.3001, Val loss: 0.3239, F1-score: 0.4636\n",
      "Epoch: 097, Train Loss: 0.2988, Val loss: 0.3254, F1-score: 0.5386\n",
      "Epoch: 098, Train Loss: 0.3014, Val loss: 0.3269, F1-score: 0.4229\n",
      "Epoch: 099, Train Loss: 0.2993, Val loss: 0.3230, F1-score: 0.4810\n",
      "Epoch: 100, Train Loss: 0.2954, Val loss: 0.3237, F1-score: 0.5044\n",
      "Epoch: 101, Train Loss: 0.2967, Val loss: 0.3286, F1-score: 0.4144\n",
      "Epoch: 102, Train Loss: 0.2986, Val loss: 0.3264, F1-score: 0.5452\n",
      "Epoch: 103, Train Loss: 0.2991, Val loss: 0.3274, F1-score: 0.4257\n",
      "Epoch: 104, Train Loss: 0.2960, Val loss: 0.3233, F1-score: 0.5059\n",
      "Epoch: 105, Train Loss: 0.2937, Val loss: 0.3232, F1-score: 0.4739\n",
      "Epoch: 106, Train Loss: 0.2924, Val loss: 0.3247, F1-score: 0.4500\n",
      "Epoch: 107, Train Loss: 0.2923, Val loss: 0.3248, F1-score: 0.5294\n",
      "Epoch: 108, Train Loss: 0.2936, Val loss: 0.3355, F1-score: 0.3783\n",
      "Epoch: 109, Train Loss: 0.3001, Val loss: 0.3430, F1-score: 0.5675\n",
      "Epoch: 110, Train Loss: 0.3151, Val loss: 0.3596, F1-score: 0.2599\n",
      "Epoch: 111, Train Loss: 0.3246, Val loss: 0.3266, F1-score: 0.4845\n",
      "Epoch: 112, Train Loss: 0.2999, Val loss: 0.3311, F1-score: 0.5240\n",
      "Epoch: 113, Train Loss: 0.3049, Val loss: 0.3384, F1-score: 0.3901\n",
      "Epoch: 114, Train Loss: 0.3049, Val loss: 0.3275, F1-score: 0.4871\n",
      "Epoch: 115, Train Loss: 0.2946, Val loss: 0.3332, F1-score: 0.5570\n",
      "Epoch: 116, Train Loss: 0.3065, Val loss: 0.3270, F1-score: 0.4227\n",
      "Epoch: 117, Train Loss: 0.2950, Val loss: 0.3288, F1-score: 0.3861\n",
      "Epoch: 118, Train Loss: 0.2965, Val loss: 0.3261, F1-score: 0.5246\n",
      "Epoch: 119, Train Loss: 0.2983, Val loss: 0.3231, F1-score: 0.4971\n",
      "Epoch: 120, Train Loss: 0.2923, Val loss: 0.3304, F1-score: 0.4123\n",
      "Epoch: 121, Train Loss: 0.2973, Val loss: 0.3229, F1-score: 0.5067\n",
      "Epoch: 122, Train Loss: 0.2907, Val loss: 0.3244, F1-score: 0.5331\n",
      "Epoch: 123, Train Loss: 0.2931, Val loss: 0.3281, F1-score: 0.4179\n",
      "Epoch: 124, Train Loss: 0.2936, Val loss: 0.3235, F1-score: 0.4751\n",
      "Epoch: 125, Train Loss: 0.2898, Val loss: 0.3251, F1-score: 0.5312\n",
      "Epoch: 126, Train Loss: 0.2923, Val loss: 0.3268, F1-score: 0.4365\n",
      "Epoch: 127, Train Loss: 0.2895, Val loss: 0.3238, F1-score: 0.4814\n",
      "Epoch: 128, Train Loss: 0.2871, Val loss: 0.3242, F1-score: 0.5243\n",
      "Epoch: 129, Train Loss: 0.2891, Val loss: 0.3283, F1-score: 0.4272\n",
      "Epoch: 130, Train Loss: 0.2883, Val loss: 0.3241, F1-score: 0.5047\n",
      "Epoch: 131, Train Loss: 0.2851, Val loss: 0.3242, F1-score: 0.4867\n",
      "Epoch: 132, Train Loss: 0.2842, Val loss: 0.3266, F1-score: 0.4540\n",
      "Epoch: 133, Train Loss: 0.2860, Val loss: 0.3258, F1-score: 0.5335\n",
      "Epoch: 134, Train Loss: 0.2870, Val loss: 0.3326, F1-score: 0.4173\n",
      "Epoch: 135, Train Loss: 0.2873, Val loss: 0.3272, F1-score: 0.5403\n",
      "Epoch: 136, Train Loss: 0.2875, Val loss: 0.3319, F1-score: 0.4097\n",
      "Epoch: 137, Train Loss: 0.2865, Val loss: 0.3250, F1-score: 0.5336\n",
      "Epoch: 138, Train Loss: 0.2843, Val loss: 0.3268, F1-score: 0.4602\n",
      "Epoch: 139, Train Loss: 0.2818, Val loss: 0.3243, F1-score: 0.5051\n",
      "Epoch: 140, Train Loss: 0.2799, Val loss: 0.3246, F1-score: 0.4998\n",
      "Epoch: 141, Train Loss: 0.2784, Val loss: 0.3261, F1-score: 0.4770\n",
      "Epoch: 142, Train Loss: 0.2784, Val loss: 0.3251, F1-score: 0.5245\n",
      "Epoch: 143, Train Loss: 0.2796, Val loss: 0.3399, F1-score: 0.3592\n",
      "Epoch: 144, Train Loss: 0.2890, Val loss: 0.3612, F1-score: 0.5707\n",
      "Epoch: 145, Train Loss: 0.3258, Val loss: 0.4118, F1-score: 0.1983\n",
      "Epoch: 146, Train Loss: 0.3639, Val loss: 0.3312, F1-score: 0.5230\n",
      "Epoch: 147, Train Loss: 0.2963, Val loss: 0.3494, F1-score: 0.5608\n",
      "Epoch: 148, Train Loss: 0.3253, Val loss: 0.3341, F1-score: 0.3576\n",
      "Epoch: 149, Train Loss: 0.2993, Val loss: 0.3544, F1-score: 0.3289\n",
      "Epoch: 150, Train Loss: 0.3165, Val loss: 0.3281, F1-score: 0.5191\n",
      "Epoch: 151, Train Loss: 0.2956, Val loss: 0.3364, F1-score: 0.5526\n",
      "Epoch: 152, Train Loss: 0.3098, Val loss: 0.3255, F1-score: 0.4144\n",
      "Epoch: 153, Train Loss: 0.2942, Val loss: 0.3343, F1-score: 0.2967\n",
      "Epoch: 154, Train Loss: 0.3007, Val loss: 0.3249, F1-score: 0.4358\n",
      "Epoch: 155, Train Loss: 0.2943, Val loss: 0.3261, F1-score: 0.5322\n",
      "Epoch: 156, Train Loss: 0.2977, Val loss: 0.3254, F1-score: 0.4650\n",
      "Epoch: 157, Train Loss: 0.2929, Val loss: 0.3266, F1-score: 0.4551\n",
      "Epoch: 158, Train Loss: 0.2932, Val loss: 0.3235, F1-score: 0.4934\n",
      "Epoch: 159, Train Loss: 0.2899, Val loss: 0.3233, F1-score: 0.5062\n",
      "Epoch: 160, Train Loss: 0.2908, Val loss: 0.3234, F1-score: 0.4654\n",
      "Epoch: 161, Train Loss: 0.2883, Val loss: 0.3245, F1-score: 0.4486\n",
      "Epoch: 162, Train Loss: 0.2876, Val loss: 0.3235, F1-score: 0.4993\n",
      "Epoch: 163, Train Loss: 0.2854, Val loss: 0.3239, F1-score: 0.5064\n",
      "Epoch: 164, Train Loss: 0.2852, Val loss: 0.3253, F1-score: 0.4753\n",
      "Epoch: 165, Train Loss: 0.2837, Val loss: 0.3244, F1-score: 0.5051\n",
      "Epoch: 166, Train Loss: 0.2815, Val loss: 0.3246, F1-score: 0.4959\n",
      "Epoch: 167, Train Loss: 0.2823, Val loss: 0.3254, F1-score: 0.4872\n",
      "Epoch: 168, Train Loss: 0.2804, Val loss: 0.3255, F1-score: 0.4913\n",
      "Epoch: 169, Train Loss: 0.2779, Val loss: 0.3261, F1-score: 0.4927\n",
      "Epoch: 170, Train Loss: 0.2789, Val loss: 0.3267, F1-score: 0.4778\n",
      "Epoch: 171, Train Loss: 0.2778, Val loss: 0.3258, F1-score: 0.5058\n",
      "Epoch: 172, Train Loss: 0.2767, Val loss: 0.3265, F1-score: 0.4865\n",
      "Epoch: 173, Train Loss: 0.2772, Val loss: 0.3263, F1-score: 0.4895\n",
      "Epoch: 174, Train Loss: 0.2750, Val loss: 0.3264, F1-score: 0.4931\n",
      "Epoch: 175, Train Loss: 0.2736, Val loss: 0.3279, F1-score: 0.4773\n",
      "Epoch: 176, Train Loss: 0.2734, Val loss: 0.3264, F1-score: 0.5148\n",
      "Epoch: 177, Train Loss: 0.2738, Val loss: 0.3312, F1-score: 0.4445\n",
      "Epoch: 178, Train Loss: 0.2739, Val loss: 0.3286, F1-score: 0.5512\n",
      "Epoch: 179, Train Loss: 0.2770, Val loss: 0.3438, F1-score: 0.3740\n",
      "Epoch: 180, Train Loss: 0.2836, Val loss: 0.3390, F1-score: 0.5689\n",
      "Epoch: 181, Train Loss: 0.2912, Val loss: 0.3486, F1-score: 0.3461\n",
      "Epoch: 182, Train Loss: 0.2877, Val loss: 0.3256, F1-score: 0.5222\n",
      "Epoch: 183, Train Loss: 0.2741, Val loss: 0.3260, F1-score: 0.5362\n",
      "Epoch: 184, Train Loss: 0.2760, Val loss: 0.3414, F1-score: 0.3817\n",
      "Epoch: 185, Train Loss: 0.2832, Val loss: 0.3275, F1-score: 0.5441\n",
      "Epoch: 186, Train Loss: 0.2758, Val loss: 0.3263, F1-score: 0.4948\n",
      "Epoch: 187, Train Loss: 0.2699, Val loss: 0.3335, F1-score: 0.4178\n",
      "Epoch: 188, Train Loss: 0.2728, Val loss: 0.3292, F1-score: 0.5496\n",
      "Epoch: 189, Train Loss: 0.2787, Val loss: 0.3344, F1-score: 0.4037\n",
      "Epoch: 190, Train Loss: 0.2758, Val loss: 0.3248, F1-score: 0.5024\n",
      "Epoch: 191, Train Loss: 0.2672, Val loss: 0.3255, F1-score: 0.5163\n",
      "Epoch: 192, Train Loss: 0.2701, Val loss: 0.3344, F1-score: 0.4069\n",
      "Epoch: 193, Train Loss: 0.2734, Val loss: 0.3280, F1-score: 0.5445\n",
      "Epoch: 194, Train Loss: 0.2726, Val loss: 0.3315, F1-score: 0.4355\n",
      "Epoch: 195, Train Loss: 0.2687, Val loss: 0.3254, F1-score: 0.5073\n",
      "Epoch: 196, Train Loss: 0.2668, Val loss: 0.3249, F1-score: 0.5090\n",
      "Epoch: 197, Train Loss: 0.2645, Val loss: 0.3341, F1-score: 0.4108\n",
      "Epoch: 198, Train Loss: 0.2700, Val loss: 0.3330, F1-score: 0.5668\n",
      "Epoch: 199, Train Loss: 0.2779, Val loss: 0.3535, F1-score: 0.3348\n",
      "Epoch: 200, Train Loss: 0.2847, Val loss: 0.3364, F1-score: 0.5692\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    train_mask, val_mask = split(sss, X, y)\n",
    "    train_loss = train()\n",
    "    val_loss, f1_score = test()\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, F1-score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3364, F1-score: 0.5692\n"
     ]
    }
   ],
   "source": [
    "test_acc, f1_score = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}, F1-score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nodes, test_edges = custom_utils.gather_dataset(\"test\")\n",
    "\n",
    "test_labels = {}\n",
    "model.eval()\n",
    "for id, sentences in test_nodes.items():\n",
    "    X_test = bert.encode(sentences, convert_to_tensor=True)\n",
    "    out = model(X_test)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_labels[id] = pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to generate test labels\n",
    "# with open(\"test_labels.json\", \"w\") as json_file:\n",
    "#     json.dump(test_labels, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
