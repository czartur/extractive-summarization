{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import custom_utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>speakers</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45504</th>\n",
       "      <td>Smiling fish &lt;vocalsound&gt; .</td>\n",
       "      <td>PM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47790</th>\n",
       "      <td>Robustness , uh-huh .</td>\n",
       "      <td>ME</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44498</th>\n",
       "      <td>Because the the the electronic device's price ...</td>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>so it's very lousy .</td>\n",
       "      <td>UI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12142</th>\n",
       "      <td>Okay .</td>\n",
       "      <td>PM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences speakers  labels\n",
       "45504                        Smiling fish <vocalsound> .       PM       0\n",
       "47790                              Robustness , uh-huh .       ME       1\n",
       "44498  Because the the the electronic device's price ...       ID       0\n",
       "49154                               so it's very lousy .       UI       1\n",
       "12142                                             Okay .       PM       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read\n",
    "sentences, speakers, labels = custom_utils.read_data(\"training\", \"training_labels.json\")\n",
    "\n",
    "# split\n",
    "df = pd.DataFrame({\"sentences\" : sentences, \"speakers\" : speakers, \"labels\" : labels})\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=69, stratify=df.labels)\n",
    "\n",
    "# train, valid = train_test_split(train, test_size=0.3, random_state=69, stratify=train.labels)\n",
    "\n",
    "\n",
    "# print(f\"Train: {len(train)}\\nTest: {len(test)}\\nValid: {len(valid)}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 1816/1816 [01:25<00:00, 21.34it/s]\n",
      "Batches: 100%|██████████| 454/454 [00:14<00:00, 30.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# hot encoder for speakers\n",
    "switcher = {\n",
    "    \"PM\" : [1,0,0,0],\n",
    "    \"ME\" : [0,1,0,0],\n",
    "    \"UI\" : [0,0,1,0],\n",
    "    \"ID\" : [0,0,0,1]\n",
    "}\n",
    "\n",
    "# embed\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "train_sentences = bert.encode(train['sentences'].to_numpy(), convert_to_tensor=True, show_progress_bar=True).to(device)\n",
    "train_speaker = torch.Tensor([switcher[el] for el in train['speakers']]).to(device)\n",
    "train_len = torch.Tensor([[len(sentence.split())] for sentence in train['sentences']]).to(device)\n",
    "train_X = torch.cat((train_sentences, train_speaker, train_len), dim=1)\n",
    "# train_X = torch.cat((train_sentences, train_speaker), dim=1)\n",
    "train_y = torch.tensor(train['labels'].to_numpy())\n",
    "\n",
    "# valid_sentences = bert.encode(valid['sentences'].to_numpy(), convert_to_tensor=True, show_progress_bar=True).to(device)\n",
    "# valid_speaker = torch.Tensor([switcher[el] for el in valid['speakers']]).to(device)\n",
    "# valid_len = torch.Tensor([len(sentence) for sentence in valid['sentences']]).to(device)\n",
    "# valid_X = torch.cat((valid_sentences, valid_speaker, valid_len), dim=1)\n",
    "# valid_y = torch.tensor(valid['labels'].to_numpy())\n",
    "\n",
    "test_sentences = bert.encode(test['sentences'].to_numpy(), convert_to_tensor=True, show_progress_bar=True).to(device)\n",
    "test_speaker = torch.Tensor([switcher[el] for el in test['speakers']]).to(device)\n",
    "test_len = torch.Tensor([[len(sentence.split())] for sentence in test['sentences']]).to(device)\n",
    "test_X = torch.cat((test_sentences, test_speaker, test_len), dim=1)\n",
    "# test_X = torch.cat((test_sentences, test_speaker), dim=1)\n",
    "test_y = torch.tensor(test['labels'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def data_loader(batch_size):\n",
    "    # create tensor datasets\n",
    "    trainset = TensorDataset((train_X).to(device), (train_y).to(device))\n",
    "    # validset = TensorDataset((valid_X).to(device), (valid_y).to(device))\n",
    "    testset = TensorDataset((test_X).to(device), (test_y).to(device))\n",
    "\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(trainset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    # valid_loader = DataLoader(validset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    test_loader = DataLoader(testset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(params):\n",
    "    # Model\n",
    "    n_layers = params['n_layers']\n",
    "    layers = []\n",
    "\n",
    "    in_features = params['input_size']\n",
    "    for i in range(n_layers):\n",
    "        out_features = params[f'n_{i}_size']\n",
    "        layers.append(torch.nn.Linear(in_features, out_features))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # suggest dropout\n",
    "        p = params['n_p']\n",
    "        layers.append(torch.nn.Dropout(p))\n",
    "\n",
    "        # updating next layer size\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(torch.nn.Linear(in_features, params['output_size']))\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "def train_MLP(params):\n",
    "        \n",
    "    model = MLP(params)\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "    f1 = F1Score(task='binary', num_classes=params['output_size']).to(device)\n",
    "\n",
    "    train_loader, test_loader = data_loader(params[\"batch_size\"])\n",
    "    \n",
    "    n_epochs = 20\n",
    "    it = 0\n",
    "    hst_train_loss = [] \n",
    "    hst_test_loss = []\n",
    "    hst_f1_score = []\n",
    "\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_f1_score = 0\n",
    "    patience = 10\n",
    "    for epoch in range(n_epochs):\n",
    "        if patience == 0: break\n",
    "        for samples, labels in train_loader:\n",
    "            if patience == 0: break\n",
    "            it += 1\n",
    "\n",
    "            # train step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(samples)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            if it % 100 == 0:\n",
    "                model.eval()\n",
    "\n",
    "                train_loss = loss.cpu().detach().numpy() / 1\n",
    "                test_loss = 0\n",
    "                f1_score = 0\n",
    "                for samples, labels in test_loader:\n",
    "                    out = model(samples)\n",
    "                    loss = criterion(out, labels)\n",
    "                    test_loss += loss.cpu().detach().numpy() / len(test_loader)\n",
    "                    f1_score += f1(labels, out.argmax(dim=1)).cpu().detach().numpy() / len(test_loader)\n",
    "                \n",
    "                if f1_score > best_f1_score:\n",
    "                    best_f1_score = f1_score\n",
    "                    best_weights = model.state_dict()\n",
    "\n",
    "                # early stopping\n",
    "                if test_loss < best_test_loss:\n",
    "                    best_test_loss = test_loss\n",
    "                    patience = 10\n",
    "                else:\n",
    "                    patience -= 1 \n",
    "                \n",
    "                hst_train_loss.append(train_loss)\n",
    "                hst_test_loss.append(test_loss)\n",
    "                hst_f1_score.append(f1_score)\n",
    "\n",
    "                print('Iter: {} | Train Loss: {} | Val Loss: {} | F1-score: {}'.format(it, train_loss, test_loss, f1_score))\n",
    "\n",
    "    return best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('232', 0.589133054469571),\n",
       " ('212', 0.588377456773411),\n",
       " ('138', 0.5879984432604255),\n",
       " ('222', 0.5878143520517782),\n",
       " ('132', 0.5876851022716553),\n",
       " ('267', 0.5876206554698221),\n",
       " ('210', 0.5875831268953555),\n",
       " ('442', 0.587565704371578),\n",
       " ('238', 0.5875579280446664),\n",
       " ('385', 0.587500210286993),\n",
       " ('100', 0.5874619231960637),\n",
       " ('211', 0.5874350281186805),\n",
       " ('214', 0.5873827083092746),\n",
       " ('152', 0.5873703920259709),\n",
       " ('66', 0.5870380125939847),\n",
       " ('71', 0.5870271862456294),\n",
       " ('322', 0.5869192556729393),\n",
       " ('231', 0.5868994530403254),\n",
       " ('209', 0.5868964544100355),\n",
       " ('124', 0.5868047824720056),\n",
       " ('335', 0.5867338250532296),\n",
       " ('380', 0.5866987007532934),\n",
       " ('374', 0.5865984380696759),\n",
       " ('241', 0.5865795743284803),\n",
       " ('69', 0.5865025361852041),\n",
       " ('144', 0.586411763531293),\n",
       " ('449', 0.5863994325984988),\n",
       " ('166', 0.5863632651848523),\n",
       " ('406', 0.5863480911110387),\n",
       " ('134', 0.5863240716003238),\n",
       " ('153', 0.5862676180899143),\n",
       " ('466', 0.5862553270928789),\n",
       " ('403', 0.5862011093468892),\n",
       " ('402', 0.5861185931346632),\n",
       " ('472', 0.5861110826806416),\n",
       " ('328', 0.5861081259720253),\n",
       " ('246', 0.5860995122837641),\n",
       " ('121', 0.586092789736829),\n",
       " ('130', 0.5860882269839446),\n",
       " ('182', 0.5860499967404498),\n",
       " ('158', 0.58600547388196),\n",
       " ('252', 0.5859887808088273),\n",
       " ('109', 0.5859878030916056),\n",
       " ('181', 0.5858688344800376),\n",
       " ('427', 0.5857521993245266),\n",
       " ('218', 0.5857494534416633),\n",
       " ('223', 0.585698114425847),\n",
       " ('233', 0.5856776004939368),\n",
       " ('199', 0.5856403783569492),\n",
       " ('68', 0.5856228604912758),\n",
       " ('203', 0.585606181282338),\n",
       " ('154', 0.5856023038427035),\n",
       " ('258', 0.5855754496473254),\n",
       " ('377', 0.5855601407361752),\n",
       " ('173', 0.585545042423698),\n",
       " ('289', 0.5855328523450428),\n",
       " ('193', 0.5854804244467883),\n",
       " ('481', 0.585420054073135),\n",
       " ('483', 0.5853881549203036),\n",
       " ('331', 0.5853844668514046),\n",
       " ('497', 0.5853257530419401),\n",
       " ('381', 0.5853195264357929),\n",
       " ('299', 0.5853157128936561),\n",
       " ('339', 0.5852964104168),\n",
       " ('416', 0.5852918989898622),\n",
       " ('143', 0.5852910086166025),\n",
       " ('127', 0.5852727490285087),\n",
       " ('276', 0.5852689070757046),\n",
       " ('94', 0.5852663295964401),\n",
       " ('433', 0.5852081877316615),\n",
       " ('254', 0.5851992623824062),\n",
       " ('104', 0.5851990209212379),\n",
       " ('101', 0.5851555771943998),\n",
       " ('67', 0.5851477441688379),\n",
       " ('79', 0.5851314067840576),\n",
       " ('324', 0.5850463164231136),\n",
       " ('191', 0.5850407138466835),\n",
       " ('81', 0.5850148158768813),\n",
       " ('136', 0.5849976163534891),\n",
       " ('175', 0.5849780658880869),\n",
       " ('230', 0.584972167430922),\n",
       " ('151', 0.5849652231212646),\n",
       " ('368', 0.5848911941954585),\n",
       " ('317', 0.5848599714892252),\n",
       " ('195', 0.5848517018120464),\n",
       " ('394', 0.5848315344615416),\n",
       " ('417', 0.5848229480060664),\n",
       " ('326', 0.5848226523588574),\n",
       " ('415', 0.5847948250316438),\n",
       " ('82', 0.5847917162170706),\n",
       " ('91', 0.5847915316621464),\n",
       " ('102', 0.5847845128458791),\n",
       " ('240', 0.58476948691893),\n",
       " ('194', 0.5847562183936438),\n",
       " ('135', 0.5847371211347654),\n",
       " ('202', 0.5846316394706569),\n",
       " ('126', 0.5846306814087762),\n",
       " ('169', 0.584624942143758),\n",
       " ('308', 0.5846057410647231),\n",
       " ('285', 0.5845904786919439),\n",
       " ('145', 0.5845385697460914),\n",
       " ('75', 0.5845124855522038),\n",
       " ('386', 0.5844183440009753),\n",
       " ('491', 0.5843859881614194),\n",
       " ('268', 0.5843828688516761),\n",
       " ('103', 0.584355208447309),\n",
       " ('486', 0.5843482755479359),\n",
       " ('423', 0.5843457886876986),\n",
       " ('494', 0.5843140998991524),\n",
       " ('249', 0.5842967543953149),\n",
       " ('200', 0.5842866322823933),\n",
       " ('282', 0.5842689464489619),\n",
       " ('410', 0.5842608733232632),\n",
       " ('247', 0.5842517224096118),\n",
       " ('291', 0.5842478538668433),\n",
       " ('275', 0.5842258020332366),\n",
       " ('363', 0.5842072938301767),\n",
       " ('475', 0.584201043142992),\n",
       " ('354', 0.5841746578613918),\n",
       " ('150', 0.5841553451084509),\n",
       " ('83', 0.5841224240760009),\n",
       " ('273', 0.5840577963710755),\n",
       " ('278', 0.5840398664727355),\n",
       " ('474', 0.5840317509895149),\n",
       " ('162', 0.5840237530569236),\n",
       " ('290', 0.5839905091958452),\n",
       " ('351', 0.583947941660881),\n",
       " ('111', 0.5839297336836656),\n",
       " ('117', 0.5838948554462856),\n",
       " ('157', 0.5838914700155337),\n",
       " ('129', 0.5838762829458811),\n",
       " ('76', 0.5838714084883992),\n",
       " ('280', 0.5838640154827209),\n",
       " ('192', 0.5838504695310825),\n",
       " ('56', 0.5838425186773142),\n",
       " ('131', 0.5838239918152491),\n",
       " ('347', 0.5837777425845464),\n",
       " ('122', 0.5837200408180553),\n",
       " ('120', 0.5837096602488787),\n",
       " ('260', 0.5836855305565729),\n",
       " ('336', 0.583613628916668),\n",
       " ('443', 0.5835890821994297),\n",
       " ('455', 0.5835725304692291),\n",
       " ('392', 0.5835413682189855),\n",
       " ('266', 0.5835254610508911),\n",
       " ('208', 0.5835114932722516),\n",
       " ('70', 0.5834943173509655),\n",
       " ('198', 0.5834525710060484),\n",
       " ('174', 0.5834415855446483),\n",
       " ('244', 0.5834275146787481),\n",
       " ('358', 0.5834141934777165),\n",
       " ('188', 0.583390363889147),\n",
       " ('171', 0.5833722718060018),\n",
       " ('482', 0.5833574616631796),\n",
       " ('286', 0.5833453571512585),\n",
       " ('390', 0.5833374372485912),\n",
       " ('279', 0.5833355648111004),\n",
       " ('63', 0.5833038668322369),\n",
       " ('353', 0.5832991143970778),\n",
       " ('155', 0.5832932355503241),\n",
       " ('261', 0.5832597520924353),\n",
       " ('216', 0.5832537484891488),\n",
       " ('73', 0.5832331722217893),\n",
       " ('263', 0.5832278525287455),\n",
       " ('236', 0.5831661621729534),\n",
       " ('62', 0.5831344995072218),\n",
       " ('384', 0.583125288865363),\n",
       " ('293', 0.5831100118252657),\n",
       " ('179', 0.5830553136765957),\n",
       " ('227', 0.583014692678008),\n",
       " ('329', 0.5829895265174635),\n",
       " ('470', 0.5829631035978143),\n",
       " ('113', 0.5829614723116401),\n",
       " ('185', 0.5829549121359984),\n",
       " ('95', 0.5829028510733655),\n",
       " ('452', 0.5828977864447648),\n",
       " ('142', 0.5828160818607089),\n",
       " ('161', 0.5828134079774222),\n",
       " ('112', 0.5827840105590659),\n",
       " ('434', 0.5826799034169227),\n",
       " ('270', 0.5826617738965786),\n",
       " ('47', 0.5826320035117014),\n",
       " ('297', 0.582617084647334),\n",
       " ('146', 0.5826068751571714),\n",
       " ('307', 0.5826035065706386),\n",
       " ('64', 0.5825759700643337),\n",
       " ('224', 0.5825581391190374),\n",
       " ('453', 0.5825491683889729),\n",
       " ('167', 0.582481835673495),\n",
       " ('435', 0.58246187075521),\n",
       " ('337', 0.5824607291886973),\n",
       " ('164', 0.5824555551012357),\n",
       " ('366', 0.5824359882445562),\n",
       " ('163', 0.5824281643561232),\n",
       " ('177', 0.5824192055161037),\n",
       " ('205', 0.5823215888283119),\n",
       " ('139', 0.5823188266618465),\n",
       " ('419', 0.5823158480862315),\n",
       " ('96', 0.5822657779750661),\n",
       " ('206', 0.5822558131009814),\n",
       " ('421', 0.5822521773881689),\n",
       " ('445', 0.582236038717797),\n",
       " ('487', 0.5822321312214301),\n",
       " ('313', 0.5821824582048165),\n",
       " ('340', 0.582177722325889),\n",
       " ('360', 0.5821219687539388),\n",
       " ('359', 0.5820542038089097),\n",
       " ('318', 0.5820536583431007),\n",
       " ('221', 0.5820535400579142),\n",
       " ('44', 0.5820077206843939),\n",
       " ('300', 0.5819872511607228),\n",
       " ('186', 0.5819723227667429),\n",
       " ('341', 0.5819476199421016),\n",
       " ('388', 0.5819208379401717),\n",
       " ('80', 0.5819166224132212),\n",
       " ('213', 0.5819104973113897),\n",
       " ('234', 0.5818313923917077),\n",
       " ('17', 0.581800684040668),\n",
       " ('219', 0.5817921420400456),\n",
       " ('189', 0.5817709153260643),\n",
       " ('272', 0.5817563053249389),\n",
       " ('148', 0.581692247658737),\n",
       " ('105', 0.5816617454328233),\n",
       " ('302', 0.5816014006855044),\n",
       " ('294', 0.5815839793473955),\n",
       " ('141', 0.581510482280235),\n",
       " ('438', 0.5815063283750505),\n",
       " ('321', 0.5814818603950634),\n",
       " ('404', 0.5813942017063262),\n",
       " ('229', 0.5813847191277005),\n",
       " ('355', 0.5813461878534519),\n",
       " ('462', 0.5812935258528983),\n",
       " ('447', 0.5812865089076434),\n",
       " ('387', 0.5812849641749355),\n",
       " ('243', 0.5812548191258401),\n",
       " ('90', 0.5812443682190143),\n",
       " ('253', 0.581229719311692),\n",
       " ('485', 0.5811865239642389),\n",
       " ('61', 0.5811777352317563),\n",
       " ('393', 0.5811755922875662),\n",
       " ('133', 0.581153415261753),\n",
       " ('183', 0.5811183360533986),\n",
       " ('343', 0.581102717307306),\n",
       " ('469', 0.5810341182209197),\n",
       " ('478', 0.5810318693067088),\n",
       " ('215', 0.5809669144677393),\n",
       " ('306', 0.5809546853556777),\n",
       " ('325', 0.5809308719344255),\n",
       " ('239', 0.5808706198419844),\n",
       " ('128', 0.5808554008239654),\n",
       " ('376', 0.5808540906092917),\n",
       " ('292', 0.5808324725790458),\n",
       " ('277', 0.5807947059945455),\n",
       " ('176', 0.5807191011806329),\n",
       " ('274', 0.5806990140487277),\n",
       " ('370', 0.5806063159491665),\n",
       " ('184', 0.5806044664809374),\n",
       " ('479', 0.5805872911360206),\n",
       " ('269', 0.5805794727057219),\n",
       " ('178', 0.5804515279947766),\n",
       " ('379', 0.5803896253423174),\n",
       " ('125', 0.5802915166548598),\n",
       " ('217', 0.5802818834781646),\n",
       " ('86', 0.5802604650457699),\n",
       " ('365', 0.5801561980746514),\n",
       " ('92', 0.5800743790773245),\n",
       " ('65', 0.5800559225121166),\n",
       " ('172', 0.5800008404458691),\n",
       " ('123', 0.5799958375535271),\n",
       " ('137', 0.579990072382821),\n",
       " ('409', 0.5799330204956291),\n",
       " ('333', 0.5799220935864882),\n",
       " ('57', 0.5798866023388944),\n",
       " ('204', 0.5798154446652264),\n",
       " ('41', 0.5796569094464585),\n",
       " ('287', 0.579642034628812),\n",
       " ('401', 0.5795780185497169),\n",
       " ('444', 0.5795429922916271),\n",
       " ('107', 0.5793839453197106),\n",
       " ('54', 0.5793819620802595),\n",
       " ('367', 0.5793162061581535),\n",
       " ('338', 0.5791227563125331),\n",
       " ('160', 0.5791167840998397),\n",
       " ('310', 0.5790945328483287),\n",
       " ('116', 0.5790026236293663),\n",
       " ('431', 0.5789765326289444),\n",
       " ('495', 0.5788474570621144),\n",
       " ('484', 0.5788424943768701),\n",
       " ('93', 0.5788293137909875),\n",
       " ('330', 0.5786890161759926),\n",
       " ('436', 0.5786876408166663),\n",
       " ('85', 0.5786160064298054),\n",
       " ('31', 0.5785373838174912),\n",
       " ('301', 0.5785112882417346),\n",
       " ('305', 0.57845842422441),\n",
       " ('46', 0.578435971055712),\n",
       " ('34', 0.5783446187614113),\n",
       " ('72', 0.5782887366685),\n",
       " ('40', 0.5782263802515494),\n",
       " ('159', 0.5782169302304584),\n",
       " ('22', 0.5782095026679154),\n",
       " ('424', 0.5781920543222716),\n",
       " ('59', 0.5780362983544668),\n",
       " ('74', 0.5780280038714408),\n",
       " ('28', 0.5780192556835356),\n",
       " ('168', 0.5779613237070844),\n",
       " ('89', 0.5779113614462256),\n",
       " ('493', 0.5777269565222557),\n",
       " ('99', 0.5776945248246194),\n",
       " ('24', 0.5776235676474042),\n",
       " ('242', 0.5775193052761483),\n",
       " ('201', 0.5775190761418848),\n",
       " ('25', 0.5775024352802172),\n",
       " ('312', 0.5774619617159406),\n",
       " ('18', 0.5774557050731447),\n",
       " ('53', 0.5774479496712779),\n",
       " ('439', 0.5773834368979284),\n",
       " ('464', 0.5773686816294988),\n",
       " ('115', 0.5772284723818304),\n",
       " ('369', 0.5767612926661968),\n",
       " ('471', 0.5767409648837112),\n",
       " ('180', 0.5767074968756698),\n",
       " ('264', 0.5765676181467753),\n",
       " ('323', 0.576533625280954),\n",
       " ('108', 0.576429473265769),\n",
       " ('283', 0.5763018236617851),\n",
       " ('314', 0.5761976246580934),\n",
       " ('149', 0.5760026777547503),\n",
       " ('422', 0.5758932475433793),\n",
       " ('490', 0.5757659941680672),\n",
       " ('45', 0.5756803891088208),\n",
       " ('55', 0.5756165962498466),\n",
       " ('32', 0.5755840205785),\n",
       " ('320', 0.5755286139972281),\n",
       " ('23', 0.5753678706785043),\n",
       " ('457', 0.5753514137532977),\n",
       " ('48', 0.5752644330008417),\n",
       " ('170', 0.5751446519440754),\n",
       " ('418', 0.5751380848162102),\n",
       " ('467', 0.575135909929508),\n",
       " ('35', 0.5750858873449346),\n",
       " ('454', 0.5750853374363886),\n",
       " ('407', 0.5750340710776722),\n",
       " ('298', 0.5750266957012089),\n",
       " ('197', 0.574951396137476),\n",
       " ('334', 0.5749419749234662),\n",
       " ('255', 0.5749060005642647),\n",
       " ('425', 0.5748718236646955),\n",
       " ('429', 0.5747817115085881),\n",
       " ('106', 0.5747670302024255),\n",
       " ('412', 0.5747378642360369),\n",
       " ('295', 0.5746835054353227),\n",
       " ('190', 0.5746275380126432),\n",
       " ('378', 0.5745443819147168),\n",
       " ('468', 0.574527317701384),\n",
       " ('88', 0.5745001506237757),\n",
       " ('460', 0.5743519934740933),\n",
       " ('450', 0.574346864088015),\n",
       " ('399', 0.5741841575896092),\n",
       " ('8', 0.5740221876000602),\n",
       " ('327', 0.5739190777142844),\n",
       " ('9', 0.5738919136500118),\n",
       " ('187', 0.5736873847742876),\n",
       " ('196', 0.5736341043202965),\n",
       " ('400', 0.5735534586647685),\n",
       " ('19', 0.573524945643213),\n",
       " ('250', 0.5733798356218772),\n",
       " ('16', 0.5733317020038764),\n",
       " ('119', 0.5733176192693543),\n",
       " ('441', 0.5732633594482665),\n",
       " ('51', 0.5732187642937615),\n",
       " ('33', 0.5731982251008351),\n",
       " ('248', 0.5730672009693559),\n",
       " ('20', 0.5730000433101449),\n",
       " ('411', 0.572982828768473),\n",
       " ('38', 0.5728730398650264),\n",
       " ('382', 0.5727095667159917),\n",
       " ('7', 0.57267046697212),\n",
       " ('78', 0.5725532682642105),\n",
       " ('114', 0.5723170555704008),\n",
       " ('97', 0.5722771017324356),\n",
       " ('458', 0.5722441793907257),\n",
       " ('237', 0.572142104769862),\n",
       " ('27', 0.5720835088514814),\n",
       " ('437', 0.5719577651175242),\n",
       " ('21', 0.5718648466798995),\n",
       " ('60', 0.5715559652218453),\n",
       " ('165', 0.5715378321707248),\n",
       " ('395', 0.5715043348397396),\n",
       " ('251', 0.5714303862953942),\n",
       " ('344', 0.5714081528112869),\n",
       " ('459', 0.5712578901948854),\n",
       " ('348', 0.5711707880099615),\n",
       " ('465', 0.5710914044879204),\n",
       " ('426', 0.5709989576628715),\n",
       " ('265', 0.5704932533955388),\n",
       " ('256', 0.5704331874395862),\n",
       " ('259', 0.5703302651874779),\n",
       " ('332', 0.5700989236203275),\n",
       " ('349', 0.5700976525166238),\n",
       " ('5', 0.5698384813964368),\n",
       " ('446', 0.5696803094311195),\n",
       " ('26', 0.5696453336212371),\n",
       " ('140', 0.56960199615269),\n",
       " ('356', 0.5693410526412402),\n",
       " ('4', 0.5692182530959448),\n",
       " ('98', 0.5691954386730989),\n",
       " ('156', 0.569186388961668),\n",
       " ('58', 0.5690351814504654),\n",
       " ('11', 0.5690080887741512),\n",
       " ('373', 0.5689898093068412),\n",
       " ('0', 0.568988763718378),\n",
       " ('489', 0.5689555181870385),\n",
       " ('389', 0.5688421107077782),\n",
       " ('37', 0.5688314362607821),\n",
       " ('288', 0.5686685404542721),\n",
       " ('315', 0.5686353698136315),\n",
       " ('281', 0.5686350629636735),\n",
       " ('316', 0.5685993079529252),\n",
       " ('432', 0.5684977999282262),\n",
       " ('372', 0.5684554540529484),\n",
       " ('84', 0.5684415305417682),\n",
       " ('362', 0.5684170449167734),\n",
       " ('226', 0.5684153281829575),\n",
       " ('29', 0.5679822601023174),\n",
       " ('440', 0.5679785193818986),\n",
       " ('271', 0.5679747313261031),\n",
       " ('364', 0.567905885703636),\n",
       " ('6', 0.5677199625607693),\n",
       " ('36', 0.5675754661982259),\n",
       " ('311', 0.5674640655969129),\n",
       " ('357', 0.5674292363372504),\n",
       " ('303', 0.5673374949939668),\n",
       " ('1', 0.5672990702172761),\n",
       " ('391', 0.5671465614485363),\n",
       " ('87', 0.5670696960555183),\n",
       " ('342', 0.5670185548852581),\n",
       " ('3', 0.5667338150863847),\n",
       " ('110', 0.5666325420141219),\n",
       " ('499', 0.5664604373953559),\n",
       " ('345', 0.5661584807605277),\n",
       " ('245', 0.5660190704719041),\n",
       " ('451', 0.5657571822856413),\n",
       " ('13', 0.5655035739927962),\n",
       " ('2', 0.5654644998265249),\n",
       " ('257', 0.5654173864731713),\n",
       " ('15', 0.5652665581416202),\n",
       " ('304', 0.5652236717216895),\n",
       " ('309', 0.5650206551192299),\n",
       " ('473', 0.5649176549816889),\n",
       " ('235', 0.564654322961966),\n",
       " ('30', 0.5645954000134754),\n",
       " ('413', 0.5642987300479223),\n",
       " ('477', 0.5640591107534639),\n",
       " ('14', 0.5640074306478103),\n",
       " ('346', 0.5639882727648861),\n",
       " ('480', 0.5638221042851607),\n",
       " ('405', 0.5632736031399217),\n",
       " ('476', 0.5632287506447282),\n",
       " ('319', 0.5627374922235806),\n",
       " ('448', 0.5623030593228896),\n",
       " ('375', 0.5621336700789857),\n",
       " ('397', 0.5621281851221013),\n",
       " ('383', 0.5616585817752462),\n",
       " ('42', 0.5611081510259394),\n",
       " ('220', 0.5605898677378661),\n",
       " ('12', 0.5601069833102978),\n",
       " ('414', 0.5594390875609346),\n",
       " ('498', 0.5582626788389117),\n",
       " ('371', 0.55807173344516),\n",
       " ('488', 0.5576685926679409),\n",
       " ('396', 0.555861972046621),\n",
       " ('262', 0.5536658689957258),\n",
       " ('77', 0.5517615592756937),\n",
       " ('284', 0.5503446406634279),\n",
       " ('147', 0.5489174877152297),\n",
       " ('52', 0.5479310909907023),\n",
       " ('207', 0.5473922159129042),\n",
       " ('118', 0.5460398672592072),\n",
       " ('492', 0.5420853708141533),\n",
       " ('408', 0.5366874481692459),\n",
       " ('43', 0.5308879807188704),\n",
       " ('428', 0.5280936279940228),\n",
       " ('228', 0.5221263251101323),\n",
       " ('296', 0.5118313646978802),\n",
       " ('398', 0.4984024340330168),\n",
       " ('50', 0.4953774933261101),\n",
       " ('463', 0.4898047008255655),\n",
       " ('361', 0.4819126581960869),\n",
       " ('456', 0.47992114067980746),\n",
       " ('461', 0.4649219025265088),\n",
       " ('10', 0.4615311857901121),\n",
       " ('49', 0.457112445842986),\n",
       " ('420', 0.4528369097009537),\n",
       " ('352', 0.44693727618040047),\n",
       " ('350', 0.37201841581951484),\n",
       " ('225', 0.3527647890150547),\n",
       " ('430', 0.24246751286031665),\n",
       " ('496', 0.19855030980848132),\n",
       " ('39', 0.001032212923746556)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "score = {}\n",
    "params = {}\n",
    "for item in Path(\"models\").iterdir():\n",
    "    if not item.suffix == \".json\" : continue\n",
    "    \n",
    "    id = item.stem.split('_')[2]\n",
    "    score[id] = json.load(open(item, \"r\"))[\"score\"]\n",
    "    params[id] = json.load(open(item, \"r\"))[\"params\"]\n",
    "\n",
    "qtd = 15\n",
    "ll = sorted(score.items(), key=lambda x : x[1], reverse=True)\n",
    "best_ids, _ = zip(*ll[:qtd])\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:00<00:00, 26.69it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:00<00:00, 40.25it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:01<00:00, 15.92it/s]\n",
      "Batches: 100%|██████████| 29/29 [00:01<00:00, 16.31it/s]\n",
      "Batches: 100%|██████████| 14/14 [00:00<00:00, 16.10it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:01<00:00, 17.10it/s]\n",
      "Batches: 100%|██████████| 27/27 [00:01<00:00, 25.22it/s]\n",
      "Batches: 100%|██████████| 34/34 [00:00<00:00, 39.07it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 32.14it/s]\n",
      "Batches: 100%|██████████| 20/20 [00:00<00:00, 29.49it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:00<00:00, 30.86it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:01<00:00, 15.18it/s]\n",
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 24.55it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:01<00:00, 14.08it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:01<00:00, 15.50it/s]\n",
      "Batches: 100%|██████████| 35/35 [00:04<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:01<00:00,  6.65it/s]\n",
      "Batches: 100%|██████████| 19/19 [00:02<00:00,  6.40it/s]\n",
      "Batches: 100%|██████████| 14/14 [00:02<00:00,  6.88it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:02<00:00,  7.19it/s]\n",
      "Batches: 100%|██████████| 10/10 [00:01<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 20/20 [00:00<00:00, 28.75it/s]\n",
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 38.94it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:00<00:00, 40.29it/s]\n",
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 17.31it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:01<00:00, 18.65it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:01<00:00, 17.31it/s]\n",
      "Batches: 100%|██████████| 32/32 [00:01<00:00, 17.00it/s]\n",
      "Batches: 100%|██████████| 20/20 [00:01<00:00, 18.67it/s]\n",
      "Batches: 100%|██████████| 38/38 [00:02<00:00, 16.66it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:02<00:00, 19.28it/s]\n",
      "Batches: 100%|██████████| 37/37 [00:01<00:00, 30.53it/s]\n",
      "Batches: 100%|██████████| 20/20 [00:00<00:00, 35.30it/s]\n",
      "Batches: 100%|██████████| 37/37 [00:01<00:00, 28.21it/s]\n",
      "Batches: 100%|██████████| 40/40 [00:01<00:00, 37.66it/s]\n",
      "Batches: 100%|██████████| 52/52 [00:01<00:00, 34.21it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:00<00:00, 30.61it/s]\n",
      "Batches: 100%|██████████| 30/30 [00:00<00:00, 32.14it/s]\n",
      "Batches: 100%|██████████| 30/30 [00:00<00:00, 32.13it/s]\n",
      "Batches: 100%|██████████| 46/46 [00:01<00:00, 35.49it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sentences, test_speakers, _  = custom_utils.read_data_by_ID(\"test\", combine = False)\n",
    "\n",
    "def format_input(sentences, speakers):\n",
    "    switcher = {\n",
    "        \"PM\" : [1,0,0,0],\n",
    "        \"ME\" : [0,1,0,0],\n",
    "        \"UI\" : [0,0,1,0],\n",
    "        \"ID\" : [0,0,0,1]\n",
    "    }\n",
    "\n",
    "    train_sentences = bert.encode(sentences, convert_to_tensor=True, show_progress_bar=True).to(device)\n",
    "    train_speaker = torch.Tensor([switcher[el] for el in speakers]).to(device)\n",
    "    train_X = torch.cat((train_sentences, train_speaker), dim=1)\n",
    "    return train_X\n",
    "\n",
    "data = {}\n",
    "for id in test_sentences:\n",
    "    data[id] = format_input(test_sentences[id], test_speakers[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100 | Train Loss: 0.40085580945014954 | Val Loss: 0.4385976016521454 | F1-score: 0.0\n",
      "Iter: 200 | Train Loss: 0.35230720043182373 | Val Loss: 0.37750003238519036 | F1-score: 0.0\n",
      "Iter: 300 | Train Loss: 0.33393925428390503 | Val Loss: 0.359857436021169 | F1-score: 0.0\n",
      "Iter: 400 | Train Loss: 0.3215361535549164 | Val Loss: 0.36413959562778475 | F1-score: 0.0\n",
      "Iter: 500 | Train Loss: 0.35456258058547974 | Val Loss: 0.35692150791486116 | F1-score: 0.5802803734938304\n",
      "Iter: 600 | Train Loss: 0.3904114067554474 | Val Loss: 0.35572748879591637 | F1-score: 0.5726165334383646\n",
      "Iter: 700 | Train Loss: 0.34016886353492737 | Val Loss: 0.3527257412672043 | F1-score: 0.5755985647439956\n",
      "Iter: 800 | Train Loss: 0.3549371361732483 | Val Loss: 0.3567900826533635 | F1-score: 0.5829618016878764\n",
      "Iter: 900 | Train Loss: 0.34820765256881714 | Val Loss: 0.3471076180537541 | F1-score: 0.5693604071935017\n",
      "Iter: 1000 | Train Loss: 0.31409886479377747 | Val Loss: 0.33952231307824443 | F1-score: 0.5714761058489484\n",
      "Iter: 1100 | Train Loss: 0.3212335705757141 | Val Loss: 0.3403658191363017 | F1-score: 0.5867872208356858\n",
      "Iter: 1200 | Train Loss: 0.3449918031692505 | Val Loss: 0.34849741756916036 | F1-score: 0.5868907173474629\n",
      "Iter: 1300 | Train Loss: 0.3371809422969818 | Val Loss: 0.3506843795379002 | F1-score: 0.5777539084355037\n",
      "Iter: 1400 | Train Loss: 0.36212801933288574 | Val Loss: 0.3478262374798457 | F1-score: 0.5847141077121099\n",
      "Iter: 1500 | Train Loss: 0.36767756938934326 | Val Loss: 0.34223884443442026 | F1-score: 0.5771360476811727\n",
      "Iter: 1600 | Train Loss: 0.3575615882873535 | Val Loss: 0.33623809019724527 | F1-score: 0.580199788014094\n",
      "Iter: 1700 | Train Loss: 0.3828543722629547 | Val Loss: 0.3426348278919856 | F1-score: 0.5834526757399241\n",
      "Iter: 1800 | Train Loss: 0.3286614716053009 | Val Loss: 0.3446433246135712 | F1-score: 0.5761699507633845\n",
      "Iter: 1900 | Train Loss: 0.3025694191455841 | Val Loss: 0.3396170526742936 | F1-score: 0.5865455051263173\n",
      "Iter: 2000 | Train Loss: 0.32237952947616577 | Val Loss: 0.34307619233926145 | F1-score: 0.5817565162976582\n",
      "Iter: 2100 | Train Loss: 0.27120673656463623 | Val Loss: 0.3314676677187284 | F1-score: 0.5838045179843903\n",
      "Iter: 2200 | Train Loss: 0.2813897728919983 | Val Loss: 0.33558133343855534 | F1-score: 0.5852584540843964\n",
      "Iter: 2300 | Train Loss: 0.3016711473464966 | Val Loss: 0.34039555688699086 | F1-score: 0.5908853789170584\n",
      "Iter: 100 | Train Loss: 0.422115683555603 | Val Loss: 0.41878603498140965 | F1-score: 0.0\n",
      "Iter: 200 | Train Loss: 0.36889928579330444 | Val Loss: 0.35819056232770285 | F1-score: 0.0\n",
      "Iter: 300 | Train Loss: 0.3387375473976135 | Val Loss: 0.34768758118152615 | F1-score: 0.0\n",
      "Iter: 400 | Train Loss: 0.33087530732154846 | Val Loss: 0.3518872161706288 | F1-score: 0.0\n",
      "Iter: 500 | Train Loss: 0.36089983582496643 | Val Loss: 0.34642881254355107 | F1-score: 0.0\n",
      "Iter: 600 | Train Loss: 0.3885912299156189 | Val Loss: 0.3448659578959148 | F1-score: 0.38656065066655476\n",
      "Iter: 700 | Train Loss: 0.35696619749069214 | Val Loss: 0.3438112517197927 | F1-score: 0.5380740920702616\n",
      "Iter: 800 | Train Loss: 0.3515467941761017 | Val Loss: 0.346014454960823 | F1-score: 0.5657936205466587\n",
      "Iter: 900 | Train Loss: 0.35436826944351196 | Val Loss: 0.33890612026055655 | F1-score: 0.5595921605825426\n",
      "Iter: 1000 | Train Loss: 0.3305547833442688 | Val Loss: 0.3335094998280207 | F1-score: 0.5602123359839122\n",
      "Iter: 1100 | Train Loss: 0.34175747632980347 | Val Loss: 0.3327327857414881 | F1-score: 0.5740515321493148\n",
      "Iter: 1200 | Train Loss: 0.33693158626556396 | Val Loss: 0.33544950485229486 | F1-score: 0.5736354927221934\n",
      "Iter: 1300 | Train Loss: 0.34978747367858887 | Val Loss: 0.3394515713055929 | F1-score: 0.5685019791126252\n",
      "Iter: 1400 | Train Loss: 0.3869289755821228 | Val Loss: 0.3318748315175374 | F1-score: 0.5747535149256388\n",
      "Iter: 1500 | Train Loss: 0.35415154695510864 | Val Loss: 0.3290549904108047 | F1-score: 0.5662224332491559\n",
      "Iter: 1600 | Train Loss: 0.33633190393447876 | Val Loss: 0.3269226143757503 | F1-score: 0.5682115554809568\n",
      "Iter: 1700 | Train Loss: 0.4064791798591614 | Val Loss: 0.3325591295957565 | F1-score: 0.5756214638551077\n",
      "Iter: 1800 | Train Loss: 0.34508150815963745 | Val Loss: 0.33096739153067267 | F1-score: 0.5636117468277614\n",
      "Iter: 1900 | Train Loss: 0.29802021384239197 | Val Loss: 0.3249271223942439 | F1-score: 0.5685691098372141\n",
      "Iter: 2000 | Train Loss: 0.34286218881607056 | Val Loss: 0.32852851351102186 | F1-score: 0.5568395465612412\n",
      "Iter: 2100 | Train Loss: 0.2839365601539612 | Val Loss: 0.32322202920913695 | F1-score: 0.5675323843955994\n",
      "Iter: 2200 | Train Loss: 0.26736921072006226 | Val Loss: 0.3263426433006923 | F1-score: 0.5678243766228358\n",
      "Iter: 2300 | Train Loss: 0.32857877016067505 | Val Loss: 0.3273162911335628 | F1-score: 0.5790035784244537\n",
      "Iter: 100 | Train Loss: 0.418183833360672 | Val Loss: 0.43181804021199544 | F1-score: 0.0\n",
      "Iter: 200 | Train Loss: 0.35301142930984497 | Val Loss: 0.3629094630479813 | F1-score: 0.0\n",
      "Iter: 300 | Train Loss: 0.3286047875881195 | Val Loss: 0.35124311844507855 | F1-score: 0.0\n",
      "Iter: 400 | Train Loss: 0.32956013083457947 | Val Loss: 0.3517258942127227 | F1-score: 0.037603448455532394\n",
      "Iter: 500 | Train Loss: 0.36700743436813354 | Val Loss: 0.345959065357844 | F1-score: 0.5473189850648246\n",
      "Iter: 600 | Train Loss: 0.39274847507476807 | Val Loss: 0.3470917344093323 | F1-score: 0.5541154474020004\n",
      "Iter: 700 | Train Loss: 0.3431330621242523 | Val Loss: 0.3464626838763556 | F1-score: 0.5651362379391988\n",
      "Iter: 800 | Train Loss: 0.36530929803848267 | Val Loss: 0.34447468121846514 | F1-score: 0.5720754603544872\n",
      "Iter: 900 | Train Loss: 0.3224146366119385 | Val Loss: 0.33847777148087826 | F1-score: 0.5610705067714056\n",
      "Iter: 1000 | Train Loss: 0.3310249447822571 | Val Loss: 0.3377348820368449 | F1-score: 0.5695868492126466\n",
      "Iter: 1100 | Train Loss: 0.3188345730304718 | Val Loss: 0.3346852968136469 | F1-score: 0.5746977577606837\n",
      "Iter: 1200 | Train Loss: 0.3401179909706116 | Val Loss: 0.3396947930256525 | F1-score: 0.581496302286784\n",
      "Iter: 1300 | Train Loss: 0.3325923979282379 | Val Loss: 0.34244458178679155 | F1-score: 0.574028429389\n",
      "Iter: 1400 | Train Loss: 0.36157530546188354 | Val Loss: 0.3380968670050303 | F1-score: 0.5841980278491974\n",
      "Iter: 1500 | Train Loss: 0.36935338377952576 | Val Loss: 0.3352880915006001 | F1-score: 0.5784418513377508\n",
      "Iter: 1600 | Train Loss: 0.3472999334335327 | Val Loss: 0.33377381066481276 | F1-score: 0.5784341047207516\n",
      "Iter: 1700 | Train Loss: 0.35954561829566956 | Val Loss: 0.33772940436998994 | F1-score: 0.5827321877082189\n",
      "Iter: 1800 | Train Loss: 0.3373847007751465 | Val Loss: 0.34033912022908525 | F1-score: 0.5758060425519945\n",
      "Iter: 1900 | Train Loss: 0.3060859739780426 | Val Loss: 0.3329283306996028 | F1-score: 0.5878235479195912\n",
      "Iter: 2000 | Train Loss: 0.32876065373420715 | Val Loss: 0.33977731962998714 | F1-score: 0.5760975072781244\n",
      "Iter: 2100 | Train Loss: 0.27573636174201965 | Val Loss: 0.3303838744759559 | F1-score: 0.5884250551462173\n",
      "Iter: 2200 | Train Loss: 0.29931700229644775 | Val Loss: 0.3344470451275508 | F1-score: 0.5899463693300883\n",
      "Iter: 2300 | Train Loss: 0.3073563277721405 | Val Loss: 0.3368334114551544 | F1-score: 0.5915792346000671\n",
      "Iter: 100 | Train Loss: 0.391032874584198 | Val Loss: 0.4002581189076106 | F1-score: 0.0\n",
      "Iter: 200 | Train Loss: 0.3462548553943634 | Val Loss: 0.3585042973359426 | F1-score: 0.0\n",
      "Iter: 300 | Train Loss: 0.33818984031677246 | Val Loss: 0.34925694068272906 | F1-score: 0.0\n",
      "Iter: 400 | Train Loss: 0.3241243362426758 | Val Loss: 0.35355005264282235 | F1-score: 0.0\n",
      "Iter: 500 | Train Loss: 0.36309725046157837 | Val Loss: 0.3435191720724107 | F1-score: 0.0\n",
      "Iter: 600 | Train Loss: 0.40104052424430847 | Val Loss: 0.3431232343117396 | F1-score: 0.521202634771665\n",
      "Iter: 700 | Train Loss: 0.34204864501953125 | Val Loss: 0.3411813388268153 | F1-score: 0.5632973869641622\n",
      "Iter: 800 | Train Loss: 0.3591567575931549 | Val Loss: 0.3426210870345433 | F1-score: 0.5778521279493968\n",
      "Iter: 900 | Train Loss: 0.3348076343536377 | Val Loss: 0.33690485060215003 | F1-score: 0.5678607294956844\n",
      "Iter: 1000 | Train Loss: 0.3379248082637787 | Val Loss: 0.3337443103392919 | F1-score: 0.5692166686058046\n",
      "Iter: 1100 | Train Loss: 0.3242987394332886 | Val Loss: 0.3301515152057012 | F1-score: 0.5744676689306895\n",
      "Iter: 1200 | Train Loss: 0.3417125642299652 | Val Loss: 0.3361984441677729 | F1-score: 0.5818958699703217\n",
      "Iter: 1300 | Train Loss: 0.33502131700515747 | Val Loss: 0.3359517008066178 | F1-score: 0.5617742985486985\n",
      "Iter: 1400 | Train Loss: 0.35253453254699707 | Val Loss: 0.3328109125296274 | F1-score: 0.5735059738159178\n",
      "Iter: 1500 | Train Loss: 0.3773624897003174 | Val Loss: 0.32976472377777105 | F1-score: 0.5669122099876404\n",
      "Iter: 1600 | Train Loss: 0.3360241651535034 | Val Loss: 0.3293101529280345 | F1-score: 0.5741025060415268\n",
      "Iter: 1700 | Train Loss: 0.3852115571498871 | Val Loss: 0.3342297712961833 | F1-score: 0.579824189345042\n",
      "Iter: 1800 | Train Loss: 0.3379081189632416 | Val Loss: 0.33468084633350365 | F1-score: 0.572157523036003\n",
      "Iter: 1900 | Train Loss: 0.2970011234283447 | Val Loss: 0.32873589297135675 | F1-score: 0.5753875354925793\n",
      "Iter: 2000 | Train Loss: 0.3212232291698456 | Val Loss: 0.3325475047032039 | F1-score: 0.5690847704807916\n",
      "Iter: 2100 | Train Loss: 0.2828013598918915 | Val Loss: 0.32822488248348236 | F1-score: 0.5808173467715582\n",
      "Iter: 2200 | Train Loss: 0.27229344844818115 | Val Loss: 0.32754864891370133 | F1-score: 0.5779734055201213\n",
      "Iter: 2300 | Train Loss: 0.30769670009613037 | Val Loss: 0.32980430324872334 | F1-score: 0.5787184288104376\n",
      "Iter: 100 | Train Loss: 0.3905602991580963 | Val Loss: 0.3954695105552674 | F1-score: 0.0\n",
      "Iter: 200 | Train Loss: 0.3531515896320343 | Val Loss: 0.35908649861812597 | F1-score: 0.0\n",
      "Iter: 300 | Train Loss: 0.3349247872829437 | Val Loss: 0.34665326277414954 | F1-score: 0.0\n",
      "Iter: 400 | Train Loss: 0.31169816851615906 | Val Loss: 0.35173737605412797 | F1-score: 0.0\n",
      "Iter: 500 | Train Loss: 0.36820530891418457 | Val Loss: 0.34092381298542024 | F1-score: 0.4971500804026921\n",
      "Iter: 600 | Train Loss: 0.3985554277896881 | Val Loss: 0.35052803357442214 | F1-score: 0.5691431850194931\n",
      "Iter: 700 | Train Loss: 0.3579706847667694 | Val Loss: 0.3412131557861964 | F1-score: 0.567012036840121\n",
      "Iter: 800 | Train Loss: 0.3572370409965515 | Val Loss: 0.3429527183373769 | F1-score: 0.5783454736073811\n",
      "Iter: 900 | Train Loss: 0.3317970335483551 | Val Loss: 0.3360009213288625 | F1-score: 0.5645076940457028\n",
      "Iter: 1000 | Train Loss: 0.31540602445602417 | Val Loss: 0.33328046600023903 | F1-score: 0.5686570783456167\n",
      "Iter: 1100 | Train Loss: 0.32685840129852295 | Val Loss: 0.33082535366217297 | F1-score: 0.5789897869030636\n",
      "Iter: 1200 | Train Loss: 0.3316563069820404 | Val Loss: 0.33755069971084595 | F1-score: 0.5855986952781678\n",
      "Iter: 1300 | Train Loss: 0.3298732042312622 | Val Loss: 0.33453217744827274 | F1-score: 0.5670652925968169\n",
      "Iter: 1400 | Train Loss: 0.3403303623199463 | Val Loss: 0.33183067341645556 | F1-score: 0.5777840157349905\n",
      "Iter: 1500 | Train Loss: 0.3491937816143036 | Val Loss: 0.3320582906405131 | F1-score: 0.579493738214175\n",
      "Iter: 1600 | Train Loss: 0.33250197768211365 | Val Loss: 0.3293955326080322 | F1-score: 0.5827548891305924\n",
      "Iter: 1700 | Train Loss: 0.3629675507545471 | Val Loss: 0.3336873839298884 | F1-score: 0.5832727164030074\n",
      "Iter: 1800 | Train Loss: 0.3320775032043457 | Val Loss: 0.33269639611244206 | F1-score: 0.5748993863662083\n",
      "Iter: 1900 | Train Loss: 0.286298543214798 | Val Loss: 0.32638142208258314 | F1-score: 0.5763644317785899\n",
      "Iter: 2000 | Train Loss: 0.32354995608329773 | Val Loss: 0.3304659803708395 | F1-score: 0.5691654215256373\n",
      "Iter: 2100 | Train Loss: 0.2755845785140991 | Val Loss: 0.3262804225087166 | F1-score: 0.5785479128360749\n",
      "Iter: 2200 | Train Loss: 0.2766781747341156 | Val Loss: 0.3265809267759323 | F1-score: 0.5742219160000481\n",
      "Iter: 2300 | Train Loss: 0.30125895142555237 | Val Loss: 0.329941933353742 | F1-score: 0.5896229426066082\n"
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "for bid in best_ids:\n",
    "    model = MLP(params[bid])\n",
    "    state_dict = train_MLP(params[bid])\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    model.eval()\n",
    "    test_labels = {}\n",
    "    for id in test_sentences.keys():\n",
    "        out = model(data[id])\n",
    "        pred = out.argmax(dim=1)\n",
    "        test_labels[id] = pred.cpu().detach().numpy()\n",
    "    all_models.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = {}\n",
    "\n",
    "n = len(all_models)\n",
    "for model_labels in all_models:\n",
    "    for id in model_labels:\n",
    "        if not id in test_labels:\n",
    "            test_labels[id] = model_labels[id] / n\n",
    "        else:\n",
    "            test_labels[id] += model_labels[id] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in test_labels:\n",
    "    test_labels[id] = [int(el) for el in list(np.where(test_labels[id] >= 0.5, 1, 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(test_labels, open(\"test_labels.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
