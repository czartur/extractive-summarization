{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import src.custom_utils as custom_utils\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labels\n",
    "with open(\"training_labels.json\", \"r\") as json_file:\n",
    "    labels = json.load(json_file)\n",
    "\n",
    "# read nodes and edges\n",
    "dialogs, speakers, edges = custom_utils.read_data_by_ID(\"../training\", combine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data from all dialogs\n",
    "X_train_dialog, y_train, X_train_speaker, train_edge_idx, train_edge_attr = [], [], [], [], []\n",
    "X_test_dialog, y_test, X_test_speaker, test_edge_idx, test_edge_attr = [], [], [], [], []\n",
    "\n",
    "count_train, count_test = 0, 0\n",
    "for id in dialogs.keys():\n",
    "        if False: # dumb --> dialogs starting with T for test\n",
    "                X_test_dialog += dialogs[id]\n",
    "                X_test_speaker += speakers[id]\n",
    "                y_test += labels[id]\n",
    "                test_edge_idx += [[e[0] + count_test, e[2] + count_test] for e in edges[id]]\n",
    "                test_edge_attr += [e[1] for e in edges[id]]\n",
    "                count_test += len(labels[id]) \n",
    "        else:\n",
    "                X_train_dialog += dialogs[id]\n",
    "                X_train_speaker += speakers[id]\n",
    "                y_train += labels[id]\n",
    "                train_edge_idx += [[e[0] + count_train, e[2] + count_train] for e in edges[id]]\n",
    "                train_edge_attr += [e[1] for e in edges[id]]\n",
    "                count_train += len(labels[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot encode speakers name\n",
    "switcher = {\n",
    "        \"PM\" : [1,0,0,0],\n",
    "        \"ME\" : [0,1,0,0],\n",
    "        \"UI\" : [0,0,1,0],\n",
    "        \"ID\" : [0,0,0,1]\n",
    "}\n",
    "\n",
    "# ordinal encoder (eda)\n",
    "# switcher = {\n",
    "#         \"PM\" : [1],\n",
    "#         \"ME\" : [0],\n",
    "#         \"UI\" : [0],\n",
    "#         \"ID\" : [0]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2270/2270 [03:54<00:00,  9.68it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# edge_idx = torch.Tensor(train_edge_idx).long().transpose(0,1)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# edge_attr = bert.encode(train_edge_attr, show_progress_bar=True, convert_to_tensor=False)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[39m# test formatting\u001b[39;00m\n\u001b[1;32m      9\u001b[0m X_test_speaker \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([switcher[el] \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m X_test_speaker])\n\u001b[0;32m---> 10\u001b[0m X_test_dialog \u001b[39m=\u001b[39m bert\u001b[39m.\u001b[39;49mencode(X_test_dialog, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mto(X_test_speaker\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m X_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((X_test_dialog, X_test_speaker), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     12\u001b[0m \u001b[39m# test_edge_idx = torch.Tensor(test_edge_idx).long().transpose(0,1)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# test_edge_attr = bert.encode(test_edge_attr, show_progress_bar=True, convert_to_tensor=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projects/extractive-summarization/env/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:195\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    192\u001b[0m all_embeddings \u001b[39m=\u001b[39m [all_embeddings[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39margsort(length_sorted_idx)]\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m convert_to_tensor:\n\u001b[0;32m--> 195\u001b[0m     all_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(all_embeddings)\n\u001b[1;32m    196\u001b[0m \u001b[39melif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m    197\u001b[0m     all_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([emb\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m all_embeddings])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# train formatting\n",
    "X_speaker = torch.Tensor([switcher[el] for el in X_train_speaker])\n",
    "X_dialog = bert.encode(X_train_dialog, show_progress_bar=True, convert_to_tensor=True).to(X_speaker.device)\n",
    "X_train = torch.cat((X_dialog, X_speaker), dim=1).numpy().tolist()\n",
    "# edge_idx = torch.Tensor(train_edge_idx).long().transpose(0,1)\n",
    "# edge_attr = bert.encode(train_edge_attr, show_progress_bar=True, convert_to_tensor=False)\n",
    "\n",
    "# test formatting\n",
    "X_test_speaker = torch.Tensor([switcher[el] for el in X_test_speaker])\n",
    "X_test_dialog = bert.encode(X_test_dialog, show_progress_bar=True, convert_to_tensor=True).to(X_test_speaker.device)\n",
    "X_test = torch.cat((X_test_dialog, X_test_speaker), dim = 1).numpy().tolist()\n",
    "# test_edge_idx = torch.Tensor(test_edge_idx).long().transpose(0,1)\n",
    "# test_edge_attr = bert.encode(test_edge_attr, show_progress_bar=True, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating json files\n",
    "with open('../data/combine_false_full/X_train.json', 'w') as json_file:\n",
    "    json.dump(X_train, json_file)\n",
    "with open('../data/combine_false_full/y_train.json', 'w') as json_file:\n",
    "    json.dump(y_train, json_file)\n",
    "\n",
    "# with open('data/X_test.json', 'w') as json_file:\n",
    "#     json.dump(X_test, json_file)\n",
    "# with open('data/y_test.json', 'w') as json_file:\n",
    "#     json.dump(y_test, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
