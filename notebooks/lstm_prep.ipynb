{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import custom_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM using pre-trained embedding\n",
    "\n",
    "#### + hot-encoder for speaker on the last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "model_path = \"tokenizer.model\"\n",
    "model_name = \"glove-wiki-gigaword-300\"\n",
    "\n",
    "# load model (and save if necessary)\n",
    "try:\n",
    "    tokenizer = KeyedVectors.load(model_path)\n",
    "except FileNotFoundError:\n",
    "    tokenizer = api.load(model_name)\n",
    "    tokenizer.save(model_path)\n",
    "\n",
    "def embed_sentence(sentence, tokenizer):\n",
    "    embeddings = []\n",
    "    length = 0\n",
    "    for word in sentence.split():\n",
    "        word = word.lower()\n",
    "        if not word in tokenizer: continue\n",
    "        length += 1\n",
    "        embeddings.append(tokenizer[word])\n",
    "\n",
    "    if len(embeddings) == 0: # bug fix for padding function (we need to asssure at least one element)\n",
    "        embeddings = np.zeros(shape=(1,tokenizer.vector_size))\n",
    "        length = 1\n",
    "    return torch.tensor(np.asarray(embeddings)), length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(\"cuda\") \n",
    "\n",
    "# hot encode speakers\n",
    "switcher = {\n",
    "    \"PM\" : [1,0,0,0],\n",
    "    \"ME\" : [0,1,0,0],\n",
    "    \"UI\" : [0,0,1,0],\n",
    "    \"ID\" : [0,0,0,1]\n",
    "}\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size + len(switcher), output_size)\n",
    "\n",
    "    def forward(self, sentences, speakers):\n",
    "        # from sentences to sequences of vectors (embedding)\n",
    "        embedded_sentences, lengths = list(zip(*[embed_sentence(sentence, tokenizer) for sentence in sentences]))\n",
    "\n",
    "        # pack / pad sequences (save memory) \n",
    "        packed_sentences = pack_padded_sequence(pad_sequence(embedded_sentences, batch_first=True), lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # hot encode speakers\n",
    "        encoded_speakers = torch.tensor([switcher[speaker] for speaker in speakers])\n",
    "\n",
    "        # send to device \n",
    "        packed_sentences.to(device)\n",
    "        encoded_speakers.to(device)\n",
    "        \n",
    "        # lstm layer\n",
    "        _, (ht,_) = self.lstm(packed_sentences) # it does accept packed sequences \n",
    "\n",
    "        # linear f.c. layer \n",
    "        output = self.fc(torch.cat([ht[-1], encoded_speakers], dim=1)) # use only output from the last hidden state\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + embedding from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 25417\n",
      "Test: 36312\n",
      "Valid: 10894\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>speakers</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10418</th>\n",
       "      <td>Uh . &lt;vocalsound&gt; I mean to certain cues .</td>\n",
       "      <td>UI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21576</th>\n",
       "      <td>another thought I &lt;disfmarker&gt;</td>\n",
       "      <td>UI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65989</th>\n",
       "      <td>Mm , yeah .</td>\n",
       "      <td>UI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>maybe like yellow and white &lt;disfmarker&gt;</td>\n",
       "      <td>PM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41397</th>\n",
       "      <td>Okay . &lt;vocalsound&gt; Okay .</td>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sentences speakers  labels\n",
       "10418  Uh . <vocalsound> I mean to certain cues .       UI       0\n",
       "21576              another thought I <disfmarker>       UI       0\n",
       "65989                                 Mm , yeah .       UI       0\n",
       "1872     maybe like yellow and white <disfmarker>       PM       0\n",
       "41397                  Okay . <vocalsound> Okay .       ID       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read data\n",
    "sentences, speakers, labels = custom_utils.read_data(\"training\", \"training_labels.json\")\n",
    "\n",
    "df = pd.DataFrame({\"sentences\" : sentences, \"speakers\" : speakers, \"labels\" : labels})\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.5, random_state=69)\n",
    "\n",
    "train, valid = train_test_split(train, test_size=0.3, random_state=69)\n",
    "\n",
    "\n",
    "print(f\"Train: {len(train)}\\nTest: {len(test)}\\nValid: {len(valid)}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "words=[]\n",
    "num_words = 1000\n",
    "\n",
    "for text in df[\"sentences\"]:\n",
    "    tokens=tokenizer(text)\n",
    "    words.extend(tokens)\n",
    "\n",
    "top_1k = dict(Counter(words).most_common(1000))\n",
    "top_1k['<unk>']=num_words+1\n",
    "top_1k['<pad>']=num_words+2\n",
    "\n",
    "vocab = torchtext.vocab.vocab(top_1k, specials = ['<unk>', '<pad>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences to vectors\n",
    "# elements of those vectors are ID's representing the words\n",
    "# padding / capping are applied\n",
    "max_len=80\n",
    "\n",
    "def vectorize_sentences(reviews, max_len):\n",
    "    vectors=[]\n",
    "    for text in reviews:\n",
    "        tokens=tokenizer(text)\n",
    "        v=vocab.forward(tokens)\n",
    "        if len(v) > max_len : v = v[:max_len]\n",
    "        if len(v) < max_len : #padding\n",
    "            tmp = np.full(max_len, vocab['<pad>'])\n",
    "            tmp[0:len(v)]=v \n",
    "            v = tmp\n",
    "        vectors.append(np.array(v))\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vectorize_sentences(train['sentences'], max_len)\n",
    "test_X = vectorize_sentences(test['sentences'], max_len)\n",
    "val_X = vectorize_sentences(valid['sentences'], max_len)\n",
    "\n",
    "train_y = np.array(train['labels']).reshape(-1,1)\n",
    "test_y = np.array(test['labels']).reshape(-1,1)\n",
    "val_y = np.array(valid['labels']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# create tensor datasets\n",
    "trainset = TensorDataset(torch.from_numpy(train_X).to(device), torch.from_numpy(train_y).float().to(device))\n",
    "validset = TensorDataset(torch.from_numpy(val_X).to(device), torch.from_numpy(val_y).float().to(device))\n",
    "testset = TensorDataset(torch.from_numpy(test_X).to(device), torch.from_numpy(test_y).float().to(device))\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(trainset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "valid_loader = DataLoader(validset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "test_loader = DataLoader(testset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        \"\"\"\n",
    "        vocab_size: (int) size of the vocabulary - required by embeddings\n",
    "        embed_dim: (int) size of embeddings\n",
    "        hidden_dim: (int) number of hidden units\n",
    "        num_class: (int) number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        r\"\"\"\n",
    "        Arguments:\n",
    "            text: 1-D tensor representing a bag of text tensors\n",
    "        \"\"\"\n",
    "        text=self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(text)\n",
    "        #for the sizes:\n",
    "        #output:[batch_size, sent_len, hidden_dim*num directions]\n",
    "        #hidden: [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        x = hidden.view(-1, self.hidden_dim)\n",
    "        x=self.dropout(x)\n",
    "        x=self.fc(x)\n",
    "        out = torch.sigmoid(x)\n",
    "        return out\n",
    "    \n",
    "    def get_embedding_for(self, w):\n",
    "        idx = vocab.lookup_indices([w])\n",
    "        return self.embedding(torch.Tensor(idx).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_model(model, optimizer, loss_criterion):\n",
    "    iter = 0\n",
    "    flag = 0\n",
    "    num_epochs = 10\n",
    "    history_train_acc, history_val_acc, history_train_loss, history_val_loss = [], [], [], []\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (samples, labels) in enumerate(train_loader):\n",
    "            # Training mode\n",
    "            model.train()\n",
    "\n",
    "            # Load samples\n",
    "            samples = samples.view(-1, max_len).to(device)\n",
    "            labels = labels.view(-1, 1).to(device)\n",
    "\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(samples)\n",
    "            \n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            \n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                # Get training statistics\n",
    "                train_loss = loss.data.item()\n",
    "            \n",
    "                # Testing mode\n",
    "                model.eval()\n",
    "                # Calculate Accuracy         \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                \n",
    "                true_labels = []\n",
    "                predicted_labels = []\n",
    "                # Iterate through test dataset\n",
    "                for samples, labels in valid_loader:\n",
    "                    # Load samples\n",
    "                    samples = samples.view(-1, max_len).to(device)\n",
    "                    labels = labels.view(-1).to(device)\n",
    "\n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(samples)\n",
    "\n",
    "                    # Val loss\n",
    "                    val_loss = loss_criterion(outputs.view(-1, 1), labels.view(-1, 1))\n",
    "\n",
    "                    predicted = outputs.ge(0.5).view(-1)\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum().item()\n",
    "                    # correct = (predicted == labels.byte()).int().sum().item()\n",
    "\n",
    "                    true_labels.extend(labels.cpu().numpy())\n",
    "                    predicted_labels.extend(predicted.cpu().numpy())\n",
    "                    score = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "                accuracy = 100. * correct / total\n",
    "\n",
    "                # Print Loss\n",
    "                print('Iter: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {} | F1-Score: {}'.format(iter, train_loss, val_loss.item(), round(accuracy, 2), score))\n",
    "\n",
    "                # Append to history\n",
    "                history_val_loss.append(val_loss.data.item())\n",
    "                history_val_acc.append(round(accuracy, 2))\n",
    "                history_train_loss.append(train_loss)\n",
    "\n",
    "                # Save model when accuracy beats best accuracy\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    # We can load this best model on the validation set later\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    return (history_train_acc, history_val_acc, history_train_loss, history_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next --> Learn how to use the train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100 | Train Loss: 0.5442842841148376 | Val Loss: 0.5196453332901001 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 200 | Train Loss: 0.45501670241355896 | Val Loss: 0.34091874957084656 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 300 | Train Loss: 0.39740949869155884 | Val Loss: 0.6293905377388 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 400 | Train Loss: 0.4156252145767212 | Val Loss: 0.43384283781051636 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 500 | Train Loss: 0.526523768901825 | Val Loss: 0.7034732699394226 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 600 | Train Loss: 0.44506949186325073 | Val Loss: 0.2481091022491455 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 700 | Train Loss: 0.599042534828186 | Val Loss: 0.32586905360221863 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 800 | Train Loss: 0.4241156578063965 | Val Loss: 0.4211306869983673 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 900 | Train Loss: 0.5314010381698608 | Val Loss: 0.4248012900352478 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1000 | Train Loss: 0.5768743753433228 | Val Loss: 0.5206735134124756 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1100 | Train Loss: 0.5068106055259705 | Val Loss: 0.42064812779426575 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1200 | Train Loss: 0.4866393804550171 | Val Loss: 0.6151100397109985 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1300 | Train Loss: 0.41259855031967163 | Val Loss: 0.5195353031158447 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1400 | Train Loss: 0.40042513608932495 | Val Loss: 0.7170125246047974 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1500 | Train Loss: 0.5826815366744995 | Val Loss: 0.4207199513912201 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1600 | Train Loss: 0.3501671850681305 | Val Loss: 0.5223344564437866 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1700 | Train Loss: 0.44709959626197815 | Val Loss: 0.6165263652801514 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1800 | Train Loss: 0.624539852142334 | Val Loss: 0.5199456214904785 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 1900 | Train Loss: 0.4991767704486847 | Val Loss: 0.5212284326553345 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2000 | Train Loss: 0.5397317409515381 | Val Loss: 0.5200369358062744 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2100 | Train Loss: 0.419890433549881 | Val Loss: 0.31583115458488464 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2200 | Train Loss: 0.4437885284423828 | Val Loss: 0.3227415680885315 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2300 | Train Loss: 0.5009869337081909 | Val Loss: 0.4217125475406647 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2400 | Train Loss: 0.4677230715751648 | Val Loss: 0.5241936445236206 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2500 | Train Loss: 0.5771955847740173 | Val Loss: 0.5197250247001648 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2600 | Train Loss: 0.5257147550582886 | Val Loss: 0.21309225261211395 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2700 | Train Loss: 0.511466383934021 | Val Loss: 0.3198262155056 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2800 | Train Loss: 0.5582500696182251 | Val Loss: 0.41760554909706116 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 2900 | Train Loss: 0.47945451736450195 | Val Loss: 0.7245065569877625 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3000 | Train Loss: 0.537310004234314 | Val Loss: 0.5202328562736511 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3100 | Train Loss: 0.5128694176673889 | Val Loss: 0.3247387111186981 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3200 | Train Loss: 0.6090399026870728 | Val Loss: 0.41809016466140747 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3300 | Train Loss: 0.4115881323814392 | Val Loss: 0.21922728419303894 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3400 | Train Loss: 0.4995102882385254 | Val Loss: 0.3165474534034729 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3500 | Train Loss: 0.4197351038455963 | Val Loss: 0.3197287619113922 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3600 | Train Loss: 0.5055530071258545 | Val Loss: 0.4206722378730774 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3700 | Train Loss: 0.4372518062591553 | Val Loss: 0.5209187865257263 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3800 | Train Loss: 0.4184092879295349 | Val Loss: 0.7218640446662903 | Val Accuracy: 81.74 | F1-Score: 0.0\n",
      "Iter: 3900 | Train Loss: 0.4789331555366516 | Val Loss: 0.31945210695266724 | Val Accuracy: 81.74 | F1-Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "input_dim = num_words + 2 #add 2 for start and end sentence symbols\n",
    "embedding_dim = 100\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "\n",
    "model = LSTMEmbedder(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "# criterion = torch.nn.CrossEntropyLoss() \n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-3)\n",
    "\n",
    "(train_acc, val_acc, train_loss, val_loss) = train_model(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "num_classes = 2\n",
    "num_features = tokenizer.vector_size # vector size of word embeddings\n",
    "\n",
    "# train data\n",
    "sentences, speakers, labels = custom_utils.read_data(\"training\", \"training_labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sentences, speakers, labels, model, criterion, optimizer):\n",
    "    assert(len(labels) == len(sentences))\n",
    "    model.train() \n",
    "    optimizer.zero_grad()\n",
    "    out = model(sentences, speakers)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def validate(sentences, speakers, labels, model, criterion):\n",
    "    assert(len(labels) == len(sentences))\n",
    "    model.eval()\n",
    "    out = model(sentences, speakers)\n",
    "    loss = criterion(out, labels)\n",
    "    pred_labels = out.argmax(dim=1)\n",
    "    f1 = F1Score(task='binary', num_classes=num_classes)\n",
    "    score = f1(labels, pred_labels)\n",
    "    return loss, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    n_folds = 5\n",
    "    n_epochs = 200\n",
    "    patience = 10\n",
    "    avg_score = 0\n",
    "    \n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(sentences, labels)):\n",
    "        # split data\n",
    "        train_labels = torch.tensor([labels[i] for i in train_idx])\n",
    "        valid_labels = torch.tensor([labels[i] for i in valid_idx])\n",
    "        \n",
    "        train_sentences = [sentences[i] for i in train_idx]\n",
    "        valid_sentences = [sentences[i] for i in valid_idx]\n",
    "\n",
    "        train_speakers = [speakers[i] for i in train_idx]\n",
    "        valid_speakers = [speakers[i] for i in valid_idx]\n",
    "        \n",
    "        # set model, criterion and optimizers\n",
    "        # 1. parameters\n",
    "        hidden_size = trial.suggest_int(f'hidden_size', 10, 100)\n",
    "        # hidden_size = 64\n",
    "        lr = trial.suggest_float(f'lr', 1e-3, 1e-2)\n",
    "        # lr = 0.001\n",
    "        # weight_decay = trial.suggest_float(f'weight_decay', 1e-5, 1e-2)\n",
    "        weight_decay = 5e-4\n",
    "        # 2. objects\n",
    "        model = LSTMClassifier(num_features, hidden_size, num_classes).to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss() # need to check input\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        best_weights = model.state_dict()\n",
    "\n",
    "        # Epoch\n",
    "        best_valid_loss = float('inf')\n",
    "        score_at_best = -1\n",
    "        current_patience = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = train(train_sentences, train_speakers, train_labels, model, criterion, optimizer)\n",
    "            valid_loss, score = validate(valid_sentences, valid_speakers, valid_labels, model, criterion)\n",
    "            \n",
    "            # Stopping criteria            \n",
    "            if valid_loss > best_valid_loss:\n",
    "                current_patience += 1\n",
    "            else:\n",
    "                best_weights = model.state_dict()\n",
    "                best_valid_loss = valid_loss    \n",
    "                score_at_best = score \n",
    "                current_patience = 0\n",
    "            \n",
    "            if current_patience == patience:\n",
    "                break\n",
    "            \n",
    "            print(f'Fold: {fold}, Epoch: {epoch}, Train loss: {train_loss:.4f}, Valid loss: {valid_loss:.4f}, Score: {score:.4f}')\n",
    "        \n",
    "        avg_score += score_at_best/n_folds\n",
    "        \n",
    "        # save model and params \n",
    "        torch.save(best_weights, f\"models/lstm_{trial.number}_{fold}.pt\")\n",
    "        json.dump(trial.params, open(f\"models/params_{trial.number}.json\", \"w\"))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_number = 0\n",
    "params = json.load(open(f\"models/params_{trial_number}.json\", \"r\"))\n",
    "\n",
    "# maybe load model from a certain fold ?\n",
    "# fold = 0\n",
    "# model.load_state_dict(torch.load(f\"models/lstm_{trial_number}_{fold}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(num_features, 64, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss() # need to check input\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use entire dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, train_speakers, _ = custom_utils.gather_dataset(\"training\", combine = False)\n",
    "sentences, speakers, labels = read_data(\"training\", \"training_labels.json\") \n",
    "\n",
    "y = labels\n",
    "X = list(zip(sentences, speakers))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "train_sentences, train_speakers = zip(*X_train)\n",
    "test_sentences, test_speakers = zip(*X_test)\n",
    "\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "n_epochs = 200\n",
    "patience = 10\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_sentences, train_speakers, train_labels, model, criterion, optimizer)\n",
    "    valid_loss, score = validate(test_sentences, test_speakers, test_labels, model, criterion)\n",
    "            \n",
    "    # Stopping criteria            \n",
    "    if valid_loss > best_valid_loss:\n",
    "        current_patience += 1\n",
    "    else:\n",
    "        best_weights = model.state_dict()\n",
    "        best_valid_loss = valid_loss    \n",
    "        score_at_best = score \n",
    "        current_patience = 0\n",
    "    \n",
    "    if current_patience == patience:\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Train loss: {train_loss:.4f}, Valid loss: {valid_loss:.4f}, Score: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data (dicitonary-like)\n",
    "test_sentences, test_speakers, _  = custom_utils.gather_dataset(\"test\", combine = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "model.eval()\n",
    "test_labels = {}\n",
    "for id in test_sentences.keys():\n",
    "    out = model(test_sentences[id], test_speakers[id])\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_labels[id] = pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(test_labels, open(\"test_labels.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
