{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizerFast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import src.custom_utils as cu\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordFeatureDataset(Dataset):\n",
    "    def __init__(self, data : pd.DataFrame, tokenizer : AutoTokenizer, max_seq_len : int):\n",
    "        # gather data\n",
    "        sentences = data['sentences'].to_list()\n",
    "        speakers = data['speakers'].to_list()\n",
    "        labels = data['labels'].to_list()\n",
    "\n",
    "        # token parameters\n",
    "        params = {\n",
    "            'max_length' : max_seq_len,\n",
    "            'padding' : True,\n",
    "            'truncation' : True,\n",
    "            'return_token_type_ids' : False\n",
    "        }\n",
    "        tokens = tokenizer.batch_encode_plus(sentences, **params)\n",
    "        \n",
    "        # hot encoder for speakers\n",
    "        switcher = {\n",
    "            \"PM\" : [1,0,0,0],\n",
    "            \"ME\" : [0,1,0,0],\n",
    "            \"UI\" : [0,0,1,0],\n",
    "            \"ID\" : [0,0,0,1]\n",
    "        }\n",
    "\n",
    "        self.sequences = torch.tensor(tokens['input_ids']).to(device)\n",
    "        self.attention_masks = torch.tensor(tokens['attention_mask']).to(device)\n",
    "        self.speakers = torch.Tensor([switcher[el] for el in speakers]).to(device)\n",
    "        self.lengths = torch.Tensor([[len(sentence.split())] for sentence in sentences]).to(device)\n",
    "        self.labels = torch.tensor(labels).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        speaker = self.speakers[idx]\n",
    "        length = self.lengths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = {\n",
    "            'sequence': sequence,\n",
    "            'attention_mask': attention_mask,\n",
    "            'speaker': speaker,\n",
    "            'length': length,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size : int, train : pd.DataFrame, valid : pd.DataFrame, tokenizer : AutoTokenizer, max_seq_len : int = 80) -> tuple[DataLoader, DataLoader]:\n",
    "    # create custom datasets\n",
    "    train_dataset = WordFeatureDataset(train, tokenizer, max_seq_len)\n",
    "    valid_dataset = WordFeatureDataset(valid, tokenizer, max_seq_len)\n",
    "\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate MLP from parameters\n",
    "def MLP(params):\n",
    "    n_layers = params['n_layers']\n",
    "    layers = []\n",
    "\n",
    "    in_features = params['input_size']\n",
    "    for i in range(n_layers):\n",
    "        out_features = params[f'n_{i}_size']\n",
    "        layers.append(torch.nn.Linear(in_features, out_features))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # dropout\n",
    "        p = params['n_p']\n",
    "        layers.append(torch.nn.Dropout(p))\n",
    "\n",
    "        # updating next layer size\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(torch.nn.Linear(in_features, params['output_size']))\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "class MLP_FT(torch.nn.Module):\n",
    "    def __init__(self, base_model, params):\n",
    "        super(MLP_FT, self).__init__()\n",
    "        self.base_model = deepcopy(base_model)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.mlp = MLP(params)\n",
    "\n",
    "    def forward(self, seq, mask, speakers, lengths):\n",
    "        # language model pass\n",
    "        outputs = self.base_model(seq, attention_mask=mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        x = hidden_states[:,0,:]\n",
    "\n",
    "        # MLP pass\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat((x, speakers, lengths), dim=1)\n",
    "        x = self.mlp(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, metric, params, train_loader, valid_loader):\n",
    "    n_epochs = params['n_epochs']\n",
    "    eval_at = params['eval_at']\n",
    "    max_patience = params['max_patience']\n",
    "\n",
    "    it = 0\n",
    "    hst_train_loss = [] \n",
    "    hst_valid_loss = []\n",
    "    hst_f1_score = []\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    patience = max_patience\n",
    "    best_weights = None\n",
    "    \n",
    "    # itera nas epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        if patience == 0: break\n",
    "        \n",
    "        # itera nos train batches\n",
    "        for idx, samples in enumerate(train_loader):\n",
    "            if patience == 0: break\n",
    "            it += 1\n",
    "\n",
    "            # train step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "            loss = criterion(out, samples['label'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = loss.cpu().detach().numpy() / 1\n",
    "\n",
    "            if it % eval_at == 0:\n",
    "                model.eval()\n",
    "\n",
    "                valid_loss = 0\n",
    "                f1_score = 0\n",
    "                \n",
    "                # itera nos valid batches\n",
    "                for idx, samples in enumerate(valid_loader):\n",
    "                    out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "                    loss = criterion(out, samples['label'])\n",
    "                    valid_loss += loss.cpu().detach().numpy() / len(valid_loader)\n",
    "                    f1_score += metric(samples['label'], out.argmax(dim=1)).cpu().detach().numpy() / len(valid_loader)\n",
    "                \n",
    "                # early stopping\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    best_weights = model.state_dict()\n",
    "                    patience = max_patience\n",
    "                else:\n",
    "                    patience -= 1 \n",
    "                \n",
    "                hst_train_loss.append(train_loss)\n",
    "                hst_valid_loss.append(valid_loss)\n",
    "                hst_f1_score.append(f1_score)\n",
    "\n",
    "                print('Iter: {} | Train Loss: {} | Val Loss: {} | F1-score: {}'.format(it, train_loss, valid_loss, f1_score))\n",
    "\n",
    "    # objective function criterion\n",
    "    combined = sorted(zip(hst_valid_loss, hst_f1_score), key=lambda x : x[0])\n",
    "    _, scores = zip(*combined)\n",
    "    qtd = 3\n",
    "    final_score = sum(scores[:qtd]) / qtd\n",
    "\n",
    "    results = {\n",
    "        \"score\" : final_score,\n",
    "        \"params\" : params,\n",
    "        \"valid_loss\" : hst_valid_loss,\n",
    "        \"train_loss\" : hst_train_loss,\n",
    "        \"f1_score\" : hst_f1_score, \n",
    "    }\n",
    "    \n",
    "    return best_weights, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperTuner:\n",
    "    def __init__(self, base_model, train, valid, tokenizer, max_seq_len):\n",
    "        self.base_model = base_model\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def objective(self, trial):\n",
    "        assert os.path.isdir(\"models\")\n",
    "        \n",
    "        model_params = {\n",
    "            \"input_size\" : self.base_model.config.hidden_size + 4 + 1,\n",
    "            \"output_size\" : 2,\n",
    "            \"n_layers\" : trial.suggest_int(\"n_layers\", 2, 3), \n",
    "            \"n_p\" : trial.suggest_float(\"n_p\", 0.2, 0.7),\n",
    "        }\n",
    "        for i in range(model_params[\"n_layers\"]):\n",
    "            model_params[f\"n_{i}_size\"] = trial.suggest_int(f\"n_{i}_size\", 200, 800)\n",
    "\n",
    "        training_params = {\n",
    "            \"batch_size\" : 100,\n",
    "            \"lr\" : trial.suggest_float(\"lr\", 1e-5, 1e-4),\n",
    "            \"weight_decay\" : trial.suggest_float(\"weight_decay\", 1e-5, 1e-4),\n",
    "            \"n_epochs\" : 5,\n",
    "            \"eval_at\" : 100,\n",
    "            \"max_patience\" : 10,\n",
    "        }\n",
    "\n",
    "        # model\n",
    "        model = MLP_FT(self.base_model, model_params)\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(self.train['labels'].to_numpy()), y=self.train['labels'].to_numpy())\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float()) \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=training_params['lr'], weight_decay=training_params['weight_decay'])\n",
    "        metric = F1Score(task='binary', num_classes=2).to(device)\n",
    "\n",
    "        # data loaders\n",
    "        train_loader, valid_loader = data_loader(training_params['batch_size'], self.train, self.valid, self.tokenizer, self.max_seq_len)\n",
    "        \n",
    "        trained_weights, results = train_model(model, criterion, optimizer, metric, training_params, train_loader, valid_loader)\n",
    "\n",
    "        # save weights\n",
    "        json.dump(results, open(f\"models/trial_{trial.number}.json\", \"w\"))\n",
    "\n",
    "        return results[\"score\"]\n",
    "    \n",
    "    def optimize(self, n_trials):\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self.objective, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "sentences, speakers, labels = cu.read_data(\"training\", \"training_labels.json\")\n",
    "\n",
    "# split data\n",
    "df = pd.DataFrame({\"sentences\" : sentences, \"speakers\" : speakers, \"labels\" : labels})\n",
    "train, valid = train_test_split(df, test_size=0.2, random_state=69, stratify=df.labels)\n",
    "\n",
    "print(f\"Train: {len(train)}\\nValid: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define base mode (embedder)\n",
    "# base_model_name = 'roberta-base'\n",
    "base_model_name = 'bert-base-uncased'\n",
    "base_model = AutoModel.from_pretrained(base_model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(base_model_name)\n",
    "max_seq_len = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt = HyperTuner(base_model, train, valid, tokenizer, max_seq_len)\n",
    "hpt.optimize(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Manual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"input_size\" : base_model.config.hidden_size + 4 + 1,\n",
    "    \"output_size\" : 2,\n",
    "    \"n_layers\" : 2, \n",
    "    \"n_0_size\" : 480,\n",
    "    \"n_1_size\" : 352,\n",
    "    \"n_p\" : 0.5,\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    \"batch_size\" : 64,\n",
    "    \"lr\" : 5e-5,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"n_epochs\" : 5,\n",
    "    \"eval_at\" : 100,\n",
    "    \"max_patience\" : 10,\n",
    "}\n",
    "\n",
    "# model\n",
    "model = MLP_FT(base_model, model_params)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train['labels'].to_numpy()), y=train['labels'].to_numpy())\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float()) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=training_params['lr'], weight_decay=training_params['weight_decay'])\n",
    "metric = F1Score(task='binary', num_classes=2).to(device)\n",
    "\n",
    "# data loaders\n",
    "train_loader, valid_loader = data_loader(training_params['batch_size'], train, valid, tokenizer, max_seq_len)\n",
    "\n",
    "# train model\n",
    "# trained_weights, score = train_model(model, criterion, optimizer, metric, training_params, train_loader, valid_loader)\n",
    "\n",
    "# save model\n",
    "# model.load_state_dict(trained_weights)\n",
    "# torch.save(model, \"MLP_FT.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format test input data\n",
    "def predict_labels(test_sentences : dict, test_speakers : dict, model : AutoModel, device : str = device) -> dict:\n",
    "    model.to(device)\n",
    "    test_data = {}\n",
    "    for id in test_sentences:\n",
    "        test_data[id] = cu.format_input(test_sentences[id], test_speakers[id], tokenizer, max_seq_len, device)\n",
    "\n",
    "    model.eval()\n",
    "    test_labels = {}\n",
    "\n",
    "    for id in test_sentences.keys():\n",
    "        out = model(**test_data[id])\n",
    "        pred = out.argmax(dim=1)\n",
    "        test_labels[id] = pred.cpu().detach().numpy()\n",
    "\n",
    "    return test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model \n",
    "model = torch.load(\"MLP_FT.pt\")\n",
    "\n",
    "# load test data\n",
    "test_sentences, test_speakers, _  = cu.read_data_by_ID(\"test\", combine = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predict_labels(test_sentences, test_speakers, model)\n",
    "except RuntimeError as e:\n",
    "    if device == \"cuda\" and \"CUDA out of memory\" in str(e):\n",
    "        print(\"Insufficient memory on gpu, predictions will be calculated using cpu\")\n",
    "        predict_labels(test_sentences, test_speakers, model, device=\"cpu\")\n",
    "    else: raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
