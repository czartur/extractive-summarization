{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizerFast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import src.custom_utils as cu\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p logs\n",
    "logging.basicConfig(level=logging.INFO, filename='logs/hyperTuner.log', encoding='utf-8', filemode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordFeatureDataset(Dataset):\n",
    "    def __init__(self, data : pd.DataFrame, tokenizer : AutoTokenizer, max_seq_len : int):\n",
    "        # gather data\n",
    "        sentences = data['sentences'].to_list()\n",
    "        speakers = data['speakers'].to_list()\n",
    "        labels = data['labels'].to_list()\n",
    "\n",
    "        # token parameters\n",
    "        params = {\n",
    "            'max_length' : max_seq_len,\n",
    "            'padding' : True,\n",
    "            'truncation' : True,\n",
    "            'return_token_type_ids' : False\n",
    "        }\n",
    "        tokens = tokenizer.batch_encode_plus(sentences, **params)\n",
    "        \n",
    "        # hot encoder for speakers\n",
    "        switcher = {\n",
    "            \"PM\" : [1,0,0,0],\n",
    "            \"ME\" : [0,1,0,0],\n",
    "            \"UI\" : [0,0,1,0],\n",
    "            \"ID\" : [0,0,0,1]\n",
    "        }\n",
    "\n",
    "        self.sequences = torch.tensor(tokens['input_ids']).to(device)\n",
    "        self.attention_masks = torch.tensor(tokens['attention_mask']).to(device)\n",
    "        self.speakers = torch.Tensor([switcher[el] for el in speakers]).to(device)\n",
    "        self.lengths = torch.Tensor([[len(sentence.split())] for sentence in sentences]).to(device)\n",
    "        self.labels = torch.tensor(labels).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        speaker = self.speakers[idx]\n",
    "        length = self.lengths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = {\n",
    "            'sequence': sequence,\n",
    "            'attention_mask': attention_mask,\n",
    "            'speaker': speaker,\n",
    "            'length': length,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size : int, train : pd.DataFrame, valid : pd.DataFrame, tokenizer : AutoTokenizer, max_seq_len : int = 80) -> tuple[DataLoader, DataLoader]:\n",
    "    # create custom datasets\n",
    "    train_dataset = WordFeatureDataset(train, tokenizer, max_seq_len)\n",
    "    valid_dataset = WordFeatureDataset(valid, tokenizer, max_seq_len)\n",
    "\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate MLP from parameters\n",
    "def MLP(params):\n",
    "    n_layers = params['n_layers']\n",
    "    layers = []\n",
    "\n",
    "    in_features = params['input_size']\n",
    "    for i in range(n_layers):\n",
    "        out_features = params[f'n_{i}_size']\n",
    "        layers.append(torch.nn.Linear(in_features, out_features))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # dropout\n",
    "        p = params['n_p']\n",
    "        layers.append(torch.nn.Dropout(p))\n",
    "\n",
    "        # updating next layer size\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(torch.nn.Linear(in_features, params['output_size']))\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "class MLP_FT(torch.nn.Module):\n",
    "    def __init__(self, base_model, params):\n",
    "        super(MLP_FT, self).__init__()\n",
    "        self.base_model = deepcopy(base_model)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.mlp = MLP(params)\n",
    "\n",
    "    def forward(self, seq, mask, speakers, lengths):\n",
    "        # language model pass\n",
    "        outputs = self.base_model(seq, attention_mask=mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        x = hidden_states[:,0,:]\n",
    "\n",
    "        # MLP pass\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat((x, speakers, lengths), dim=1)\n",
    "        x = self.mlp(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, metric, params, train_loader, valid_loader):\n",
    "    n_epochs = params['n_epochs']\n",
    "    eval_at = params['eval_at']\n",
    "    max_patience = params['max_patience']\n",
    "\n",
    "    hst_train_loss = [] \n",
    "    hst_valid_loss = []\n",
    "    hst_f1_score = []\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    patience = max_patience\n",
    "    best_weights = None\n",
    "    \n",
    "    it = 0\n",
    "    # itera nas epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        if patience == 0: break\n",
    "        \n",
    "        # itera nos train batches\n",
    "        for samples in tqdm(train_loader):\n",
    "            if patience == 0: break\n",
    "            it += 1\n",
    "\n",
    "            # train step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "            loss = criterion(out, samples['label'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = loss.cpu().detach().numpy() / 1\n",
    "\n",
    "            if it % eval_at == 0:\n",
    "                model.eval()\n",
    "\n",
    "                valid_loss = 0\n",
    "                f1_score = 0\n",
    "                \n",
    "                # itera nos valid batches\n",
    "                for idx, samples in enumerate(valid_loader):\n",
    "                    out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "                    loss = criterion(out, samples['label'])\n",
    "                    valid_loss += loss.cpu().detach().numpy() / len(valid_loader)\n",
    "                    f1_score += metric(samples['label'], out.argmax(dim=1)).cpu().detach().numpy() / len(valid_loader)\n",
    "                \n",
    "                # early stopping\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    best_weights = model.state_dict()\n",
    "                    patience = max_patience\n",
    "                else:\n",
    "                    patience -= 1 \n",
    "                \n",
    "                hst_train_loss.append(train_loss)\n",
    "                hst_valid_loss.append(valid_loss)\n",
    "                hst_f1_score.append(f1_score)\n",
    "\n",
    "                logging.info('Iter: {} | Train Loss: {} | Val Loss: {} | F1-score: {}'.format(it, train_loss, valid_loss, f1_score))\n",
    "\n",
    "    # objective function criterion\n",
    "    combined = sorted(zip(hst_valid_loss, hst_f1_score), key=lambda x : x[0])\n",
    "    _, scores = zip(*combined)\n",
    "    qtd = 3\n",
    "    final_score = sum(scores[:qtd]) / qtd\n",
    "\n",
    "    results = {\n",
    "        \"score\" : final_score,\n",
    "        \"params\" : params,\n",
    "        \"valid_loss\" : hst_valid_loss,\n",
    "        \"train_loss\" : hst_train_loss,\n",
    "        \"f1_score\" : hst_f1_score, \n",
    "    }\n",
    "    \n",
    "    return best_weights, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperTuner:\n",
    "    def __init__(self, base_model, train, valid, tokenizer, max_seq_len):\n",
    "        self.base_model = base_model\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def objective(self, trial):\n",
    "        assert os.path.isdir(\"models\")\n",
    "        \n",
    "        model_params = {\n",
    "            \"input_size\" : self.base_model.config.hidden_size + 4 + 1,\n",
    "            \"output_size\" : 2,\n",
    "            \"n_layers\" : trial.suggest_int(\"n_layers\", 2, 3), \n",
    "            \"n_p\" : trial.suggest_float(\"n_p\", 0.2, 0.7),\n",
    "        }\n",
    "        for i in range(model_params[\"n_layers\"]):\n",
    "            model_params[f\"n_{i}_size\"] = trial.suggest_int(f\"n_{i}_size\", 200, 800)\n",
    "\n",
    "        training_params = {\n",
    "            \"batch_size\" : 100,\n",
    "            \"lr\" : trial.suggest_float(\"lr\", 1e-5, 1e-4),\n",
    "            \"weight_decay\" : trial.suggest_float(\"weight_decay\", 1e-5, 1e-4),\n",
    "            \"n_epochs\" : 5,\n",
    "            \"eval_at\" : 50,\n",
    "            \"max_patience\" : 10,\n",
    "        }\n",
    "\n",
    "        # model\n",
    "        model = MLP_FT(self.base_model, model_params)\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(self.train['labels'].to_numpy()), y=self.train['labels'].to_numpy())\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float()) \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=training_params['lr'], weight_decay=training_params['weight_decay'])\n",
    "        metric = F1Score(task='binary', num_classes=2).to(device)\n",
    "\n",
    "        # data loaders\n",
    "        train_loader, valid_loader = data_loader(training_params['batch_size'], self.train, self.valid, self.tokenizer, self.max_seq_len)\n",
    "        \n",
    "        _, results = train_model(model, criterion, optimizer, metric, training_params, train_loader, valid_loader)\n",
    "\n",
    "        # save results\n",
    "        json.dump(training_params, open(f\"tuner/parameters/training_{trial.number}.json\", \"w\"))\n",
    "        json.dump(model_params, open(f\"tuner/parameters/model_{trial.number}.json\", \"w\"))\n",
    "        json.dump(results, open(f\"tuner/results/results_{trial.number}.json\", \"w\"))\n",
    "\n",
    "        return results[\"score\"]\n",
    "    \n",
    "    def optimize(self, n_trials):\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self.objective, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "sentences, speakers, labels = cu.read_data(\"training\", \"training_labels.json\")\n",
    "\n",
    "# split data\n",
    "df = pd.DataFrame({\"sentences\" : sentences, \"speakers\" : speakers, \"labels\" : labels})\n",
    "train, valid = train_test_split(df, test_size=0.2, random_state=69, stratify=df.labels)\n",
    "\n",
    "print(f\"Train: {len(train)}\\nValid: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define base mode (embedder)\n",
    "base_model_name = 'bert-base-uncased'\n",
    "base_model = AutoModel.from_pretrained(base_model_name)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(base_model_name)\n",
    "max_seq_len = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tuner/parameters # save model and training parameters\n",
    "!mkdir -p tuner/results # save results\n",
    "\n",
    "hpt = HyperTuner(base_model, train, valid, tokenizer, max_seq_len)\n",
    "hpt.optimize(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training from parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirty, only for this notebook\n",
    "def train_from_params(model_params, training_params, file_name : Optional[str] = None):\n",
    "    # model\n",
    "    model = MLP_FT(base_model, model_params)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train['labels'].to_numpy()), y=train['labels'].to_numpy())\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float()) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=training_params['lr'], weight_decay=training_params['weight_decay'])\n",
    "    metric = F1Score(task='binary', num_classes=2).to(device)\n",
    "\n",
    "    # data loaders\n",
    "    train_loader, valid_loader = data_loader(training_params['batch_size'], train, valid, tokenizer, max_seq_len)\n",
    "\n",
    "    # train model\n",
    "    trained_weights, _ = train_model(model, criterion, optimizer, metric, training_params, train_loader, valid_loader)\n",
    "\n",
    "    # reload weights\n",
    "    model.load_state_dict(trained_weights)\n",
    "\n",
    "    if file_name:\n",
    "        torch.save(model, file_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format test input data\n",
    "def predict_labels(test_sentences : dict, test_speakers : dict, model : AutoModel, device : str = device) -> dict:\n",
    "    model.to(device)\n",
    "    test_data = {}\n",
    "    for id in test_sentences:\n",
    "        test_data[id] = cu.format_input(test_sentences[id], test_speakers[id], tokenizer, max_seq_len, device)\n",
    "\n",
    "    model.eval()\n",
    "    test_labels = {}\n",
    "\n",
    "    for id in test_sentences.keys():\n",
    "        out = model(**test_data[id])\n",
    "        pred = out.argmax(dim=1)\n",
    "        test_labels[id] = pred.cpu().detach().tolist()\n",
    "\n",
    "    return test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 using a selected model to train (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"input_size\" : base_model.config.hidden_size + 4 + 1,\n",
    "    \"output_size\" : 2,\n",
    "    \"n_layers\" : 3,\n",
    "    \"n_p\" : 0.5,\n",
    "    \"n_0_size\" : 400,\n",
    "    \"n_1_size\" : 300,\n",
    "    \"n_2_size\" : 200\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    \"batch_size\" : 100,\n",
    "    \"lr\" : 5e-5,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"n_epochs\" : 5,\n",
    "    \"eval_at\" : 50,\n",
    "    \"max_patience\" : 10,\n",
    "}\n",
    "\n",
    "# # uncomment to save and predict labels\n",
    "# model = train_from_params(model_params, training_params)\n",
    "# torch.save(model, \"selected_model.pt\")\n",
    "# !python3 predict_labels.py --model_path selected_model.pt --labels_path labels_from_selected.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 using best models from hyper tuning --> majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pick some params for trainin (based on score during hyp. tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "all_training_params = {}\n",
    "all_model_params = {}\n",
    "\n",
    "for item in Path(\"tuner/results/\").iterdir():\n",
    "    if not item.suffix == \".json\" : continue\n",
    "    \n",
    "    idx = item.stem.split('_')[2]\n",
    "    all_scores[idx] = json.load(open(item, \"r\"))[\"score\"]\n",
    "    \n",
    "    tp_path = f\"tuner/parameters/training_{idx}.json\"\n",
    "    m_path = f\"tuner/parameters/model_{idx}.json\"\n",
    "\n",
    "    all_training_params[idx] = json.load(open(tp_path, \"r\"))\n",
    "    all_model_params[idx] = json.load(open(m_path, \"r\"))\n",
    "    \n",
    "# take 10 trials with best scores\n",
    "number_of_models = 10\n",
    "idx_score = sorted(all_scores.items(), key=lambda x : x[1], reverse=True)\n",
    "print(idx_score[:number_of_models])\n",
    "\n",
    "best_idxs, _ = zip(*idx_score[:number_of_models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> train with selected params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_sentences, test_speakers, _  = cu.read_data_by_ID(\"test\", combine = False)\n",
    " \n",
    "# save predictions for each model\n",
    "all_predictions = []\n",
    "\n",
    "for idx in best_idxs:\n",
    "    model = train_from_params(all_model_params[idx], all_training_params[idx])\n",
    "    \n",
    "    model.eval()\n",
    "    try:\n",
    "        prediction = predict_labels(test_sentences, test_speakers, model)\n",
    "    except RuntimeError as e:\n",
    "        if device == \"cuda\" and \"CUDA out of memory\" in str(e):\n",
    "            print(\"Insufficient memory on gpu, predictions will be calculated using cpu\")\n",
    "            test_labels = predict_labels(test_sentences, test_speakers, model, device=\"cpu\")\n",
    "        else: raise e\n",
    "    \n",
    "    all_predictions.append(prediction) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> majority vote itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = {}\n",
    "\n",
    "for key in all_predictions[0].keys():\n",
    "    pred_for_key = [preds[key] for preds in all_predictions]\n",
    "    \n",
    "    # calculate mean and cap to 1 where val >= 0.5\n",
    "    major_preds = [1 if sum(x) / len(all_predictions) >= 0.5 else 0 for x in zip(*pred_for_key)]\n",
    "    \n",
    "    # assign to test_labels\n",
    "    test_labels[key] = major_preds\n",
    "\n",
    "print(test_labels)\n",
    "\n",
    "json.dump(test_labels, open(\"labels_from_majority_vote.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
