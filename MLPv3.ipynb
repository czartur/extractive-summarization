{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/Documents/projects/extractive-summarization/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import custom_utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 40668\n",
      "Test: 14525\n",
      "Valid: 17430\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read\n",
    "sentences, speakers, labels = custom_utils.read_data(\"training\", \"training_labels.json\")\n",
    "\n",
    "# split\n",
    "df = pd.DataFrame({\"sentences\" : sentences, \"speakers\" : speakers, \"labels\" : labels})\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=69, stratify=df.labels)\n",
    "train, valid = train_test_split(train, test_size=0.3, random_state=69, stratify=train.labels)\n",
    "\n",
    "print(f\"Train: {len(train)}\\nTest: {len(test)}\\nValid: {len(valid)}\")\n",
    "train.head()\n",
    "\n",
    "# Getting setences list\n",
    "train_sentences = train['sentences'].to_list()\n",
    "valid_sentences = valid['sentences'].to_list()\n",
    "test_sentences = test['sentences'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "params = {\n",
    "    'max_length' : max_seq_len,\n",
    "    'padding' : True,\n",
    "    'truncation' : True,\n",
    "    'return_token_type_ids' : False\n",
    "}\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "train_tokens = tokenizer.batch_encode_plus(train_sentences, **params)\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "valid_tokens = tokenizer.batch_encode_plus(valid_sentences, **params)\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "test_tokens = tokenizer.batch_encode_plus(test_sentences, **params)\n",
    "\n",
    "# hot encoder for speakers\n",
    "switcher = {\n",
    "    \"PM\" : [1,0,0,0],\n",
    "    \"ME\" : [0,1,0,0],\n",
    "    \"UI\" : [0,0,1,0],\n",
    "    \"ID\" : [0,0,0,1]\n",
    "}\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(train_tokens['input_ids'])\n",
    "train_mask = torch.tensor(train_tokens['attention_mask'])\n",
    "train_speaker = torch.Tensor([switcher[el] for el in train['speakers']]).to(device)\n",
    "train_len = torch.Tensor([[len(sentence.split())] for sentence in train['sentences']]).to(device)\n",
    "train_y = torch.tensor(train['labels'].to_numpy())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(valid_tokens['input_ids'])\n",
    "val_mask = torch.tensor(valid_tokens['attention_mask'])\n",
    "valid_speaker = torch.Tensor([switcher[el] for el in valid['speakers']]).to(device)\n",
    "valid_len = torch.Tensor([[len(sentence.split())] for sentence in valid['sentences']]).to(device)\n",
    "valid_y = torch.tensor(valid['labels'].to_numpy())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(test_tokens['input_ids'])\n",
    "test_mask = torch.tensor(test_tokens['attention_mask'])\n",
    "test_speaker = torch.Tensor([switcher[el] for el in test['speakers']]).to(device)\n",
    "test_len = torch.Tensor([[len(sentence.split())] for sentence in test['sentences']]).to(device)\n",
    "test_y = torch.tensor(test['labels'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WordFeatureDataset(Dataset):\n",
    "    def __init__(self, sequences, attention_masks, speakers, lengths, labels):\n",
    "        self.sequences = sequences\n",
    "        self.attention_masks = attention_masks\n",
    "        self.speakers = speakers\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        speaker = self.speakers[idx]\n",
    "        length = self.lengths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = {\n",
    "            'sequence': sequence,\n",
    "            'attention_mask': attention_mask,\n",
    "            'speaker': speaker,\n",
    "            'length': length,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def data_loader(batch_size):\n",
    "    # create tensor datasets\n",
    "    train_dataset = WordFeatureDataset(train_seq, train_mask, train_speaker, train_len, train_y)\n",
    "    valid_dataset = WordFeatureDataset(val_seq, val_mask, valid_speaker, valid_len, valid_y)\n",
    "    test_dataset = WordFeatureDataset(test_seq, test_mask, test_speaker, test_len, test_y)\n",
    "\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Bert(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(MLP_Bert, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        # softmax activation function\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        # pass the inputs to the model  \n",
    "        outputs = self.bert(sent_id, attention_mask=mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Average pooling across the entire sequence\n",
    "        avg_pooled = torch.mean(hidden_states, dim=1)\n",
    "\n",
    "        x = self.fc1(avg_pooled)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # apply softmax activation\n",
    "        # x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchmetrics.classification import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6957338452339172\n",
      "0.6357496380805969\n",
      "0.6081460118293762\n",
      "0.5195344686508179\n",
      "0.5750989317893982\n",
      "0.6164846420288086\n",
      "0.47410687804222107\n",
      "0.44061678647994995\n",
      "0.5451452732086182\n",
      "0.5210814476013184\n",
      "Iter: 10 | Train Loss: 0.5210814476013184 | Val Loss: 0.4649866223335266 | F1-score: 0.5434243381023407\n",
      "0.47182002663612366\n",
      "0.6088654398918152\n",
      "0.4705297648906708\n",
      "0.4009742736816406\n",
      "0.48865222930908203\n",
      "0.38142427802085876\n",
      "0.4440515637397766\n",
      "0.4908559024333954\n",
      "0.42344650626182556\n",
      "0.4834602177143097\n",
      "Iter: 20 | Train Loss: 0.4834602177143097 | Val Loss: 0.434982031583786 | F1-score: 0.5966748893260956\n",
      "0.4789697527885437\n",
      "0.5518483519554138\n",
      "0.4203784465789795\n",
      "0.44918760657310486\n",
      "0.5708299875259399\n",
      "0.41181573271751404\n",
      "0.4768314063549042\n",
      "0.3348318338394165\n",
      "0.4104020297527313\n",
      "0.42413556575775146\n",
      "Iter: 30 | Train Loss: 0.42413556575775146 | Val Loss: 0.4026656746864319 | F1-score: 0.4749823957681656\n",
      "0.37824708223342896\n",
      "0.5124050974845886\n",
      "0.319325715303421\n",
      "0.41193506121635437\n",
      "0.4364067018032074\n",
      "0.5681343078613281\n",
      "0.43738558888435364\n",
      "0.4743936359882355\n",
      "0.3577970266342163\n",
      "0.4311961531639099\n",
      "Iter: 40 | Train Loss: 0.4311961531639099 | Val Loss: 0.5194012224674225 | F1-score: 0.47641509771347046\n",
      "0.5783412456512451\n",
      "0.38588830828666687\n",
      "0.4131656587123871\n",
      "0.42669761180877686\n",
      "0.48023220896720886\n",
      "0.41691097617149353\n",
      "0.48412102460861206\n",
      "0.3917158842086792\n",
      "0.46430495381355286\n",
      "0.4172203540802002\n",
      "Iter: 50 | Train Loss: 0.4172203540802002 | Val Loss: 0.3866553157567978 | F1-score: 0.5760368704795837\n",
      "0.5393063426017761\n",
      "0.4533608853816986\n",
      "0.49016764760017395\n",
      "0.4420533776283264\n",
      "0.461416631937027\n",
      "0.5234756469726562\n",
      "0.37696242332458496\n",
      "0.3790853023529053\n",
      "0.5556069016456604\n",
      "0.4698581397533417\n",
      "Iter: 60 | Train Loss: 0.4698581397533417 | Val Loss: 0.42970722913742065 | F1-score: 0.6136617958545685\n",
      "0.3472796380519867\n",
      "0.42253977060317993\n",
      "0.475497305393219\n",
      "0.48616304993629456\n",
      "0.46732327342033386\n",
      "0.4423373341560364\n",
      "0.4577408730983734\n",
      "0.45227283239364624\n",
      "0.41517767310142517\n",
      "0.41539761424064636\n",
      "Iter: 70 | Train Loss: 0.41539761424064636 | Val Loss: 0.4359036982059479 | F1-score: 0.5512917786836624\n",
      "0.39144468307495117\n",
      "0.4097665846347809\n",
      "0.4390749931335449\n",
      "0.5053189396858215\n",
      "0.4045422077178955\n",
      "0.4382252097129822\n",
      "0.32040098309516907\n",
      "0.3758023977279663\n",
      "0.44472700357437134\n",
      "0.5284786224365234\n",
      "Iter: 80 | Train Loss: 0.5284786224365234 | Val Loss: 0.4799579381942749 | F1-score: 0.6390697956085205\n",
      "0.4755184054374695\n",
      "0.5525733232498169\n",
      "0.4516907036304474\n",
      "0.5261081457138062\n",
      "0.5492680668830872\n",
      "0.4888942241668701\n",
      "0.5006685256958008\n",
      "0.3699663281440735\n",
      "0.42347508668899536\n",
      "0.47474032640457153\n",
      "Iter: 90 | Train Loss: 0.47474032640457153 | Val Loss: 0.4763955622911453 | F1-score: 0.4851643890142441\n",
      "0.4933742880821228\n",
      "0.4696200489997864\n",
      "0.4589654803276062\n",
      "0.3335117995738983\n",
      "0.41761285066604614\n",
      "0.452772319316864\n",
      "0.5122241377830505\n",
      "0.4875355660915375\n",
      "0.3918950855731964\n",
      "0.4183273911476135\n",
      "Iter: 100 | Train Loss: 0.4183273911476135 | Val Loss: 0.45873500406742096 | F1-score: 0.5494087934494019\n",
      "0.4844379723072052\n",
      "0.46242642402648926\n",
      "0.45126402378082275\n",
      "0.43641743063926697\n",
      "0.32729265093803406\n",
      "0.3549848198890686\n",
      "0.5937459468841553\n",
      "0.481774240732193\n",
      "0.3397144675254822\n",
      "0.34161365032196045\n",
      "Iter: 110 | Train Loss: 0.34161365032196045 | Val Loss: 0.45076288282871246 | F1-score: 0.5306544303894043\n",
      "0.4979017376899719\n",
      "0.4967684745788574\n",
      "0.5444652438163757\n",
      "0.5342247486114502\n",
      "0.42932045459747314\n",
      "0.5424495339393616\n",
      "0.3923940062522888\n",
      "0.605285108089447\n",
      "0.3755902349948883\n",
      "0.4124642312526703\n",
      "Iter: 120 | Train Loss: 0.4124642312526703 | Val Loss: 0.45126523077487946 | F1-score: 0.604519784450531\n",
      "0.4168083369731903\n",
      "0.40383654832839966\n",
      "0.3757117688655853\n",
      "0.4334857165813446\n",
      "0.42179369926452637\n",
      "0.4320571720600128\n",
      "0.400358647108078\n",
      "0.4098855257034302\n",
      "0.47879642248153687\n",
      "0.42256200313568115\n",
      "Iter: 130 | Train Loss: 0.42256200313568115 | Val Loss: 0.4590958058834076 | F1-score: 0.5678321719169617\n",
      "0.543606698513031\n",
      "0.4982971251010895\n",
      "0.5513581037521362\n"
     ]
    }
   ],
   "source": [
    "model = MLP_Bert(bert=bert)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train['labels'].to_numpy()), y=train['labels'].to_numpy())\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float()) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "f1 = F1Score(task='binary', num_classes=2).to(device)\n",
    "\n",
    "train_loader, valid_loader, test_loader = data_loader(100)\n",
    "\n",
    "n_epochs = 20\n",
    "it = 0\n",
    "hst_train_loss = [] \n",
    "hst_valid_loss = []\n",
    "hst_f1_score = []\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "patience = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch > 0:\n",
    "        break\n",
    "    if patience == 0: break\n",
    "    \n",
    "    for samples in train_loader:\n",
    "        if patience == 0: break\n",
    "        it += 1\n",
    "\n",
    "        # train step\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(sent_id = samples['sequence'], mask = samples['attention_mask'])\n",
    "        loss = criterion(out, samples['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss = loss.cpu().detach().numpy() / 1\n",
    "        print(train_loss)\n",
    "\n",
    "        if it % 10 == 0:\n",
    "            model.eval()\n",
    "\n",
    "            valid_loss = 0\n",
    "            f1_score = 0\n",
    "            \n",
    "            qnt = 2\n",
    "            for idx, samples in enumerate(valid_loader):\n",
    "                if idx >= qnt:\n",
    "                    break \n",
    "                out = model(sent_id = samples['sequence'], mask = samples['attention_mask'])\n",
    "                loss = criterion(out, samples['label'])\n",
    "                # valid_loss += loss.cpu().detach().numpy() / len(valid_loader)\n",
    "                valid_loss += loss.cpu().detach().numpy() / qnt\n",
    "                # f1_score += f1(samples['label'], out.argmax(dim=1)).cpu().detach().numpy() / len(valid_loader)\n",
    "                f1_score += f1(samples['label'], out.argmax(dim=1)).cpu().detach().numpy() / qnt\n",
    "            \n",
    "            # early stopping\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                best_weights = model.state_dict()\n",
    "                patience = 10\n",
    "            else:\n",
    "                patience -= 1 \n",
    "            \n",
    "            hst_train_loss.append(train_loss)\n",
    "            hst_valid_loss.append(valid_loss)\n",
    "            hst_f1_score.append(f1_score)\n",
    "\n",
    "            print('Iter: {} | Train Loss: {} | Val Loss: {} | F1-score: {}'.format(it, train_loss, valid_loss, f1_score))\n",
    "\n",
    "# objective function criterion\n",
    "combined = sorted(zip(hst_valid_loss, hst_f1_score), key=lambda x : x[0])\n",
    "_, scores = zip(*combined)\n",
    "qtd = 3\n",
    "final_score = sum(scores[:qtd]) / qtd\n",
    "\n",
    "# torch.save(best_weights, f\"models/mlp_{trial.number}.pt\")\n",
    "# results = {\n",
    "#     \"score\" : final_score,\n",
    "#     \"params\" : params,\n",
    "#     \"valid_loss\" : hst_valid_loss,\n",
    "#     \"train_loss\" : hst_train_loss,\n",
    "#     \"f1_score\" : hst_f1_score, \n",
    "# }\n",
    "# json.dump(results, open(f\"models/mlp_results_{trial.number}.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
