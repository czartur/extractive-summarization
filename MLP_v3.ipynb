{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/czartur/code/course/deep_learning/datachallenge/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import optuna\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizerFast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import src.custom_utils as cu\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordFeatureDataset(Dataset):\n",
    "    def __init__(self, data : pd.DataFrame, tokenizer : AutoTokenizer, max_seq_len : int):\n",
    "        # gather data\n",
    "        sentences = data['sentences'].to_list()\n",
    "        speakers = data['speakers'].to_list()\n",
    "        labels = data['labels'].to_list()\n",
    "\n",
    "        # token parameters\n",
    "        params = {\n",
    "            'max_length' : max_seq_len,\n",
    "            'padding' : True,\n",
    "            'truncation' : True,\n",
    "            'return_token_type_ids' : False\n",
    "        }\n",
    "        tokens = tokenizer.batch_encode_plus(sentences, **params)\n",
    "        \n",
    "        # hot encoder for speakers\n",
    "        switcher = {\n",
    "            \"PM\" : [1,0,0,0],\n",
    "            \"ME\" : [0,1,0,0],\n",
    "            \"UI\" : [0,0,1,0],\n",
    "            \"ID\" : [0,0,0,1]\n",
    "        }\n",
    "\n",
    "        self.sequences = torch.tensor(tokens['input_ids']).to(device)\n",
    "        self.attention_masks = torch.tensor(tokens['attention_mask']).to(device)\n",
    "        self.speakers = torch.Tensor([switcher[el] for el in speakers]).to(device)\n",
    "        self.lengths = torch.Tensor([[len(sentence.split())] for sentence in sentences]).to(device)\n",
    "        self.labels = torch.tensor(labels).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        speaker = self.speakers[idx]\n",
    "        length = self.lengths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = {\n",
    "            'sequence': sequence,\n",
    "            'attention_mask': attention_mask,\n",
    "            'speaker': speaker,\n",
    "            'length': length,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size : int, train : pd.DataFrame, valid : pd.DataFrame, tokenizer : AutoTokenizer, max_seq_len : int = 80) -> tuple[DataLoader, DataLoader]:\n",
    "    # create custom datasets\n",
    "    train_dataset = WordFeatureDataset(train, tokenizer, max_seq_len)\n",
    "    valid_dataset = WordFeatureDataset(valid, tokenizer, max_seq_len)\n",
    "\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate MLP from parameters\n",
    "def MLP(params):\n",
    "    n_layers = params['n_layers']\n",
    "    layers = []\n",
    "\n",
    "    in_features = params['input_size']\n",
    "    for i in range(n_layers):\n",
    "        out_features = params[f'n_{i}_size']\n",
    "        layers.append(torch.nn.Linear(in_features, out_features))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # dropout\n",
    "        p = params['n_p']\n",
    "        layers.append(torch.nn.Dropout(p))\n",
    "\n",
    "        # updating next layer size\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(torch.nn.Linear(in_features, params['output_size']))\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "class MLP_FT(torch.nn.Module):\n",
    "    def __init__(self, base_model, params):\n",
    "        super(MLP_FT, self).__init__()\n",
    "        self.base_model = deepcopy(base_model)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.mlp = MLP(params)\n",
    "\n",
    "    def forward(self, seq, mask, speakers, lengths):\n",
    "        # language model pass\n",
    "        outputs = self.base_model(seq, attention_mask=mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        x = hidden_states[:,0,:]\n",
    "\n",
    "        # MLP pass\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat((x, speakers, lengths), dim=1)\n",
    "        x = self.mlp(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_MLP_FT(model, params, train, valid, tokenizer, max_seq_len):\n",
    "def train_MLP_FT(model, criterion, optimizer, metric, params, train_loader, valid_loader):\n",
    "    # n_epochs = 5\n",
    "    # eval_at = 100\n",
    "    n_epochs = params['n_epochs']\n",
    "    eval_at = params['eval_at']\n",
    "    max_patience = params['max_patience']\n",
    "\n",
    "    # model = MLP_FT(base_model, params).to(device)\n",
    "\n",
    "    # train_loader, valid_loader = data_loader(batch_size, train, valid, tokenizer, max_seq_len)\n",
    "\n",
    "\n",
    "    it = 0\n",
    "    hst_train_loss = [] \n",
    "    hst_valid_loss = []\n",
    "    hst_f1_score = []\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    patience = max_patience\n",
    "    best_model = None\n",
    "    \n",
    "    # itera nas epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        if patience == 0: break\n",
    "        \n",
    "        # itera nos train batches\n",
    "        for idx, samples in enumerate(train_loader):\n",
    "            if patience == 0: break\n",
    "            it += 1\n",
    "\n",
    "            # train step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "            loss = criterion(out, samples['label'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = loss.cpu().detach().numpy() / 1\n",
    "\n",
    "            if it % eval_at == 0:\n",
    "                model.eval()\n",
    "\n",
    "                valid_loss = 0\n",
    "                f1_score = 0\n",
    "                \n",
    "                # itera nos valid batches\n",
    "                for idx, samples in enumerate(valid_loader):\n",
    "                    out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "                    loss = criterion(out, samples['label'])\n",
    "                    valid_loss += loss.cpu().detach().numpy() / len(valid_loader)\n",
    "                    f1_score += metric(samples['label'], out.argmax(dim=1)).cpu().detach().numpy() / len(valid_loader)\n",
    "                \n",
    "                # early stopping\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    best_weights = model.state_dict()\n",
    "                    patience = max_patience\n",
    "                else:\n",
    "                    patience -= 1 \n",
    "                \n",
    "                hst_train_loss.append(train_loss)\n",
    "                hst_valid_loss.append(valid_loss)\n",
    "                hst_f1_score.append(f1_score)\n",
    "\n",
    "                print('Iter: {} | Train Loss: {} | Val Loss: {} | F1-score: {}'.format(it, train_loss, valid_loss, f1_score))\n",
    "\n",
    "    # objective function criterion\n",
    "    combined = sorted(zip(hst_valid_loss, hst_f1_score), key=lambda x : x[0])\n",
    "    _, scores = zip(*combined)\n",
    "    qtd = 3\n",
    "    final_score = sum(scores[:qtd]) / qtd\n",
    "\n",
    "    results = {\n",
    "        \"score\" : final_score,\n",
    "        \"params\" : params,\n",
    "        \"valid_loss\" : hst_valid_loss,\n",
    "        \"train_loss\" : hst_train_loss,\n",
    "        \"f1_score\" : hst_f1_score, \n",
    "    }\n",
    "    \n",
    "    return best_weights, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     # model parameters\n",
    "#     params = {\n",
    "#         # \"n_layers\" : trial.suggest_int(\"n_layers\", 2, 5),\n",
    "#         \"n_layers\" : 2,\n",
    "#         \"input_size\" : base_model.config.hidden_size + 4 + 1,\n",
    "#         \"output_size\" : 2,\n",
    "#         \"n_p\" : trial.suggest_float(\"n_p\", 0.2, 0.7),\n",
    "#         \"lr\" : trial.suggest_float(\"lr\", 1e-5, 1e-4),\n",
    "#         \"weight_decay\" : trial.suggest_float(\"weight_decay\", 1e-5, 1e-4),\n",
    "#         \"batch_size\" : 100\n",
    "#     }\n",
    "#     for i in range(params[\"n_layers\"]):\n",
    "#         params[f\"n_{i}_size\"] = trial.suggest_int(f\"n_{i}_size\", 200, 800)\n",
    "    \n",
    "#     _, results = train_MLP_FT(base_model, params, trial)\n",
    "    \n",
    "#     # save results\n",
    "#     json.dump(results, open(f\"models/mlp_results_{trial.number}.json\", \"w\"))\n",
    "\n",
    "#     return results['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 58098\n",
      "Valid: 14525\n"
     ]
    }
   ],
   "source": [
    "# read\n",
    "sentences, speakers, labels = cu.read_data(\"training\", \"training_labels.json\")\n",
    "\n",
    "# split\n",
    "df = pd.DataFrame({\"sentences\" : sentences, \"speakers\" : speakers, \"labels\" : labels})\n",
    "train, valid = train_test_split(df, test_size=0.2, random_state=69, stratify=df.labels)\n",
    "\n",
    "print(f\"Train: {len(train)}\\nValid: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define base mode (embedder)\n",
    "# base_model_name = 'roberta-base'\n",
    "base_model_name = 'bert-base-uncased'\n",
    "base_model = AutoModel.from_pretrained(base_model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(base_model_name)\n",
    "max_seq_len = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"input_size\" : base_model.config.hidden_size + 4 + 1,\n",
    "    \"output_size\" : 2,\n",
    "    \"n_layers\" : 2, \n",
    "    \"n_0_size\" : 480,\n",
    "    \"n_1_size\" : 352,\n",
    "    \"n_p\" : 0.5,\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    \"batch_size\" : 64,\n",
    "    \"lr\" : 5e-5,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"n_epochs\" : 5,\n",
    "    \"eval_at\" : 100,\n",
    "    \"max_patience\" : 10,\n",
    "}\n",
    "\n",
    "# model\n",
    "model = MLP_FT(base_model, model_params)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train['labels'].to_numpy()), y=train['labels'].to_numpy())\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float()) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=training_params['lr'], weight_decay=training_params['weight_decay'])\n",
    "metric = F1Score(task='binary', num_classes=2).to(device)\n",
    "\n",
    "# data loaders\n",
    "train_loader, valid_loader = data_loader(training_params['batch_size'], train, valid, tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trained_model, score \u001b[39m=\u001b[39m train_MLP_FT(model, criterion, optimizer, metric, training_params, train_loader, valid_loader)\n",
      "\u001b[1;32m/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m out \u001b[39m=\u001b[39m model(samples[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m], samples[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m], samples[\u001b[39m'\u001b[39m\u001b[39mspeaker\u001b[39m\u001b[39m'\u001b[39m], samples[\u001b[39m'\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, samples[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/czartur/code/course/deep_learning/datachallenge/MLP_v3.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m train_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39m/\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/code/course/deep_learning/datachallenge/.venv/lib/python3.11/site-packages/torch/_tensor.py:483\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[39mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39;49mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39;49m,),\n\u001b[1;32m    486\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    487\u001b[0m         gradient\u001b[39m=\u001b[39;49mgradient,\n\u001b[1;32m    488\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    489\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[1;32m    492\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/code/course/deep_learning/datachallenge/.venv/lib/python3.11/site-packages/torch/overrides.py:1560\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1557\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1560\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1561\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1562\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/code/course/deep_learning/datachallenge/.venv/lib/python3.11/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/course/deep_learning/datachallenge/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/code/course/deep_learning/datachallenge/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "trained_model, score = train_MLP_FT(model, criterion, optimizer, metric, training_params, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences, test_speakers, _  = cu.read_data_by_ID(\"test\", combine = False)\n",
    "\n",
    "# format test input data\n",
    "test_data = {}\n",
    "for id in test_sentences:\n",
    "    test_data[id] = cu.format_input(test_sentences[id], test_speakers[id], tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = {\n",
    "#     \"input_size\" : base_model.config.hidden_size + 4 + 1,\n",
    "#     \"output_size\" : 2,\n",
    "#     \"n_layers\" : 2, \n",
    "#     \"n_0_size\" : 480,\n",
    "#     \"n_1_size\" : 352,\n",
    "#     \"n_p\" : 0.5,\n",
    "#     \"lr\" : 5e-5,\n",
    "#     \"weight_decay\" : 5e-4,\n",
    "#     \"batch_size\" : 64,\n",
    "# }\n",
    "\n",
    "# model = MLP_FT(base_model, model_params).to(device)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"rezero.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # given that we have already a model\n",
    "# test_labels = {}\n",
    "\n",
    "# model.eval()\n",
    "# test_labels = {}\n",
    "# for id in test_sentences.keys():\n",
    "#     print(id)\n",
    "#     out = model(**test_data[id])\n",
    "#     pred = out.argmax(dim=1)\n",
    "#     test_labels[id] = pred.cpu().detach().numpy()\n",
    "#     print(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
