{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import custom_utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 52288\n",
      "Test: 14525\n",
      "Valid: 5810\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read\n",
    "sentences, speakers, labels = custom_utils.read_data(\"training\", \"training_labels.json\")\n",
    "\n",
    "# split\n",
    "df = pd.DataFrame({\"sentences\" : sentences, \"speakers\" : speakers, \"labels\" : labels})\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=69, stratify=df.labels)\n",
    "train, valid = train_test_split(train, test_size=0.1, random_state=69, stratify=train.labels)\n",
    "\n",
    "print(f\"Train: {len(train)}\\nTest: {len(test)}\\nValid: {len(valid)}\")\n",
    "train.head()\n",
    "\n",
    "# Getting setences list\n",
    "train_sentences = train['sentences'].to_list()\n",
    "valid_sentences = valid['sentences'].to_list()\n",
    "test_sentences = test['sentences'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "params = {\n",
    "    'max_length' : max_seq_len,\n",
    "    'padding' : True,\n",
    "    'truncation' : True,\n",
    "    'return_token_type_ids' : False\n",
    "}\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "train_tokens = tokenizer.batch_encode_plus(train_sentences, **params)\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "valid_tokens = tokenizer.batch_encode_plus(valid_sentences, **params)\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "test_tokens = tokenizer.batch_encode_plus(test_sentences, **params)\n",
    "\n",
    "# hot encoder for speakers\n",
    "switcher = {\n",
    "    \"PM\" : [1,0,0,0],\n",
    "    \"ME\" : [0,1,0,0],\n",
    "    \"UI\" : [0,0,1,0],\n",
    "    \"ID\" : [0,0,0,1]\n",
    "}\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(train_tokens['input_ids'])\n",
    "train_mask = torch.tensor(train_tokens['attention_mask'])\n",
    "train_speaker = torch.Tensor([switcher[el] for el in train['speakers']]).to(device)\n",
    "train_len = torch.Tensor([[len(sentence.split())] for sentence in train['sentences']]).to(device)\n",
    "train_y = torch.tensor(train['labels'].to_numpy())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(valid_tokens['input_ids'])\n",
    "val_mask = torch.tensor(valid_tokens['attention_mask'])\n",
    "valid_speaker = torch.Tensor([switcher[el] for el in valid['speakers']]).to(device)\n",
    "valid_len = torch.Tensor([[len(sentence.split())] for sentence in valid['sentences']]).to(device)\n",
    "valid_y = torch.tensor(valid['labels'].to_numpy())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(test_tokens['input_ids'])\n",
    "test_mask = torch.tensor(test_tokens['attention_mask'])\n",
    "test_speaker = torch.Tensor([switcher[el] for el in test['speakers']]).to(device)\n",
    "test_len = torch.Tensor([[len(sentence.split())] for sentence in test['sentences']]).to(device)\n",
    "test_y = torch.tensor(test['labels'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WordFeatureDataset(Dataset):\n",
    "    def __init__(self, sequences, attention_masks, speakers, lengths, labels):\n",
    "        self.sequences = sequences\n",
    "        self.attention_masks = attention_masks\n",
    "        self.speakers = speakers\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        speaker = self.speakers[idx]\n",
    "        length = self.lengths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = {\n",
    "            'sequence': sequence,\n",
    "            'attention_mask': attention_mask,\n",
    "            'speaker': speaker,\n",
    "            'length': length,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def data_loader(batch_size):\n",
    "    # create tensor datasets\n",
    "    train_dataset = WordFeatureDataset(train_seq, train_mask, train_speaker, train_len, train_y)\n",
    "    valid_dataset = WordFeatureDataset(val_seq, val_mask, valid_speaker, valid_len, valid_y)\n",
    "    test_dataset = WordFeatureDataset(test_seq, test_mask, test_speaker, test_len, test_y)\n",
    "\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(params):\n",
    "    # Model\n",
    "    n_layers = params['n_layers']\n",
    "    layers = []\n",
    "\n",
    "    in_features = params['input_size']\n",
    "    for i in range(n_layers):\n",
    "        out_features = params[f'n_{i}_size']\n",
    "        layers.append(torch.nn.Linear(in_features, out_features))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # suggest dropout\n",
    "        p = params['n_p']\n",
    "        layers.append(torch.nn.Dropout(p))\n",
    "\n",
    "        # updating next layer size\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(torch.nn.Linear(in_features, params['output_size']))\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "class MLP_Bert(nn.Module):\n",
    "    def __init__(self, bert, params):\n",
    "        super(MLP_Bert, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.mlp = MLP(params)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, seq, mask, speakers, lengths):\n",
    "\n",
    "        # pass the inputs to the model  \n",
    "        outputs = self.bert(seq, attention_mask=mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Average pooling across the entire sequence\n",
    "        avg_pooled = torch.mean(hidden_states, dim=1)\n",
    "        \n",
    "        cls_input = torch.cat((avg_pooled, speakers, lengths), dim=1)\n",
    "        x = self.mlp(cls_input) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP_Bert(nn.Module):\n",
    "#     def __init__(self, bert):\n",
    "#         super(MLP_Bert, self).__init__()\n",
    "\n",
    "#         self.bert = bert \n",
    "        \n",
    "#         # dropout layer\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "#         # relu activation function\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#         # dense layer 1\n",
    "#         self.fc1 = nn.Linear(768, 512)\n",
    "        \n",
    "#         # dense layer 2 (Output layer)\n",
    "#         self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "#     # define the forward pass\n",
    "#     def forward(self, sent_id, mask):\n",
    "\n",
    "#         # pass the inputs to the model  \n",
    "#         outputs = self.bert(sent_id, attention_mask=mask)\n",
    "#         hidden_states = outputs.last_hidden_state\n",
    "\n",
    "#         # Average pooling across the entire sequence\n",
    "#         avg_pooled = torch.mean(hidden_states, dim=1)\n",
    "\n",
    "#         x = self.fc1(avg_pooled)\n",
    "\n",
    "#         x = self.relu(x)\n",
    "\n",
    "#         x = self.dropout(x)\n",
    "\n",
    "#         # output layer\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchmetrics.classification import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = json.load(open(\"models/mlp_results_232.json\", \"r\"))[\"params\"]\n",
    "# params['input_size'] = bert.config.hidden_size + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        \"n_layers\" : trial.suggest_int(\"n_layers\", 2, 4),\n",
    "        \"input_size\" : bert.config.hidden_size + 4 + 1,\n",
    "        \"output_size\" : 2,\n",
    "        \"n_p\" : trial.suggest_float(\"n_p\", 0.5, 0.8),\n",
    "        \"lr\" : trial.suggest_float(\"lr\", 1e-4, 1e-3),\n",
    "        \"weight_decay\" : trial.suggest_float(\"weight_decay\", 1e-5, 1e-4),\n",
    "        \"batch_size\" : 250\n",
    "    }\n",
    "    for i in range(trial.params[\"n_layers\"]):\n",
    "        params[f\"n_{i}_size\"] = trial.suggest_int(f\"n_{i}_size\", 200, 800)\n",
    "\n",
    "    model = MLP_Bert(bert, params).to(device)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train['labels'].to_numpy()), y=train['labels'].to_numpy())\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float()) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "    f1 = F1Score(task='binary', num_classes=2).to(device)\n",
    "\n",
    "    train_loader, valid_loader, _ = data_loader(params[\"batch_size\"])\n",
    "\n",
    "    n_epochs = 20\n",
    "    it = 0\n",
    "    hst_train_loss = [] \n",
    "    hst_valid_loss = []\n",
    "    hst_f1_score = []\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    patience = 10\n",
    "\n",
    "    # itera nas epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch > 0:\n",
    "            break\n",
    "        if patience == 0: break\n",
    "        \n",
    "        # itera nos train batches\n",
    "        for idx, samples in enumerate(train_loader):\n",
    "            if patience == 0: break\n",
    "            it += 1\n",
    "\n",
    "            # train step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "            loss = criterion(out, samples['label'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = loss.cpu().detach().numpy() / 1\n",
    "            \n",
    "            if it % 25 == 0:\n",
    "                print(f\"{it} : {train_loss}\")\n",
    "\n",
    "            if it % 75 == 0:\n",
    "                model.eval()\n",
    "\n",
    "                valid_loss = 0\n",
    "                f1_score = 0\n",
    "                \n",
    "                # itera nos valid batches\n",
    "                for idx, samples in enumerate(valid_loader):\n",
    "                    out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "                    loss = criterion(out, samples['label'])\n",
    "                    valid_loss += loss.cpu().detach().numpy() / len(valid_loader)\n",
    "                    f1_score += f1(samples['label'], out.argmax(dim=1)).cpu().detach().numpy() / len(valid_loader)\n",
    "                \n",
    "                # early stopping\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    best_weights = model.state_dict()\n",
    "                    patience = 10\n",
    "                else:\n",
    "                    patience -= 1 \n",
    "                \n",
    "                hst_train_loss.append(train_loss)\n",
    "                hst_valid_loss.append(valid_loss)\n",
    "                hst_f1_score.append(f1_score)\n",
    "\n",
    "                print('Iter: {} | Train Loss: {} | Val Loss: {} | F1-score: {}'.format(it, train_loss, valid_loss, f1_score))\n",
    "\n",
    "    # objective function criterion\n",
    "    combined = sorted(zip(hst_valid_loss, hst_f1_score), key=lambda x : x[0])\n",
    "    _, scores = zip(*combined)\n",
    "    qtd = 3\n",
    "    final_score = sum(scores[:qtd]) / qtd\n",
    "\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 19:42:46,175] A new study created in memory with name: no-name-c150634f-00ca-4c64-a023-0b5e82c5d463\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
