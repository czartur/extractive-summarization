{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2021/artur.araujo-alves/code/extractive-summarization/env/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import optuna\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import custom_utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn\n",
    "from torchmetrics.classification import F1Score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 52288\n",
      "Test: 14525\n",
      "Valid: 5810\n"
     ]
    }
   ],
   "source": [
    "# read\n",
    "sentences, speakers, labels = custom_utils.read_data(\"training\", \"training_labels.json\")\n",
    "\n",
    "# split\n",
    "df = pd.DataFrame({\"sentences\" : sentences, \"speakers\" : speakers, \"labels\" : labels})\n",
    "df['l-sentences'] = df['sentences'].shift(1)\n",
    "df['r-sentences'] = df['sentences'].shift(-1)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=69, stratify=df.labels)\n",
    "train, valid = train_test_split(train, test_size=0.1, random_state=69, stratify=train.labels)\n",
    "\n",
    "print(f\"Train: {len(train)}\\nTest: {len(test)}\\nValid: {len(valid)}\")\n",
    "\n",
    "# Getting setences list\n",
    "train_sentences = train['sentences'].to_list()\n",
    "train_r_sentences = train['r-sentences'].to_list()\n",
    "train_l_sentences = train['l-sentences'].to_list()\n",
    "\n",
    "valid_sentences = valid['sentences'].to_list()\n",
    "valid_r_sentences = valid['r-sentences'].to_list()\n",
    "valid_l_sentences = valid['l-sentences'].to_list()\n",
    "\n",
    "test_sentences = test['sentences'].to_list()\n",
    "test_r_sentences = test['r-sentences'].to_list()\n",
    "test_l_sentences = test['l-sentences'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "params = {\n",
    "    'max_length' : max_seq_len,\n",
    "    'padding' : True,\n",
    "    'truncation' : True,\n",
    "    'return_token_type_ids' : False\n",
    "}\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "train_tokens = tokenizer.batch_encode_plus(train_sentences, **params)\n",
    "train_l_tokens = tokenizer.batch_encode_plus(train_sentences, **params)\n",
    "train_r_tokens = tokenizer.batch_encode_plus(train_sentences, **params)\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "valid_tokens = tokenizer.batch_encode_plus(valid_sentences, **params)\n",
    "valid_l_tokens = tokenizer.batch_encode_plus(valid_l_sentences, **params)\n",
    "valid_r_tokens = tokenizer.batch_encode_plus(valid_r_sentences, **params)\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "test_tokens = tokenizer.batch_encode_plus(test_sentences, **params)\n",
    "test_l_tokens = tokenizer.batch_encode_plus(test_l_sentences, **params)\n",
    "test_r_tokens = tokenizer.batch_encode_plus(test_r_sentences, **params)\n",
    "\n",
    "# hot encoder for speakers\n",
    "switcher = {\n",
    "    \"PM\" : [1,0,0,0],\n",
    "    \"ME\" : [0,1,0,0],\n",
    "    \"UI\" : [0,0,1,0],\n",
    "    \"ID\" : [0,0,0,1]\n",
    "}\n",
    "\n",
    "# for train set\n",
    "train_seq = {\n",
    "    \"mid\" : torch.tensor(train_tokens['input_ids']),\n",
    "    \"l\" : torch.tensor(train_l_tokens['input_ids']),\n",
    "    \"r\" : torch.tensor(train_r_tokens['input_ids'])\n",
    "}\n",
    "train_mask = {\n",
    "    \"mid\" : torch.tensor(train_tokens['attention_mask']),\n",
    "    \"l\" : torch.tensor(train_l_tokens['attention_mask']),\n",
    "    \"r\" : torch.tensor(train_r_tokens['attention_mask'])\n",
    "}\n",
    "train_speaker = torch.Tensor([switcher[el] for el in train['speakers']]).to(device)\n",
    "train_len = torch.Tensor([[len(sentence.split())] for sentence in train['sentences']]).to(device)\n",
    "train_y = torch.tensor(train['labels'].to_numpy())\n",
    "\n",
    "# for validation set\n",
    "valid_seq = {\n",
    "    \"mid\": torch.tensor(valid_tokens['input_ids']),\n",
    "    \"l\": torch.tensor(valid_l_tokens['input_ids']),\n",
    "    \"r\": torch.tensor(valid_r_tokens['input_ids'])\n",
    "}\n",
    "valid_mask = {\n",
    "    \"mid\": torch.tensor(valid_tokens['attention_mask']),\n",
    "    \"l\": torch.tensor(valid_l_tokens['attention_mask']),\n",
    "    \"r\": torch.tensor(valid_r_tokens['attention_mask'])\n",
    "}\n",
    "valid_speaker = torch.Tensor([switcher[el] for el in valid['speakers']]).to(device)\n",
    "valid_len = torch.Tensor([[len(sentence.split())] for sentence in valid['sentences']]).to(device)\n",
    "valid_y = torch.tensor(valid['labels'].to_numpy())\n",
    "\n",
    "# for test set\n",
    "test_seq = {\n",
    "    \"mid\": torch.tensor(test_tokens['input_ids']),\n",
    "    \"l\": torch.tensor(test_l_tokens['input_ids']),\n",
    "    \"r\": torch.tensor(test_r_tokens['input_ids'])\n",
    "}\n",
    "test_mask = {\n",
    "    \"mid\": torch.tensor(test_tokens['attention_mask']),\n",
    "    \"l\": torch.tensor(test_l_tokens['attention_mask']),\n",
    "    \"r\": torch.tensor(test_r_tokens['attention_mask'])\n",
    "}\n",
    "test_speaker = torch.Tensor([switcher[el] for el in test['speakers']]).to(device)\n",
    "test_len = torch.Tensor([[len(sentence.split())] for sentence in test['sentences']]).to(device)\n",
    "test_y = torch.tensor(test['labels'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordFeatureDataset(Dataset):\n",
    "    def __init__(self, sequences, attention_masks, speakers, lengths, labels):\n",
    "        self.sequences = sequences\n",
    "        self.attention_masks = attention_masks\n",
    "        self.speakers = speakers\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = {}\n",
    "        for key in self.sequences:\n",
    "            sequence[key] = self.sequences[key][idx]\n",
    "\n",
    "        attention_mask = {}\n",
    "        for key in self.attention_masks:\n",
    "            attention_mask[key] = self.attention_masks[key][idx]\n",
    "\n",
    "        speaker = self.speakers[idx]\n",
    "        length = self.lengths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = {\n",
    "            'sequence': sequence,\n",
    "            'attention_mask': attention_mask,\n",
    "            'speaker': speaker,\n",
    "            'length': length,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size):\n",
    "    # create tensor datasets\n",
    "    train_dataset = WordFeatureDataset(train_seq, train_mask, train_speaker, train_len, train_y)\n",
    "    valid_dataset = WordFeatureDataset(valid_seq, valid_mask, valid_speaker, valid_len, valid_y)\n",
    "    test_dataset = WordFeatureDataset(test_seq, test_mask, test_speaker, test_len, test_y)\n",
    "\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, generator=torch.Generator(device=device))\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(params):\n",
    "    # Model\n",
    "    n_layers = params['n_layers']\n",
    "    layers = []\n",
    "\n",
    "    in_features = params['input_size']\n",
    "    for i in range(n_layers):\n",
    "        out_features = params[f'n_{i}_size']\n",
    "        layers.append(torch.nn.Linear(in_features, out_features))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # suggest dropout\n",
    "        p = params['n_p']\n",
    "        layers.append(torch.nn.Dropout(p))\n",
    "\n",
    "        # updating next layer size\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(torch.nn.Linear(in_features, params['output_size']))\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "class MLP_Bert(nn.Module):\n",
    "    def __init__(self, bert, params):\n",
    "        super(MLP_Bert, self).__init__()\n",
    "\n",
    "        self.bert = {\n",
    "            \"mid\" : AutoModel.from_pretrained('bert-base-uncased'),\n",
    "            \"l\" : AutoModel.from_pretrained('bert-base-uncased'),\n",
    "            \"r\" : AutoModel.from_pretrained('bert-base-uncased')\n",
    "        }\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.mlp = MLP(params)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, seq, mask, speakers, lengths):\n",
    "        out = {}\n",
    "        for key in self.bert:\n",
    "            outputs = self.bert[key](seq[key], attention_mask=mask[key])\n",
    "            out[key] = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "        bert_output = torch.cat(tuple(out.values()), dim=1)\n",
    "        fc_input = torch.cat((bert_output, speakers, lengths), dim=1)\n",
    "        x = self.mlp(fc_input) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_epochs = 20\n",
    "    eval_at = 300\n",
    "    \n",
    "    params = {\n",
    "        # \"n_layers\" : trial.suggest_int(\"n_layers\", 2, 5),\n",
    "        \"n_layers\" : 3,\n",
    "        \"input_size\" : 3*bert.config.hidden_size + 4 + 1,\n",
    "        \"output_size\" : 2,\n",
    "        \"n_p\" : trial.suggest_float(\"n_p\", 0.2, 0.7),\n",
    "        \"lr\" : trial.suggest_float(\"lr\", 1e-5, 1e-4),\n",
    "        \"weight_decay\" : trial.suggest_float(\"weight_decay\", 1e-5, 1e-4),\n",
    "        \"batch_size\" : 30\n",
    "    }\n",
    "    for i in range(params[\"n_layers\"]):\n",
    "        params[f\"n_{i}_size\"] = trial.suggest_int(f\"n_{i}_size\", 200, 800)\n",
    "\n",
    "    model = MLP_Bert(bert, params).to(device)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train['labels'].to_numpy()), y=train['labels'].to_numpy())\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float()) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "    f1 = F1Score(task='binary', num_classes=2).to(device)\n",
    "\n",
    "    train_loader, valid_loader, _ = data_loader(params[\"batch_size\"])\n",
    "\n",
    "   \n",
    "    it = 0\n",
    "    hst_train_loss = [] \n",
    "    hst_valid_loss = []\n",
    "    hst_f1_score = []\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    patience = 10\n",
    "    \n",
    "    # itera nas epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        if patience == 0: break\n",
    "        \n",
    "        # itera nos train batches\n",
    "        for idx, samples in enumerate(train_loader):\n",
    "            if patience == 0: break\n",
    "            it += 1\n",
    "\n",
    "            # train step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "            loss = criterion(out, samples['label'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = loss.cpu().detach().numpy() / 1\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(idx, train_loss)\n",
    "            if it % eval_at == 0:\n",
    "                model.eval()\n",
    "\n",
    "                valid_loss = 0\n",
    "                f1_score = 0\n",
    "                \n",
    "                # itera nos valid batches\n",
    "                for idx, samples in enumerate(valid_loader):\n",
    "                    out = model(samples['sequence'], samples['attention_mask'], samples['speaker'], samples['length'])\n",
    "                    loss = criterion(out, samples['label'])\n",
    "                    valid_loss += loss.cpu().detach().numpy() / len(valid_loader)\n",
    "                    f1_score += f1(samples['label'], out.argmax(dim=1)).cpu().detach().numpy() / len(valid_loader)\n",
    "                \n",
    "                # early stopping\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    best_weights = model.state_dict()\n",
    "                    patience = 10\n",
    "                else:\n",
    "                    patience -= 1 \n",
    "                \n",
    "                hst_train_loss.append(train_loss)\n",
    "                hst_valid_loss.append(valid_loss)\n",
    "                hst_f1_score.append(f1_score)\n",
    "\n",
    "                print('Iter: {} | Train Loss: {} | Val Loss: {} | F1-score: {}'.format(it, train_loss, valid_loss, f1_score))\n",
    "\n",
    "    # objective function criterion\n",
    "    combined = sorted(zip(hst_valid_loss, hst_f1_score), key=lambda x : x[0])\n",
    "    _, scores = zip(*combined)\n",
    "    qtd = 3\n",
    "    final_score = sum(scores[:qtd]) / qtd\n",
    "\n",
    "    results = {\n",
    "        \"score\" : final_score,\n",
    "        \"params\" : params,\n",
    "        \"valid_loss\" : hst_valid_loss,\n",
    "        \"train_loss\" : hst_train_loss,\n",
    "        \"f1_score\" : hst_f1_score, \n",
    "    }\n",
    "    json.dump(results, open(f\"models/mlp_results_{trial.number}.json\", \"w\"))\n",
    "    \n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 15:39:04,874] A new study created in memory with name: no-name-a8a08499-52da-4100-82cd-55d6dec3f191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.662815272808075\n",
      "199 0.5843757390975952\n",
      "299 0.47338905930519104\n",
      "Iter: 300 | Train Loss: 0.47338905930519104 | Val Loss: 0.565304029233677 | F1-score: 0.4934636928986024\n",
      "399 0.4944024384021759\n",
      "499 0.5007961392402649\n",
      "599 0.3792397975921631\n",
      "Iter: 600 | Train Loss: 0.3792397975921631 | Val Loss: 0.5365272754246425 | F1-score: 0.4903187608903213\n",
      "699 0.2982766628265381\n",
      "799 0.3855109214782715\n",
      "899 0.46472597122192383\n",
      "Iter: 900 | Train Loss: 0.46472597122192383 | Val Loss: 0.5229114019071933 | F1-score: 0.5022879118848703\n",
      "999 0.7689629197120667\n",
      "1099 0.4054035246372223\n",
      "1199 0.5328714847564697\n",
      "Iter: 1200 | Train Loss: 0.5328714847564697 | Val Loss: 0.5319560886043865 | F1-score: 0.49330339734394485\n",
      "1299 0.6226460933685303\n",
      "1399 0.5811951160430908\n",
      "1499 0.35184311866760254\n",
      "Iter: 1500 | Train Loss: 0.35184311866760254 | Val Loss: 0.5101152317425641 | F1-score: 0.49932992957609174\n",
      "1599 0.4939771592617035\n",
      "1699 0.4182641804218292\n",
      "56 0.39936646819114685\n",
      "Iter: 1800 | Train Loss: 0.39936646819114685 | Val Loss: 0.515652038080176 | F1-score: 0.510291309623988\n",
      "156 0.39580774307250977\n",
      "256 0.46790075302124023\n",
      "356 0.42325443029403687\n",
      "Iter: 2100 | Train Loss: 0.42325443029403687 | Val Loss: 0.5202154875108878 | F1-score: 0.514856201394931\n",
      "456 0.3298032581806183\n",
      "556 0.41631007194519043\n",
      "656 0.2834770083427429\n",
      "Iter: 2400 | Train Loss: 0.2834770083427429 | Val Loss: 0.5070213256115764 | F1-score: 0.5206986608118124\n",
      "756 0.4209057688713074\n",
      "856 0.4152618944644928\n",
      "956 0.41778066754341125\n",
      "Iter: 2700 | Train Loss: 0.41778066754341125 | Val Loss: 0.5125555968315331 | F1-score: 0.5168132538494372\n",
      "1056 0.2942977547645569\n",
      "1156 0.4571555554866791\n",
      "1256 0.45002099871635437\n",
      "Iter: 3000 | Train Loss: 0.45002099871635437 | Val Loss: 0.5133709043418012 | F1-score: 0.5119255234285722\n",
      "1356 0.45972612500190735\n",
      "1456 0.5230240225791931\n",
      "1556 0.5437435507774353\n",
      "Iter: 3300 | Train Loss: 0.5437435507774353 | Val Loss: 0.4973424738369036 | F1-score: 0.5119692844521138\n",
      "1656 0.32266202569007874\n",
      "13 0.3805907964706421\n",
      "113 0.4498802423477173\n",
      "Iter: 3600 | Train Loss: 0.4498802423477173 | Val Loss: 0.5002740773650791 | F1-score: 0.521023682405039\n",
      "213 0.5214717388153076\n",
      "313 0.5386159420013428\n",
      "413 0.44562965631484985\n",
      "Iter: 3900 | Train Loss: 0.44562965631484985 | Val Loss: 0.5103759380038254 | F1-score: 0.519717683573973\n",
      "513 0.44897782802581787\n",
      "613 0.50444096326828\n",
      "713 0.3579191565513611\n",
      "Iter: 4200 | Train Loss: 0.3579191565513611 | Val Loss: 0.5049063506507382 | F1-score: 0.5210624460092522\n",
      "813 0.294808954000473\n",
      "913 0.4223398268222809\n",
      "1013 0.34004655480384827\n",
      "Iter: 4500 | Train Loss: 0.34004655480384827 | Val Loss: 0.503952773959981 | F1-score: 0.5214922873629735\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "format_input() missing 1 required positional argument: 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/users/eleves-b/2021/artur.araujo-alves/code/extractive-summarization/MLP_v4.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506f72747567616c2d58227d/users/eleves-b/2021/artur.araujo-alves/code/extractive-summarization/MLP_v4.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m test_data \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506f72747567616c2d58227d/users/eleves-b/2021/artur.araujo-alves/code/extractive-summarization/MLP_v4.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m test_sentences:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506f72747567616c2d58227d/users/eleves-b/2021/artur.araujo-alves/code/extractive-summarization/MLP_v4.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     test_data[\u001b[39mid\u001b[39m] \u001b[39m=\u001b[39m custom_utils\u001b[39m.\u001b[39;49mformat_input(test_sentences[\u001b[39mid\u001b[39;49m], test_speakers[\u001b[39mid\u001b[39;49m],t_params\u001b[39m=\u001b[39;49mparams)\n",
      "\u001b[0;31mTypeError\u001b[0m: format_input() missing 1 required positional argument: 'tokenizer'"
     ]
    }
   ],
   "source": [
    "test_sentences, test_speakers, _  = custom_utils.read_data_by_ID(\"test\", combine = False)\n",
    "\n",
    "# tokenization\n",
    "params = {\n",
    "    'max_length' : 80,\n",
    "    'padding' : True,\n",
    "    'truncation' : True,\n",
    "    'return_token_type_ids' : False\n",
    "}\n",
    "\n",
    "test_data = {}\n",
    "for id in test_sentences:\n",
    "    test_data[id] = custom_utils.format_input(test_sentences[id], test_speakers[id], t_params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
