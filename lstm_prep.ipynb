{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "model_path = \"tokenizer.model\"\n",
    "model_name = \"glove-wiki-gigaword-300\"\n",
    "\n",
    "# load model (and save if necessary)\n",
    "try:\n",
    "    tokenizer = KeyedVectors.load(model_path)\n",
    "except FileNotFoundError:\n",
    "    tokenizer = api.load(model_name)\n",
    "    tokenizer.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence, tokenizer):\n",
    "    embeddings = []\n",
    "    length = 0\n",
    "    for word in sentence.split():\n",
    "        word = word.lower()\n",
    "        if not word in tokenizer: continue\n",
    "        length += 1\n",
    "        embeddings.append(tokenizer[word])\n",
    "    \n",
    "    if len(embeddings) == 0: # bug fix for padding function (we need to asssure at least one element)\n",
    "        embeddings = np.zeros(shape=(1,tokenizer.vector_size))\n",
    "        length = 1\n",
    "    return torch.tensor(np.asarray(embeddings)), length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# read data\n",
    "def read_data(dialogs_folder = str, labels_file = str):\n",
    "    sentences = []\n",
    "    speakers = []\n",
    "    labels = []\n",
    "    if labels_file:\n",
    "        label_data = json.load(open(labels_file, \"r\"))\n",
    "    for item in Path(dialogs_folder).iterdir():\n",
    "        if not item.is_file(): continue \n",
    "        if not item.suffix == \".json\": continue\n",
    "        \n",
    "        # load data\n",
    "        dialog = json.load(open(item, \"r\"))\n",
    "        sentences += [exchange[\"text\"] for exchange in dialog]\n",
    "        speakers += [exchange[\"speaker\"] for exchange in dialog]\n",
    "        if labels_file:\n",
    "            labels += [val for val in label_data[item.stem]]\n",
    "    if labels_file:\n",
    "        return sentences, speakers, labels\n",
    "    else:\n",
    "        return sentences, speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "sentences, speakers, labels = read_data(\"training\", \"training_labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72623 72623 72623\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(speakers), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "switcher = {\n",
    "        \"PM\" : [1,0,0,0],\n",
    "        \"ME\" : [0,1,0,0],\n",
    "        \"UI\" : [0,0,1,0],\n",
    "        \"ID\" : [0,0,0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3625, 0.4869, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1996, 0.5787, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9791, 0.3932, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.6686, 0.7506, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.8596, 0.9174, 0.0000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.6687, 0.9737, 0.0000, 1.0000, 0.0000, 0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor([switcher[sp] for sp in speakers])\n",
    "\n",
    "ht = torch.rand(size=(len(speakers),2))\n",
    "\n",
    "torch.cat([ht, test], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size + len(switcher), output_size)\n",
    "\n",
    "    def forward(self, sentences, speakers):\n",
    "        # from sentences to sequences of vectors (embedding)\n",
    "        embedded_sentences, lengths = list(zip(*[embed_sentence(sentence, tokenizer) for sentence in sentences]))\n",
    "\n",
    "        # pack / pad sequences (save memory) \n",
    "        packed_sentences = pack_padded_sequence(pad_sequence(embedded_sentences, batch_first=True), lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # hot encode speakers\n",
    "        encoded_speakers = torch.tensor([switcher[speaker] for speaker in speakers])\n",
    "\n",
    "        # send to device \n",
    "        packed_sentences.to(device)\n",
    "        encoded_speakers.to(device)\n",
    "        \n",
    "        # lstm layer\n",
    "        _, (ht,_) = self.lstm(packed_sentences) # it does accept packed sequences \n",
    "\n",
    "        # linear f.c. layer \n",
    "        output = self.fc(torch.cat([ht[-1], encoded_speakers], dim=1)) # use only output from the last hidden state\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "num_classes = 2\n",
    "num_features = tokenizer.vector_size # vector size of word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sentences, speakers, labels, model, criterion, optimizer):\n",
    "    assert(len(labels) == len(sentences))\n",
    "    model.train() \n",
    "    optimizer.zero_grad()\n",
    "    out = model(sentences, speakers)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def validate(sentences, speakers, labels, model, criterion):\n",
    "    assert(len(labels) == len(sentences))\n",
    "    model.eval()\n",
    "    out = model(sentences, speakers)\n",
    "    loss = criterion(out, labels)\n",
    "    pred_labels = out.argmax(dim=1)\n",
    "    f1 = F1Score(task='binary', num_classes=num_classes)\n",
    "    score = f1(labels, pred_labels)\n",
    "    return loss, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    n_folds = 5\n",
    "    n_epochs = 200\n",
    "    patience = 10\n",
    "    avg_score = 0\n",
    "    \n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(sentences, labels)):\n",
    "        # split data\n",
    "        train_labels = torch.tensor([labels[i] for i in train_idx])\n",
    "        valid_labels = torch.tensor([labels[i] for i in valid_idx])\n",
    "        \n",
    "        train_sentences = [sentences[i] for i in train_idx]\n",
    "        valid_sentences = [sentences[i] for i in valid_idx]\n",
    "\n",
    "        train_speakers = [speakers[i] for i in train_idx]\n",
    "        valid_speakers = [speakers[i] for i in valid_idx]\n",
    "        \n",
    "        # set model, criterion and optimizers\n",
    "        # 1. parameters\n",
    "        hidden_size = trial.suggest_int(f'hidden_size', 16, 256)\n",
    "        # hidden_size = 64\n",
    "        # lr = trial.suggest_float(f'lr', 1e-3, 1e-1)\n",
    "        lr = 0.01\n",
    "        # weight_decay = trial.suggest_float(f'weight_decay', 1e-5, 1e-2)\n",
    "        weight_decay = 5e-4\n",
    "        # 2. objects\n",
    "        model = LSTMClassifier(num_features, hidden_size, num_classes).to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss() # need to check input\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        best_weights = model.state_dict()\n",
    "\n",
    "        # Epoch\n",
    "        best_valid_loss = float('inf')\n",
    "        score_at_best = -1\n",
    "        current_patience = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = train(train_sentences, train_speakers, train_labels, model, criterion, optimizer)\n",
    "            valid_loss, score = validate(valid_sentences, valid_speakers, valid_labels, model, criterion)\n",
    "            \n",
    "            # Stopping criteria            \n",
    "            if valid_loss > best_valid_loss:\n",
    "                current_patience += 1\n",
    "            else:\n",
    "                best_weights = model.state_dict()\n",
    "                best_valid_loss = valid_loss    \n",
    "                score_at_best = score \n",
    "                current_patience = 0\n",
    "            \n",
    "            if current_patience == patience:\n",
    "                break\n",
    "            \n",
    "            print(f'Fold: {fold}, Epoch: {epoch}, Train loss: {train_loss:.4f}, Valid loss: {valid_loss:.4f}, Score: {score:.4f}')\n",
    "        \n",
    "        avg_score += score_at_best/n_folds\n",
    "        \n",
    "        # save model and params \n",
    "        torch.save(best_weights, f\"models/lstm_{trial.number}_{fold}.pt\")\n",
    "        json.dump(trial.params, open(f\"models/params_{trial.number}.json\", \"w\"))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 19:38:11,929] A new study created in memory with name: no-name-98711a11-c64f-4168-8b0d-a6ddbfa2d4f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0, Epoch: 0, Train loss: 0.6883, Valid loss: 0.4992, Score: 0.0007\n",
      "Fold: 0, Epoch: 1, Train loss: 0.4995, Valid loss: 0.4934, Score: 0.0000\n",
      "Fold: 0, Epoch: 2, Train loss: 0.4923, Valid loss: 0.4581, Score: 0.0007\n",
      "Fold: 0, Epoch: 3, Train loss: 0.4562, Valid loss: 0.4442, Score: 0.0368\n",
      "Fold: 0, Epoch: 4, Train loss: 0.4408, Valid loss: 0.4414, Score: 0.1007\n",
      "Fold: 0, Epoch: 5, Train loss: 0.4364, Valid loss: 0.4214, Score: 0.1160\n",
      "Fold: 0, Epoch: 6, Train loss: 0.4156, Valid loss: 0.3977, Score: 0.1075\n",
      "Fold: 0, Epoch: 7, Train loss: 0.3921, Valid loss: 0.3840, Score: 0.1251\n",
      "Fold: 0, Epoch: 8, Train loss: 0.3792, Valid loss: 0.3698, Score: 0.2274\n",
      "Fold: 0, Epoch: 9, Train loss: 0.3658, Valid loss: 0.3631, Score: 0.4259\n",
      "Fold: 0, Epoch: 10, Train loss: 0.3581, Valid loss: 0.3598, Score: 0.4577\n",
      "Fold: 0, Epoch: 11, Train loss: 0.3551, Valid loss: 0.3516, Score: 0.2801\n",
      "Fold: 0, Epoch: 12, Train loss: 0.3503, Valid loss: 0.3507, Score: 0.1310\n",
      "Fold: 0, Epoch: 13, Train loss: 0.3502, Valid loss: 0.3476, Score: 0.3154\n",
      "Fold: 0, Epoch: 14, Train loss: 0.3442, Valid loss: 0.3506, Score: 0.4624\n",
      "Fold: 0, Epoch: 15, Train loss: 0.3448, Valid loss: 0.3410, Score: 0.3540\n",
      "Fold: 0, Epoch: 16, Train loss: 0.3384, Valid loss: 0.3384, Score: 0.3322\n",
      "Fold: 0, Epoch: 17, Train loss: 0.3385, Valid loss: 0.3398, Score: 0.5271\n",
      "Fold: 0, Epoch: 18, Train loss: 0.3356, Valid loss: 0.3369, Score: 0.5202\n",
      "Fold: 0, Epoch: 19, Train loss: 0.3336, Valid loss: 0.3344, Score: 0.4228\n",
      "Fold: 0, Epoch: 20, Train loss: 0.3344, Valid loss: 0.3360, Score: 0.5079\n",
      "Fold: 0, Epoch: 21, Train loss: 0.3309, Valid loss: 0.3383, Score: 0.5167\n",
      "Fold: 0, Epoch: 22, Train loss: 0.3314, Valid loss: 0.3342, Score: 0.3863\n",
      "Fold: 0, Epoch: 23, Train loss: 0.3311, Valid loss: 0.3336, Score: 0.4347\n",
      "Fold: 0, Epoch: 24, Train loss: 0.3294, Valid loss: 0.3374, Score: 0.5292\n",
      "Fold: 0, Epoch: 25, Train loss: 0.3294, Valid loss: 0.3316, Score: 0.4958\n",
      "Fold: 0, Epoch: 26, Train loss: 0.3276, Valid loss: 0.3305, Score: 0.4818\n",
      "Fold: 0, Epoch: 27, Train loss: 0.3282, Valid loss: 0.3374, Score: 0.5483\n",
      "Fold: 0, Epoch: 28, Train loss: 0.3286, Valid loss: 0.3298, Score: 0.4725\n",
      "Fold: 0, Epoch: 29, Train loss: 0.3260, Valid loss: 0.3300, Score: 0.4614\n",
      "Fold: 0, Epoch: 30, Train loss: 0.3253, Valid loss: 0.3349, Score: 0.5314\n",
      "Fold: 0, Epoch: 31, Train loss: 0.3264, Valid loss: 0.3292, Score: 0.4250\n",
      "Fold: 0, Epoch: 32, Train loss: 0.3253, Valid loss: 0.3293, Score: 0.5146\n",
      "Fold: 0, Epoch: 33, Train loss: 0.3233, Valid loss: 0.3282, Score: 0.5230\n",
      "Fold: 0, Epoch: 34, Train loss: 0.3229, Valid loss: 0.3266, Score: 0.4673\n",
      "Fold: 0, Epoch: 35, Train loss: 0.3240, Valid loss: 0.3365, Score: 0.5563\n",
      "Fold: 0, Epoch: 36, Train loss: 0.3274, Valid loss: 0.3313, Score: 0.3399\n",
      "Fold: 0, Epoch: 37, Train loss: 0.3297, Valid loss: 0.3344, Score: 0.5434\n",
      "Fold: 0, Epoch: 38, Train loss: 0.3253, Valid loss: 0.3296, Score: 0.5121\n",
      "Fold: 0, Epoch: 39, Train loss: 0.3225, Valid loss: 0.3286, Score: 0.3430\n",
      "Fold: 0, Epoch: 40, Train loss: 0.3265, Valid loss: 0.3281, Score: 0.5210\n",
      "Fold: 0, Epoch: 41, Train loss: 0.3213, Valid loss: 0.3318, Score: 0.5487\n",
      "Fold: 0, Epoch: 42, Train loss: 0.3232, Valid loss: 0.3255, Score: 0.4601\n",
      "Fold: 0, Epoch: 43, Train loss: 0.3236, Valid loss: 0.3256, Score: 0.5183\n",
      "Fold: 0, Epoch: 44, Train loss: 0.3200, Valid loss: 0.3308, Score: 0.5471\n",
      "Fold: 0, Epoch: 45, Train loss: 0.3218, Valid loss: 0.3255, Score: 0.4502\n",
      "Fold: 0, Epoch: 46, Train loss: 0.3207, Valid loss: 0.3258, Score: 0.4790\n",
      "Fold: 0, Epoch: 47, Train loss: 0.3195, Valid loss: 0.3308, Score: 0.5402\n",
      "Fold: 0, Epoch: 48, Train loss: 0.3210, Valid loss: 0.3249, Score: 0.4713\n",
      "Fold: 0, Epoch: 49, Train loss: 0.3193, Valid loss: 0.3249, Score: 0.5071\n",
      "Fold: 0, Epoch: 50, Train loss: 0.3181, Valid loss: 0.3288, Score: 0.5442\n",
      "Fold: 0, Epoch: 51, Train loss: 0.3192, Valid loss: 0.3250, Score: 0.4501\n",
      "Fold: 0, Epoch: 52, Train loss: 0.3208, Valid loss: 0.3337, Score: 0.5530\n",
      "Fold: 0, Epoch: 53, Train loss: 0.3218, Valid loss: 0.3250, Score: 0.4298\n",
      "Fold: 0, Epoch: 54, Train loss: 0.3191, Valid loss: 0.3246, Score: 0.4731\n",
      "Fold: 0, Epoch: 55, Train loss: 0.3173, Valid loss: 0.3292, Score: 0.5431\n",
      "Fold: 0, Epoch: 56, Train loss: 0.3189, Valid loss: 0.3244, Score: 0.4348\n",
      "Fold: 0, Epoch: 57, Train loss: 0.3207, Valid loss: 0.3329, Score: 0.5601\n",
      "Fold: 0, Epoch: 58, Train loss: 0.3213, Valid loss: 0.3233, Score: 0.4453\n",
      "Fold: 0, Epoch: 59, Train loss: 0.3180, Valid loss: 0.3237, Score: 0.4983\n",
      "Fold: 0, Epoch: 60, Train loss: 0.3158, Valid loss: 0.3281, Score: 0.5389\n",
      "Fold: 0, Epoch: 61, Train loss: 0.3174, Valid loss: 0.3240, Score: 0.4370\n",
      "Fold: 0, Epoch: 62, Train loss: 0.3182, Valid loss: 0.3279, Score: 0.5449\n",
      "Fold: 0, Epoch: 63, Train loss: 0.3167, Valid loss: 0.3229, Score: 0.5029\n",
      "Fold: 0, Epoch: 64, Train loss: 0.3149, Valid loss: 0.3230, Score: 0.5022\n",
      "Fold: 0, Epoch: 65, Train loss: 0.3146, Valid loss: 0.3277, Score: 0.5421\n",
      "Fold: 0, Epoch: 66, Train loss: 0.3158, Valid loss: 0.3249, Score: 0.4233\n",
      "Fold: 0, Epoch: 67, Train loss: 0.3181, Valid loss: 0.3370, Score: 0.5607\n",
      "Fold: 0, Epoch: 68, Train loss: 0.3217, Valid loss: 0.3273, Score: 0.3525\n",
      "Fold: 0, Epoch: 69, Train loss: 0.3213, Valid loss: 0.3278, Score: 0.5363\n",
      "Fold: 0, Epoch: 70, Train loss: 0.3157, Valid loss: 0.3247, Score: 0.5282\n",
      "Fold: 0, Epoch: 71, Train loss: 0.3142, Valid loss: 0.3240, Score: 0.4334\n",
      "Fold: 0, Epoch: 72, Train loss: 0.3181, Valid loss: 0.3320, Score: 0.5606\n",
      "Fold: 0, Epoch: 73, Train loss: 0.3183, Valid loss: 0.3223, Score: 0.4800\n",
      "Fold: 0, Epoch: 74, Train loss: 0.3138, Valid loss: 0.3229, Score: 0.4671\n",
      "Fold: 0, Epoch: 75, Train loss: 0.3140, Valid loss: 0.3309, Score: 0.5483\n",
      "Fold: 0, Epoch: 76, Train loss: 0.3165, Valid loss: 0.3231, Score: 0.4627\n",
      "Fold: 0, Epoch: 77, Train loss: 0.3142, Valid loss: 0.3231, Score: 0.5117\n",
      "Fold: 0, Epoch: 78, Train loss: 0.3122, Valid loss: 0.3273, Score: 0.5432\n",
      "Fold: 0, Epoch: 79, Train loss: 0.3138, Valid loss: 0.3235, Score: 0.4489\n",
      "Fold: 0, Epoch: 80, Train loss: 0.3155, Valid loss: 0.3291, Score: 0.5464\n",
      "Fold: 0, Epoch: 81, Train loss: 0.3146, Valid loss: 0.3227, Score: 0.4740\n",
      "Fold: 0, Epoch: 82, Train loss: 0.3122, Valid loss: 0.3224, Score: 0.4794\n",
      "Fold: 1, Epoch: 0, Train loss: 0.7199, Valid loss: 0.5118, Score: 0.0029\n",
      "Fold: 1, Epoch: 1, Train loss: 0.5190, Valid loss: 0.4780, Score: 0.0037\n",
      "Fold: 1, Epoch: 2, Train loss: 0.4899, Valid loss: 0.4511, Score: 0.0235\n",
      "Fold: 1, Epoch: 3, Train loss: 0.4618, Valid loss: 0.4436, Score: 0.1081\n",
      "Fold: 1, Epoch: 4, Train loss: 0.4524, Valid loss: 0.4290, Score: 0.1671\n",
      "Fold: 1, Epoch: 5, Train loss: 0.4382, Valid loss: 0.4039, Score: 0.1850\n",
      "Fold: 1, Epoch: 6, Train loss: 0.4155, Valid loss: 0.3812, Score: 0.2313\n",
      "Fold: 1, Epoch: 7, Train loss: 0.3954, Valid loss: 0.3641, Score: 0.3876\n",
      "Fold: 1, Epoch: 8, Train loss: 0.3771, Valid loss: 0.3617, Score: 0.5258\n",
      "Fold: 1, Epoch: 9, Train loss: 0.3693, Valid loss: 0.3493, Score: 0.4352\n",
      "Fold: 1, Epoch: 10, Train loss: 0.3648, Valid loss: 0.3391, Score: 0.4816\n",
      "Fold: 1, Epoch: 11, Train loss: 0.3476, Valid loss: 0.3379, Score: 0.4428\n",
      "Fold: 1, Epoch: 12, Train loss: 0.3436, Valid loss: 0.3373, Score: 0.1506\n",
      "Fold: 1, Epoch: 13, Train loss: 0.3440, Valid loss: 0.3359, Score: 0.2172\n",
      "Fold: 1, Epoch: 14, Train loss: 0.3411, Valid loss: 0.3357, Score: 0.4645\n",
      "Fold: 1, Epoch: 15, Train loss: 0.3398, Valid loss: 0.3302, Score: 0.4201\n",
      "Fold: 1, Epoch: 16, Train loss: 0.3361, Valid loss: 0.3280, Score: 0.4837\n",
      "Fold: 1, Epoch: 17, Train loss: 0.3348, Valid loss: 0.3310, Score: 0.5453\n",
      "Fold: 1, Epoch: 18, Train loss: 0.3357, Valid loss: 0.3246, Score: 0.4838\n",
      "Fold: 1, Epoch: 19, Train loss: 0.3337, Valid loss: 0.3236, Score: 0.5032\n",
      "Fold: 1, Epoch: 20, Train loss: 0.3316, Valid loss: 0.3258, Score: 0.5291\n",
      "Fold: 1, Epoch: 21, Train loss: 0.3324, Valid loss: 0.3226, Score: 0.4503\n",
      "Fold: 1, Epoch: 22, Train loss: 0.3316, Valid loss: 0.3219, Score: 0.4875\n",
      "Fold: 1, Epoch: 23, Train loss: 0.3302, Valid loss: 0.3236, Score: 0.5429\n",
      "Fold: 1, Epoch: 24, Train loss: 0.3298, Valid loss: 0.3205, Score: 0.4967\n",
      "Fold: 1, Epoch: 25, Train loss: 0.3291, Valid loss: 0.3226, Score: 0.5482\n",
      "Fold: 1, Epoch: 26, Train loss: 0.3280, Valid loss: 0.3204, Score: 0.5206\n",
      "Fold: 1, Epoch: 27, Train loss: 0.3270, Valid loss: 0.3206, Score: 0.5267\n",
      "Fold: 1, Epoch: 28, Train loss: 0.3263, Valid loss: 0.3213, Score: 0.5341\n",
      "Fold: 1, Epoch: 29, Train loss: 0.3259, Valid loss: 0.3201, Score: 0.4867\n",
      "Fold: 1, Epoch: 30, Train loss: 0.3261, Valid loss: 0.3235, Score: 0.5509\n",
      "Fold: 1, Epoch: 31, Train loss: 0.3266, Valid loss: 0.3208, Score: 0.4323\n",
      "Fold: 1, Epoch: 32, Train loss: 0.3275, Valid loss: 0.3272, Score: 0.5693\n",
      "Fold: 1, Epoch: 33, Train loss: 0.3291, Valid loss: 0.3201, Score: 0.4386\n",
      "Fold: 1, Epoch: 34, Train loss: 0.3266, Valid loss: 0.3196, Score: 0.5294\n",
      "Fold: 1, Epoch: 35, Train loss: 0.3236, Valid loss: 0.3199, Score: 0.5356\n",
      "Fold: 1, Epoch: 36, Train loss: 0.3237, Valid loss: 0.3188, Score: 0.4508\n",
      "Fold: 1, Epoch: 37, Train loss: 0.3255, Valid loss: 0.3226, Score: 0.5649\n",
      "Fold: 1, Epoch: 38, Train loss: 0.3255, Valid loss: 0.3172, Score: 0.4896\n",
      "Fold: 1, Epoch: 39, Train loss: 0.3229, Valid loss: 0.3170, Score: 0.5027\n",
      "Fold: 1, Epoch: 40, Train loss: 0.3220, Valid loss: 0.3206, Score: 0.5543\n",
      "Fold: 1, Epoch: 41, Train loss: 0.3234, Valid loss: 0.3181, Score: 0.4534\n",
      "Fold: 1, Epoch: 42, Train loss: 0.3246, Valid loss: 0.3224, Score: 0.5659\n",
      "Fold: 1, Epoch: 43, Train loss: 0.3242, Valid loss: 0.3168, Score: 0.4814\n",
      "Fold: 1, Epoch: 44, Train loss: 0.3220, Valid loss: 0.3171, Score: 0.5237\n",
      "Fold: 1, Epoch: 45, Train loss: 0.3206, Valid loss: 0.3185, Score: 0.5471\n",
      "Fold: 1, Epoch: 46, Train loss: 0.3211, Valid loss: 0.3172, Score: 0.4571\n",
      "Fold: 1, Epoch: 47, Train loss: 0.3228, Valid loss: 0.3254, Score: 0.5777\n",
      "Fold: 1, Epoch: 48, Train loss: 0.3262, Valid loss: 0.3212, Score: 0.3880\n",
      "Fold: 1, Epoch: 49, Train loss: 0.3284, Valid loss: 0.3251, Score: 0.5752\n",
      "Fold: 1, Epoch: 50, Train loss: 0.3261, Valid loss: 0.3160, Score: 0.4886\n",
      "Fold: 1, Epoch: 51, Train loss: 0.3201, Valid loss: 0.3172, Score: 0.4292\n",
      "Fold: 1, Epoch: 52, Train loss: 0.3229, Valid loss: 0.3231, Score: 0.5773\n",
      "Fold: 1, Epoch: 53, Train loss: 0.3242, Valid loss: 0.3154, Score: 0.5092\n",
      "Fold: 1, Epoch: 54, Train loss: 0.3194, Valid loss: 0.3161, Score: 0.4773\n",
      "Fold: 1, Epoch: 55, Train loss: 0.3212, Valid loss: 0.3226, Score: 0.5769\n",
      "Fold: 1, Epoch: 56, Train loss: 0.3233, Valid loss: 0.3151, Score: 0.4953\n",
      "Fold: 1, Epoch: 57, Train loss: 0.3193, Valid loss: 0.3152, Score: 0.4759\n",
      "Fold: 1, Epoch: 58, Train loss: 0.3196, Valid loss: 0.3210, Score: 0.5684\n",
      "Fold: 1, Epoch: 59, Train loss: 0.3220, Valid loss: 0.3147, Score: 0.4872\n",
      "Fold: 1, Epoch: 60, Train loss: 0.3191, Valid loss: 0.3142, Score: 0.5112\n",
      "Fold: 1, Epoch: 61, Train loss: 0.3180, Valid loss: 0.3194, Score: 0.5715\n",
      "Fold: 1, Epoch: 62, Train loss: 0.3204, Valid loss: 0.3153, Score: 0.4670\n",
      "Fold: 1, Epoch: 63, Train loss: 0.3207, Valid loss: 0.3168, Score: 0.5563\n",
      "Fold: 1, Epoch: 64, Train loss: 0.3183, Valid loss: 0.3147, Score: 0.5280\n",
      "Fold: 1, Epoch: 65, Train loss: 0.3171, Valid loss: 0.3144, Score: 0.4750\n",
      "Fold: 1, Epoch: 66, Train loss: 0.3184, Valid loss: 0.3191, Score: 0.5709\n",
      "Fold: 1, Epoch: 67, Train loss: 0.3195, Valid loss: 0.3149, Score: 0.4683\n",
      "Fold: 1, Epoch: 68, Train loss: 0.3192, Valid loss: 0.3177, Score: 0.5625\n",
      "Fold: 1, Epoch: 69, Train loss: 0.3182, Valid loss: 0.3139, Score: 0.4908\n",
      "Fold: 1, Epoch: 70, Train loss: 0.3172, Valid loss: 0.3154, Score: 0.5452\n",
      "Fold: 1, Epoch: 71, Train loss: 0.3165, Valid loss: 0.3136, Score: 0.5057\n",
      "Fold: 1, Epoch: 72, Train loss: 0.3160, Valid loss: 0.3145, Score: 0.5372\n",
      "Fold: 1, Epoch: 73, Train loss: 0.3157, Valid loss: 0.3134, Score: 0.5119\n",
      "Fold: 1, Epoch: 74, Train loss: 0.3156, Valid loss: 0.3156, Score: 0.5530\n",
      "Fold: 1, Epoch: 75, Train loss: 0.3159, Valid loss: 0.3157, Score: 0.4487\n",
      "Fold: 1, Epoch: 76, Train loss: 0.3199, Valid loss: 0.3476, Score: 0.5960\n",
      "Fold: 1, Epoch: 77, Train loss: 0.3439, Valid loss: 0.3647, Score: 0.0897\n",
      "Fold: 1, Epoch: 78, Train loss: 0.3760, Valid loss: 0.3211, Score: 0.4905\n",
      "Fold: 1, Epoch: 79, Train loss: 0.3221, Valid loss: 0.3539, Score: 0.5872\n",
      "Fold: 1, Epoch: 80, Train loss: 0.3522, Valid loss: 0.3225, Score: 0.3180\n",
      "Fold: 1, Epoch: 81, Train loss: 0.3255, Valid loss: 0.3364, Score: 0.1046\n",
      "Fold: 1, Epoch: 82, Train loss: 0.3435, Valid loss: 0.3183, Score: 0.4698\n",
      "Fold: 2, Epoch: 0, Train loss: 0.7295, Valid loss: 0.5119, Score: 0.0015\n",
      "Fold: 2, Epoch: 1, Train loss: 0.5060, Valid loss: 0.4996, Score: 0.0007\n",
      "Fold: 2, Epoch: 2, Train loss: 0.4944, Valid loss: 0.4665, Score: 0.0030\n",
      "Fold: 2, Epoch: 3, Train loss: 0.4628, Valid loss: 0.4513, Score: 0.0267\n",
      "Fold: 2, Epoch: 4, Train loss: 0.4500, Valid loss: 0.4439, Score: 0.0859\n",
      "Fold: 2, Epoch: 5, Train loss: 0.4445, Valid loss: 0.4256, Score: 0.1231\n",
      "Fold: 2, Epoch: 6, Train loss: 0.4263, Valid loss: 0.4035, Score: 0.1355\n",
      "Fold: 2, Epoch: 7, Train loss: 0.4031, Valid loss: 0.3872, Score: 0.1576\n",
      "Fold: 2, Epoch: 8, Train loss: 0.3864, Valid loss: 0.3708, Score: 0.2490\n",
      "Fold: 2, Epoch: 9, Train loss: 0.3716, Valid loss: 0.3624, Score: 0.4569\n",
      "Fold: 2, Epoch: 10, Train loss: 0.3656, Valid loss: 0.3562, Score: 0.4648\n",
      "Fold: 2, Epoch: 11, Train loss: 0.3599, Valid loss: 0.3519, Score: 0.2892\n",
      "Fold: 2, Epoch: 12, Train loss: 0.3550, Valid loss: 0.3460, Score: 0.1347\n",
      "Fold: 2, Epoch: 13, Train loss: 0.3492, Valid loss: 0.3412, Score: 0.1586\n",
      "Fold: 2, Epoch: 14, Train loss: 0.3454, Valid loss: 0.3394, Score: 0.2361\n",
      "Fold: 2, Epoch: 15, Train loss: 0.3438, Valid loss: 0.3364, Score: 0.1918\n",
      "Fold: 2, Epoch: 16, Train loss: 0.3398, Valid loss: 0.3344, Score: 0.3295\n",
      "Fold: 2, Epoch: 17, Train loss: 0.3371, Valid loss: 0.3333, Score: 0.5122\n",
      "Fold: 2, Epoch: 18, Train loss: 0.3359, Valid loss: 0.3340, Score: 0.4694\n",
      "Fold: 2, Epoch: 19, Train loss: 0.3349, Valid loss: 0.3328, Score: 0.4760\n",
      "Fold: 2, Epoch: 20, Train loss: 0.3332, Valid loss: 0.3321, Score: 0.4883\n",
      "Fold: 2, Epoch: 21, Train loss: 0.3325, Valid loss: 0.3334, Score: 0.3743\n",
      "Fold: 2, Epoch: 22, Train loss: 0.3332, Valid loss: 0.3325, Score: 0.5088\n",
      "Fold: 2, Epoch: 23, Train loss: 0.3331, Valid loss: 0.3315, Score: 0.3830\n",
      "Fold: 2, Epoch: 24, Train loss: 0.3308, Valid loss: 0.3288, Score: 0.4676\n",
      "Fold: 2, Epoch: 25, Train loss: 0.3283, Valid loss: 0.3290, Score: 0.5038\n",
      "Fold: 2, Epoch: 26, Train loss: 0.3285, Valid loss: 0.3305, Score: 0.4303\n",
      "Fold: 2, Epoch: 27, Train loss: 0.3290, Valid loss: 0.3292, Score: 0.5177\n",
      "Fold: 2, Epoch: 28, Train loss: 0.3277, Valid loss: 0.3281, Score: 0.4816\n",
      "Fold: 2, Epoch: 29, Train loss: 0.3258, Valid loss: 0.3285, Score: 0.4416\n",
      "Fold: 2, Epoch: 30, Train loss: 0.3255, Valid loss: 0.3294, Score: 0.5121\n",
      "Fold: 2, Epoch: 31, Train loss: 0.3264, Valid loss: 0.3305, Score: 0.3629\n",
      "Fold: 2, Epoch: 32, Train loss: 0.3267, Valid loss: 0.3293, Score: 0.5083\n",
      "Fold: 2, Epoch: 33, Train loss: 0.3254, Valid loss: 0.3278, Score: 0.4406\n",
      "Fold: 2, Epoch: 34, Train loss: 0.3233, Valid loss: 0.3270, Score: 0.4814\n",
      "Fold: 2, Epoch: 35, Train loss: 0.3222, Valid loss: 0.3272, Score: 0.5063\n",
      "Fold: 2, Epoch: 36, Train loss: 0.3224, Valid loss: 0.3295, Score: 0.4207\n",
      "Fold: 2, Epoch: 37, Train loss: 0.3245, Valid loss: 0.3372, Score: 0.5667\n",
      "Fold: 2, Epoch: 38, Train loss: 0.3338, Valid loss: 0.3381, Score: 0.2645\n",
      "Fold: 2, Epoch: 39, Train loss: 0.3340, Valid loss: 0.3256, Score: 0.4912\n",
      "Fold: 2, Epoch: 40, Train loss: 0.3228, Valid loss: 0.3294, Score: 0.5232\n",
      "Fold: 2, Epoch: 41, Train loss: 0.3272, Valid loss: 0.3264, Score: 0.3599\n",
      "Fold: 2, Epoch: 42, Train loss: 0.3234, Valid loss: 0.3278, Score: 0.3597\n",
      "Fold: 2, Epoch: 43, Train loss: 0.3245, Valid loss: 0.3265, Score: 0.5274\n",
      "Fold: 2, Epoch: 44, Train loss: 0.3229, Valid loss: 0.3269, Score: 0.5346\n",
      "Fold: 2, Epoch: 45, Train loss: 0.3225, Valid loss: 0.3285, Score: 0.4361\n",
      "Fold: 2, Epoch: 46, Train loss: 0.3236, Valid loss: 0.3254, Score: 0.4749\n",
      "Fold: 2, Epoch: 47, Train loss: 0.3200, Valid loss: 0.3279, Score: 0.5326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-01 20:52:55,545] Trial 0 failed with parameters: {'hidden_size': 78} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\astus\\AppData\\Local\\Temp\\ipykernel_19696\\2811287571.py\", line 43, in objective\n",
      "    valid_loss, score = validate(valid_sentences, valid_speakers, valid_labels, model, criterion)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\astus\\AppData\\Local\\Temp\\ipykernel_19696\\1996007288.py\", line 14, in validate\n",
      "    out = model(sentences, speakers)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\astus\\AppData\\Local\\Temp\\ipykernel_19696\\1595066264.py\", line 17, in forward\n",
      "    packed_sentences = pack_padded_sequence(pad_sequence(embedded_sentences, batch_first=True), lengths, batch_first=True, enforce_sorted=False)\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py\", line 400, in pad_sequence\n",
      "    return torch._C._nn.pad_sequence(sequences, batch_first, padding_value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\utils\\_device.py\", line 77, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2023-12-01 20:52:55,564] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\lstm_prep.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\lstm_prep.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(train_sentences, train_speakers, train_labels, model, criterion, optimizer)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     valid_loss, score \u001b[39m=\u001b[39m validate(valid_sentences, valid_speakers, valid_labels, model, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# Stopping criteria            \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m valid_loss \u001b[39m>\u001b[39m best_valid_loss:\n",
      "\u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\lstm_prep.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39mlen\u001b[39m(labels) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(sentences))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m out \u001b[39m=\u001b[39m model(sentences, speakers)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pred_labels \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\lstm_prep.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m embedded_sentences, lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[embed_sentence(sentence, tokenizer) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# pack / pad sequences (save memory) \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m packed_sentences \u001b[39m=\u001b[39m pack_padded_sequence(pad_sequence(embedded_sentences, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# hot encode speakers\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/lstm_prep.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m encoded_speakers \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([switcher[speaker] \u001b[39mfor\u001b[39;00m speaker \u001b[39min\u001b[39;00m speakers])\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:400\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[0;32m    396\u001b[0m         sequences \u001b[39m=\u001b[39m sequences\u001b[39m.\u001b[39munbind(\u001b[39m0\u001b[39m)\n\u001b[0;32m    398\u001b[0m \u001b[39m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mpad_sequence(sequences, batch_first, padding_value)\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_number = 0\n",
    "params = json.load(open(f\"models/params_{trial_number}.json\", \"r\"))\n",
    "\n",
    "# maybe load model from a certain fold ?\n",
    "# fold = 0\n",
    "# model.load_state_dict(torch.load(f\"models/lstm_{trial_number}_{fold}.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use entire dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(num_features, 64, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss() # need to check input\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.3152, Valid loss: 0.3296, Score: 0.4708\n",
      "Epoch: 1, Train loss: 0.3156, Valid loss: 0.3293, Score: 0.5065\n",
      "Epoch: 2, Train loss: 0.3150, Valid loss: 0.3297, Score: 0.5193\n",
      "Epoch: 3, Train loss: 0.3153, Valid loss: 0.3293, Score: 0.4938\n",
      "Epoch: 4, Train loss: 0.3148, Valid loss: 0.3295, Score: 0.4822\n",
      "Epoch: 5, Train loss: 0.3150, Valid loss: 0.3295, Score: 0.5119\n",
      "Epoch: 6, Train loss: 0.3147, Valid loss: 0.3295, Score: 0.5123\n",
      "Epoch: 7, Train loss: 0.3147, Valid loss: 0.3293, Score: 0.4835\n",
      "Epoch: 8, Train loss: 0.3146, Valid loss: 0.3292, Score: 0.4881\n",
      "Epoch: 9, Train loss: 0.3144, Valid loss: 0.3293, Score: 0.5115\n",
      "Epoch: 10, Train loss: 0.3144, Valid loss: 0.3291, Score: 0.5023\n",
      "Epoch: 11, Train loss: 0.3141, Valid loss: 0.3292, Score: 0.4814\n",
      "Epoch: 12, Train loss: 0.3142, Valid loss: 0.3292, Score: 0.5036\n",
      "Epoch: 13, Train loss: 0.3139, Valid loss: 0.3293, Score: 0.5108\n",
      "Epoch: 14, Train loss: 0.3139, Valid loss: 0.3292, Score: 0.4909\n",
      "Epoch: 15, Train loss: 0.3138, Valid loss: 0.3292, Score: 0.4932\n",
      "Epoch: 16, Train loss: 0.3136, Valid loss: 0.3293, Score: 0.5111\n",
      "Epoch: 17, Train loss: 0.3136, Valid loss: 0.3291, Score: 0.4971\n",
      "Epoch: 18, Train loss: 0.3134, Valid loss: 0.3291, Score: 0.4890\n",
      "Epoch: 19, Train loss: 0.3134, Valid loss: 0.3292, Score: 0.5061\n"
     ]
    }
   ],
   "source": [
    "import custom_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, train_speakers, _ = custom_utils.gather_dataset(\"training\", combine = False)\n",
    "sentences, speakers, labels = read_data(\"training\", \"training_labels.json\") \n",
    "\n",
    "y = labels\n",
    "X = list(zip(sentences, speakers))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "train_sentences, train_speakers = zip(*X_train)\n",
    "test_sentences, test_speakers = zip(*X_test)\n",
    "\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "n_epochs = 200\n",
    "patience = 10\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_sentences, train_speakers, train_labels, model, criterion, optimizer)\n",
    "    valid_loss, score = validate(test_sentences, test_speakers, test_labels, model, criterion)\n",
    "            \n",
    "    # Stopping criteria            \n",
    "    if valid_loss > best_valid_loss:\n",
    "        current_patience += 1\n",
    "    else:\n",
    "        best_weights = model.state_dict()\n",
    "        best_valid_loss = valid_loss    \n",
    "        score_at_best = score \n",
    "        current_patience = 0\n",
    "    \n",
    "    if current_patience == patience:\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Train loss: {train_loss:.4f}, Valid loss: {valid_loss:.4f}, Score: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_utils\n",
    "# read test data (dicitonary-like)\n",
    "test_sentences, test_speakers, _  = custom_utils.gather_dataset(\"test\", combine = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "model.eval()\n",
    "test_labels = {}\n",
    "for id in test_sentences.keys():\n",
    "    out = model(test_sentences[id], test_speakers[id])\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_labels[id] = pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(test_labels, open(\"test_labels.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
