{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import custom_utils\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "\n",
    "torch.set_default_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "with open('data/X_train.json', 'r') as json_file:\n",
    "    X_train = json.load(json_file)\n",
    "   \n",
    "with open('data/y_train.json', 'r') as json_file:\n",
    "    y_train = json.load(json_file)\n",
    "   \n",
    "with open('data/X_test.json', 'r') as json_file:\n",
    "    X_test = json.load(json_file) \n",
    "\n",
    "with open('data/y_test.json', 'r') as json_file:\n",
    "    y_test = json.load(json_file)\n",
    "    \n",
    "# converting to appropriate format (and device)\n",
    "X_train = torch.Tensor(X_train).float()\n",
    "y_train = torch.Tensor(y_train).long()\n",
    "\n",
    "X_test = torch.Tensor(X_test).float()\n",
    "y_test = torch.Tensor(y_test).long()\n",
    "\n",
    "# getting shapes\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = 2\n",
    "# edge_dim = edge_attr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchmetrics.classification import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(trial):\n",
    "    # Model\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    layers = []\n",
    "\n",
    "    in_features = num_features\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(f'n_units_l{i}', 200, 800)\n",
    "        layers.append(torch.nn.Linear(in_features, out_features))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # suggest dropout\n",
    "        p = trial.suggest_float(f'p{i}', 0, 0.8)\n",
    "        layers.append(torch.nn.Dropout(p))\n",
    "\n",
    "        # updating next layer size\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(torch.nn.Linear(in_features, num_classes))\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    def train(X_train, y_train):\n",
    "        model.train() \n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_train)\n",
    "        loss = criterion(out, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def validate(X_valid, y_valid):\n",
    "        model.eval()\n",
    "        out = model(X_valid)\n",
    "        loss = criterion(out, y_valid)\n",
    "        y_pred = out.argmax(dim=1)\n",
    "        f1 = F1Score(task='binary', num_classes=num_classes).to(device)\n",
    "        score = f1(y_valid, y_pred)\n",
    "        # score = f1_score(y_valid, y_pred)\n",
    "        return loss, score\n",
    "        \n",
    "    \n",
    "    n_folds = 5\n",
    "    n_epochs = 200\n",
    "    patience = 10\n",
    "    avg_score = 0\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    for i, (train_idx, val_idx) in enumerate(skf.split(X_train,y_train)):\n",
    "        \n",
    "        # set model, criterion and optimizers\n",
    "        model = MLP(trial).to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss() # need to check input\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "        # Selecing fold train and validation\n",
    "        train_mask = torch.zeros(len(X_train), dtype=torch.bool)\n",
    "        val_mask = torch.zeros(len(X_train), dtype=torch.bool)\n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        \n",
    "        X_train_fold = X_train[train_mask].to(device)\n",
    "        y_train_fold = y_train[train_mask].to(device)\n",
    "        X_valid_fold = X_train[val_mask].to(device)\n",
    "        y_valid_fold = y_train[val_mask].to(device)\n",
    "        \n",
    "        # Epoch\n",
    "        best_valid_loss = float('inf')\n",
    "        score_at_best = -1\n",
    "        current_patience = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = train(X_train_fold, y_train_fold)\n",
    "            valid_loss, score = validate(X_valid_fold, y_valid_fold)\n",
    "           \n",
    "            # Stopping criterium            \n",
    "            if valid_loss > best_valid_loss:\n",
    "                current_patience += 1\n",
    "            else:\n",
    "                best_valid_loss = valid_loss\n",
    "                score_at_best = score \n",
    "                current_patience = 0\n",
    "            \n",
    "            if current_patience == patience:\n",
    "                break\n",
    "            \n",
    "            print(f'Epoch: {epoch}, Train loss: {train_loss:.4f}, Valid loss: {valid_loss:.4f}, Score: {score:.4f}')\n",
    "        \n",
    "        avg_score += score_at_best/n_folds\n",
    "        \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-29 21:56:19,286] A new study created in memory with name: no-name-26debf4b-35de-4cbf-ab4a-7701aeb54d3b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.7105, Valid loss: 0.5037, Score: 0.0000\n",
      "Epoch: 1, Train loss: 0.5047, Valid loss: 0.4324, Score: 0.0000\n",
      "Epoch: 2, Train loss: 0.4306, Valid loss: 0.4475, Score: 0.0000\n",
      "Epoch: 3, Train loss: 0.4421, Valid loss: 0.4172, Score: 0.0000\n",
      "Epoch: 4, Train loss: 0.4124, Valid loss: 0.3823, Score: 0.0000\n",
      "Epoch: 5, Train loss: 0.3797, Valid loss: 0.3681, Score: 0.0000\n",
      "Epoch: 6, Train loss: 0.3676, Valid loss: 0.3684, Score: 0.0000\n",
      "Epoch: 7, Train loss: 0.3691, Valid loss: 0.3706, Score: 0.0000\n",
      "Epoch: 8, Train loss: 0.3715, Valid loss: 0.3689, Score: 0.0000\n",
      "Epoch: 9, Train loss: 0.3699, Valid loss: 0.3638, Score: 0.0000\n",
      "Epoch: 10, Train loss: 0.3643, Valid loss: 0.3582, Score: 0.0000\n",
      "Epoch: 11, Train loss: 0.3586, Valid loss: 0.3551, Score: 0.0000\n",
      "Epoch: 12, Train loss: 0.3546, Valid loss: 0.3550, Score: 0.0000\n",
      "Epoch: 13, Train loss: 0.3540, Valid loss: 0.3557, Score: 0.0000\n",
      "Epoch: 14, Train loss: 0.3542, Valid loss: 0.3552, Score: 0.0000\n",
      "Epoch: 15, Train loss: 0.3539, Valid loss: 0.3533, Score: 0.0000\n",
      "Epoch: 16, Train loss: 0.3518, Valid loss: 0.3508, Score: 0.0000\n",
      "Epoch: 17, Train loss: 0.3497, Valid loss: 0.3488, Score: 0.0000\n",
      "Epoch: 18, Train loss: 0.3480, Valid loss: 0.3479, Score: 0.0000\n",
      "Epoch: 19, Train loss: 0.3471, Valid loss: 0.3479, Score: 0.0000\n",
      "Epoch: 20, Train loss: 0.3471, Valid loss: 0.3480, Score: 0.0000\n",
      "Epoch: 21, Train loss: 0.3473, Valid loss: 0.3476, Score: 0.0000\n",
      "Epoch: 22, Train loss: 0.3466, Valid loss: 0.3468, Score: 0.0000\n",
      "Epoch: 23, Train loss: 0.3452, Valid loss: 0.3461, Score: 0.0000\n",
      "Epoch: 24, Train loss: 0.3444, Valid loss: 0.3460, Score: 0.0000\n",
      "Epoch: 25, Train loss: 0.3440, Valid loss: 0.3461, Score: 0.0216\n",
      "Epoch: 26, Train loss: 0.3435, Valid loss: 0.3457, Score: 0.1830\n",
      "Epoch: 27, Train loss: 0.3433, Valid loss: 0.3449, Score: 0.2905\n",
      "Epoch: 28, Train loss: 0.3425, Valid loss: 0.3439, Score: 0.3561\n",
      "Epoch: 29, Train loss: 0.3416, Valid loss: 0.3429, Score: 0.3949\n",
      "Epoch: 30, Train loss: 0.3411, Valid loss: 0.3420, Score: 0.4108\n",
      "Epoch: 31, Train loss: 0.3401, Valid loss: 0.3410, Score: 0.4174\n",
      "Epoch: 32, Train loss: 0.3395, Valid loss: 0.3399, Score: 0.4338\n",
      "Epoch: 33, Train loss: 0.3386, Valid loss: 0.3389, Score: 0.4498\n",
      "Epoch: 34, Train loss: 0.3375, Valid loss: 0.3380, Score: 0.4717\n",
      "Epoch: 35, Train loss: 0.3362, Valid loss: 0.3374, Score: 0.4811\n",
      "Epoch: 36, Train loss: 0.3358, Valid loss: 0.3370, Score: 0.4792\n",
      "Epoch: 37, Train loss: 0.3349, Valid loss: 0.3367, Score: 0.4774\n",
      "Epoch: 38, Train loss: 0.3338, Valid loss: 0.3362, Score: 0.4790\n",
      "Epoch: 39, Train loss: 0.3335, Valid loss: 0.3358, Score: 0.4776\n",
      "Epoch: 40, Train loss: 0.3329, Valid loss: 0.3354, Score: 0.4729\n",
      "Epoch: 41, Train loss: 0.3324, Valid loss: 0.3349, Score: 0.4814\n",
      "Epoch: 42, Train loss: 0.3313, Valid loss: 0.3345, Score: 0.4876\n",
      "Epoch: 43, Train loss: 0.3314, Valid loss: 0.3343, Score: 0.4778\n",
      "Epoch: 44, Train loss: 0.3303, Valid loss: 0.3341, Score: 0.4854\n",
      "Epoch: 45, Train loss: 0.3304, Valid loss: 0.3340, Score: 0.4770\n",
      "Epoch: 46, Train loss: 0.3292, Valid loss: 0.3339, Score: 0.4767\n",
      "Epoch: 47, Train loss: 0.3293, Valid loss: 0.3337, Score: 0.4895\n",
      "Epoch: 48, Train loss: 0.3280, Valid loss: 0.3337, Score: 0.4700\n",
      "Epoch: 49, Train loss: 0.3275, Valid loss: 0.3330, Score: 0.5153\n",
      "Epoch: 50, Train loss: 0.3278, Valid loss: 0.3327, Score: 0.4813\n",
      "Epoch: 51, Train loss: 0.3274, Valid loss: 0.3322, Score: 0.5057\n",
      "Epoch: 52, Train loss: 0.3266, Valid loss: 0.3322, Score: 0.5147\n",
      "Epoch: 53, Train loss: 0.3261, Valid loss: 0.3329, Score: 0.4736\n",
      "Epoch: 54, Train loss: 0.3264, Valid loss: 0.3324, Score: 0.5308\n",
      "Epoch: 55, Train loss: 0.3259, Valid loss: 0.3320, Score: 0.4833\n",
      "Epoch: 56, Train loss: 0.3251, Valid loss: 0.3314, Score: 0.4981\n",
      "Epoch: 57, Train loss: 0.3249, Valid loss: 0.3317, Score: 0.5240\n",
      "Epoch: 58, Train loss: 0.3246, Valid loss: 0.3325, Score: 0.4606\n",
      "Epoch: 59, Train loss: 0.3251, Valid loss: 0.3315, Score: 0.5211\n",
      "Epoch: 60, Train loss: 0.3245, Valid loss: 0.3312, Score: 0.5009\n",
      "Epoch: 61, Train loss: 0.3235, Valid loss: 0.3316, Score: 0.4716\n",
      "Epoch: 62, Train loss: 0.3229, Valid loss: 0.3315, Score: 0.5242\n",
      "Epoch: 63, Train loss: 0.3231, Valid loss: 0.3313, Score: 0.4822\n",
      "Epoch: 64, Train loss: 0.3225, Valid loss: 0.3306, Score: 0.5107\n",
      "Epoch: 65, Train loss: 0.3221, Valid loss: 0.3306, Score: 0.5168\n",
      "Epoch: 66, Train loss: 0.3217, Valid loss: 0.3313, Score: 0.4720\n",
      "Epoch: 67, Train loss: 0.3220, Valid loss: 0.3309, Score: 0.5274\n",
      "Epoch: 68, Train loss: 0.3216, Valid loss: 0.3308, Score: 0.4878\n",
      "Epoch: 69, Train loss: 0.3206, Valid loss: 0.3302, Score: 0.5068\n",
      "Epoch: 70, Train loss: 0.3203, Valid loss: 0.3302, Score: 0.5114\n",
      "Epoch: 71, Train loss: 0.3205, Valid loss: 0.3306, Score: 0.4837\n",
      "Epoch: 72, Train loss: 0.3202, Valid loss: 0.3305, Score: 0.5285\n",
      "Epoch: 73, Train loss: 0.3205, Valid loss: 0.3311, Score: 0.4667\n",
      "Epoch: 74, Train loss: 0.3206, Valid loss: 0.3304, Score: 0.5277\n",
      "Epoch: 75, Train loss: 0.3199, Valid loss: 0.3303, Score: 0.4896\n",
      "Epoch: 76, Train loss: 0.3188, Valid loss: 0.3298, Score: 0.5085\n",
      "Epoch: 77, Train loss: 0.3185, Valid loss: 0.3299, Score: 0.5138\n",
      "Epoch: 78, Train loss: 0.3182, Valid loss: 0.3306, Score: 0.4748\n",
      "Epoch: 79, Train loss: 0.3186, Valid loss: 0.3306, Score: 0.5330\n",
      "Epoch: 80, Train loss: 0.3191, Valid loss: 0.3313, Score: 0.4630\n",
      "Epoch: 81, Train loss: 0.3188, Valid loss: 0.3301, Score: 0.5259\n",
      "Epoch: 82, Train loss: 0.3175, Valid loss: 0.3295, Score: 0.4996\n",
      "Epoch: 83, Train loss: 0.3163, Valid loss: 0.3296, Score: 0.5000\n",
      "Epoch: 84, Train loss: 0.3168, Valid loss: 0.3298, Score: 0.5266\n",
      "Epoch: 85, Train loss: 0.3168, Valid loss: 0.3308, Score: 0.4670\n",
      "Epoch: 86, Train loss: 0.3175, Valid loss: 0.3302, Score: 0.5337\n",
      "Epoch: 87, Train loss: 0.3168, Valid loss: 0.3300, Score: 0.4796\n",
      "Epoch: 88, Train loss: 0.3159, Valid loss: 0.3291, Score: 0.5069\n",
      "Epoch: 89, Train loss: 0.3153, Valid loss: 0.3292, Score: 0.5147\n",
      "Epoch: 90, Train loss: 0.3155, Valid loss: 0.3300, Score: 0.4759\n",
      "Epoch: 91, Train loss: 0.3156, Valid loss: 0.3295, Score: 0.5334\n",
      "Epoch: 92, Train loss: 0.3156, Valid loss: 0.3297, Score: 0.4762\n",
      "Epoch: 93, Train loss: 0.3154, Valid loss: 0.3288, Score: 0.5167\n",
      "Epoch: 94, Train loss: 0.3140, Valid loss: 0.3288, Score: 0.5059\n",
      "Epoch: 95, Train loss: 0.3140, Valid loss: 0.3291, Score: 0.4900\n",
      "Epoch: 96, Train loss: 0.3133, Valid loss: 0.3291, Score: 0.5257\n",
      "Epoch: 97, Train loss: 0.3137, Valid loss: 0.3299, Score: 0.4739\n",
      "Epoch: 98, Train loss: 0.3141, Valid loss: 0.3295, Score: 0.5364\n",
      "Epoch: 99, Train loss: 0.3141, Valid loss: 0.3299, Score: 0.4709\n",
      "Epoch: 100, Train loss: 0.3131, Valid loss: 0.3289, Score: 0.5288\n",
      "Epoch: 101, Train loss: 0.3129, Valid loss: 0.3287, Score: 0.4963\n",
      "Epoch: 102, Train loss: 0.3122, Valid loss: 0.3284, Score: 0.5111\n",
      "Epoch: 103, Train loss: 0.3112, Valid loss: 0.3283, Score: 0.5130\n",
      "Epoch: 104, Train loss: 0.3121, Valid loss: 0.3286, Score: 0.4909\n",
      "Epoch: 105, Train loss: 0.3118, Valid loss: 0.3288, Score: 0.5321\n",
      "Epoch: 106, Train loss: 0.3115, Valid loss: 0.3298, Score: 0.4706\n",
      "Epoch: 107, Train loss: 0.3115, Valid loss: 0.3290, Score: 0.5374\n",
      "Epoch: 108, Train loss: 0.3123, Valid loss: 0.3295, Score: 0.4737\n",
      "Epoch: 109, Train loss: 0.3114, Valid loss: 0.3284, Score: 0.5313\n",
      "Epoch: 110, Train loss: 0.3106, Valid loss: 0.3283, Score: 0.4947\n",
      "Epoch: 111, Train loss: 0.3100, Valid loss: 0.3279, Score: 0.5053\n",
      "Epoch: 112, Train loss: 0.3094, Valid loss: 0.3281, Score: 0.5178\n",
      "Epoch: 113, Train loss: 0.3099, Valid loss: 0.3288, Score: 0.4860\n",
      "Epoch: 114, Train loss: 0.3096, Valid loss: 0.3288, Score: 0.5329\n",
      "Epoch: 115, Train loss: 0.3102, Valid loss: 0.3302, Score: 0.4604\n",
      "Epoch: 116, Train loss: 0.3105, Valid loss: 0.3299, Score: 0.5444\n",
      "Epoch: 117, Train loss: 0.3117, Valid loss: 0.3305, Score: 0.4618\n",
      "Epoch: 118, Train loss: 0.3107, Valid loss: 0.3284, Score: 0.5326\n",
      "Epoch: 119, Train loss: 0.3092, Valid loss: 0.3281, Score: 0.5022\n",
      "Epoch: 120, Train loss: 0.3077, Valid loss: 0.3287, Score: 0.4866\n",
      "Epoch: 0, Train loss: 0.6983, Valid loss: 0.5120, Score: 0.0000\n",
      "Epoch: 1, Train loss: 0.5036, Valid loss: 0.4354, Score: 0.0000\n",
      "Epoch: 2, Train loss: 0.4284, Valid loss: 0.4430, Score: 0.0000\n",
      "Epoch: 3, Train loss: 0.4400, Valid loss: 0.4177, Score: 0.0000\n",
      "Epoch: 4, Train loss: 0.4135, Valid loss: 0.3885, Score: 0.0000\n",
      "Epoch: 5, Train loss: 0.3808, Valid loss: 0.3779, Score: 0.0000\n",
      "Epoch: 6, Train loss: 0.3668, Valid loss: 0.3801, Score: 0.0000\n",
      "Epoch: 7, Train loss: 0.3668, Valid loss: 0.3837, Score: 0.0000\n",
      "Epoch: 8, Train loss: 0.3691, Valid loss: 0.3831, Score: 0.0000\n",
      "Epoch: 9, Train loss: 0.3678, Valid loss: 0.3787, Score: 0.0000\n",
      "Epoch: 10, Train loss: 0.3628, Valid loss: 0.3730, Score: 0.0000\n",
      "Epoch: 11, Train loss: 0.3572, Valid loss: 0.3684, Score: 0.0000\n",
      "Epoch: 12, Train loss: 0.3530, Valid loss: 0.3662, Score: 0.0000\n",
      "Epoch: 13, Train loss: 0.3517, Valid loss: 0.3652, Score: 0.0030\n",
      "Epoch: 14, Train loss: 0.3511, Valid loss: 0.3641, Score: 0.0823\n",
      "Epoch: 15, Train loss: 0.3502, Valid loss: 0.3624, Score: 0.2923\n",
      "Epoch: 16, Train loss: 0.3481, Valid loss: 0.3606, Score: 0.4384\n",
      "Epoch: 17, Train loss: 0.3460, Valid loss: 0.3593, Score: 0.4900\n",
      "Epoch: 18, Train loss: 0.3442, Valid loss: 0.3588, Score: 0.5143\n",
      "Epoch: 19, Train loss: 0.3433, Valid loss: 0.3584, Score: 0.5219\n",
      "Epoch: 20, Train loss: 0.3426, Valid loss: 0.3571, Score: 0.5202\n",
      "Epoch: 21, Train loss: 0.3418, Valid loss: 0.3548, Score: 0.5019\n",
      "Epoch: 22, Train loss: 0.3399, Valid loss: 0.3529, Score: 0.4689\n",
      "Epoch: 23, Train loss: 0.3391, Valid loss: 0.3520, Score: 0.4428\n",
      "Epoch: 24, Train loss: 0.3386, Valid loss: 0.3514, Score: 0.4470\n",
      "Epoch: 25, Train loss: 0.3383, Valid loss: 0.3509, Score: 0.4652\n",
      "Epoch: 26, Train loss: 0.3369, Valid loss: 0.3509, Score: 0.4855\n",
      "Epoch: 27, Train loss: 0.3367, Valid loss: 0.3513, Score: 0.4977\n",
      "Epoch: 28, Train loss: 0.3365, Valid loss: 0.3510, Score: 0.4984\n",
      "Epoch: 29, Train loss: 0.3360, Valid loss: 0.3501, Score: 0.4811\n",
      "Epoch: 30, Train loss: 0.3351, Valid loss: 0.3494, Score: 0.4692\n",
      "Epoch: 31, Train loss: 0.3344, Valid loss: 0.3491, Score: 0.4676\n",
      "Epoch: 32, Train loss: 0.3341, Valid loss: 0.3489, Score: 0.4851\n",
      "Epoch: 33, Train loss: 0.3331, Valid loss: 0.3492, Score: 0.4952\n",
      "Epoch: 34, Train loss: 0.3328, Valid loss: 0.3491, Score: 0.5036\n",
      "Epoch: 35, Train loss: 0.3324, Valid loss: 0.3482, Score: 0.4955\n",
      "Epoch: 36, Train loss: 0.3315, Valid loss: 0.3476, Score: 0.4903\n",
      "Epoch: 37, Train loss: 0.3312, Valid loss: 0.3473, Score: 0.4964\n",
      "Epoch: 38, Train loss: 0.3299, Valid loss: 0.3472, Score: 0.5042\n",
      "Epoch: 39, Train loss: 0.3296, Valid loss: 0.3462, Score: 0.4974\n",
      "Epoch: 40, Train loss: 0.3289, Valid loss: 0.3456, Score: 0.4876\n",
      "Epoch: 41, Train loss: 0.3286, Valid loss: 0.3459, Score: 0.5053\n",
      "Epoch: 42, Train loss: 0.3276, Valid loss: 0.3459, Score: 0.5094\n",
      "Epoch: 43, Train loss: 0.3270, Valid loss: 0.3449, Score: 0.4945\n",
      "Epoch: 44, Train loss: 0.3263, Valid loss: 0.3448, Score: 0.4975\n",
      "Epoch: 45, Train loss: 0.3260, Valid loss: 0.3453, Score: 0.5147\n",
      "Epoch: 46, Train loss: 0.3256, Valid loss: 0.3447, Score: 0.5056\n",
      "Epoch: 47, Train loss: 0.3249, Valid loss: 0.3445, Score: 0.5029\n",
      "Epoch: 48, Train loss: 0.3239, Valid loss: 0.3447, Score: 0.5180\n",
      "Epoch: 49, Train loss: 0.3239, Valid loss: 0.3437, Score: 0.4933\n",
      "Epoch: 50, Train loss: 0.3232, Valid loss: 0.3441, Score: 0.5157\n",
      "Epoch: 51, Train loss: 0.3231, Valid loss: 0.3433, Score: 0.5068\n",
      "Epoch: 52, Train loss: 0.3220, Valid loss: 0.3432, Score: 0.5126\n",
      "Epoch: 53, Train loss: 0.3215, Valid loss: 0.3431, Score: 0.5164\n",
      "Epoch: 54, Train loss: 0.3213, Valid loss: 0.3431, Score: 0.5160\n",
      "Epoch: 55, Train loss: 0.3206, Valid loss: 0.3431, Score: 0.5190\n",
      "Epoch: 56, Train loss: 0.3203, Valid loss: 0.3425, Score: 0.4946\n",
      "Epoch: 57, Train loss: 0.3201, Valid loss: 0.3457, Score: 0.5447\n",
      "Epoch: 58, Train loss: 0.3209, Valid loss: 0.3445, Score: 0.4432\n",
      "Epoch: 59, Train loss: 0.3235, Valid loss: 0.3465, Score: 0.5497\n",
      "Epoch: 60, Train loss: 0.3213, Valid loss: 0.3433, Score: 0.5277\n",
      "Epoch: 61, Train loss: 0.3186, Valid loss: 0.3435, Score: 0.4659\n",
      "Epoch: 62, Train loss: 0.3213, Valid loss: 0.3433, Score: 0.5308\n",
      "Epoch: 63, Train loss: 0.3183, Valid loss: 0.3447, Score: 0.5418\n",
      "Epoch: 64, Train loss: 0.3190, Valid loss: 0.3427, Score: 0.4685\n",
      "Epoch: 65, Train loss: 0.3191, Valid loss: 0.3422, Score: 0.5212\n",
      "Epoch: 66, Train loss: 0.3170, Valid loss: 0.3456, Score: 0.5500\n",
      "Epoch: 67, Train loss: 0.3189, Valid loss: 0.3415, Score: 0.4880\n",
      "Epoch: 68, Train loss: 0.3172, Valid loss: 0.3412, Score: 0.4974\n",
      "Epoch: 69, Train loss: 0.3159, Valid loss: 0.3451, Score: 0.5450\n",
      "Epoch: 70, Train loss: 0.3178, Valid loss: 0.3415, Score: 0.4987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-29 21:56:33,339] Trial 0 failed with parameters: {'n_layers': 1, 'n_units_l0': 368, 'p0': 0.20479142260877037} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\astus\\AppData\\Local\\Temp\\ipykernel_13244\\2734359479.py\", line 55, in objective\n",
      "    valid_loss, score = validate(X_valid_fold, y_valid_fold)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\astus\\AppData\\Local\\Temp\\ipykernel_13244\\2734359479.py\", line 17, in validate\n",
      "    f1 = F1Score(task='binary', num_classes=num_classes).to(device)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torchmetrics\\metric.py\", line 792, in _apply\n",
      "    this._defaults[key] = fn(value)\n",
      "                          ^^^^^^^^^\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\utils\\_device.py\", line 77, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-29 21:56:33,365] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\opt_hyp.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 3. Create a study object and optimize the objective function.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\opt_hyp.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(X_train_fold, y_train_fold)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     valid_loss, score \u001b[39m=\u001b[39m validate(X_valid_fold, y_valid_fold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# Stopping criterium            \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mif\u001b[39;00m valid_loss \u001b[39m>\u001b[39m best_valid_loss:\n",
      "\u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\opt_hyp.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, y_valid)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y_pred \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m f1 \u001b[39m=\u001b[39m F1Score(task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbinary\u001b[39;49m\u001b[39m'\u001b[39;49m, num_classes\u001b[39m=\u001b[39;49mnum_classes)\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m score \u001b[39m=\u001b[39m f1(y_valid, y_pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/astus/code/extractive-summarization/opt_hyp.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# score = f1_score(y_valid, y_pred)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torchmetrics\\metric.py:792\u001b[0m, in \u001b[0;36mMetric._apply\u001b[1;34m(self, fn, exclude_state)\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor):\n\u001b[1;32m--> 792\u001b[0m     this\u001b[39m.\u001b[39m_defaults[key] \u001b[39m=\u001b[39m fn(value)\n\u001b[0;32m    793\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Sequence):\n\u001b[0;32m    794\u001b[0m     this\u001b[39m.\u001b[39m_defaults[key] \u001b[39m=\u001b[39m [fn(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m value]\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32mc:\\Users\\astus\\code\\extractive-summarization\\.venv\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
